{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3af68-ce31-48f3-8b90-4b5291e443b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No personal conf_private.py found.\n",
      "doodad not detected\n",
      "2024-10-25 21:10:05.733398 +0330 | Variant:\n",
      "2024-10-25 21:10:05.733608 +0330 | {\n",
      "  \"algorithm_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"min_num_steps_before_training\": 10000,\n",
      "    \"num_epochs\": 68,\n",
      "    \"num_eval_paths_per_epoch\": 10,\n",
      "    \"num_expl_steps_per_train_loop\": 5000,\n",
      "    \"num_trains_per_train_loop\": 5000\n",
      "  },\n",
      "  \"iq_kwargs\": {\n",
      "    \"demos\": 1,\n",
      "    \"regularize\": \"TD_both\",\n",
      "    \"loss\": \"v0\",\n",
      "    \"chi\": 0.5,\n",
      "    \"expert_path\": \"experts/Humanoid-v2_25.pkl\",\n",
      "    \"subsample_freq\": 1\n",
      "  },\n",
      "  \"env\": \"Humanoid-v2\",\n",
      "  \"seed\": 0,\n",
      "  \"expectation_z\": true,\n",
      "  \"use_policy_expert_obs\": false,\n",
      "  \"eval_env_num\": 10,\n",
      "  \"expl_env_num\": 10,\n",
      "  \"layer_size\": 256,\n",
      "  \"num_quantiles\": 24,\n",
      "  \"replay_buffer_size\": 1000000,\n",
      "  \"trainer_kwargs\": {\n",
      "    \"alpha\": 0.01,\n",
      "    \"discount\": 0.99,\n",
      "    \"policy_lr\": 1e-05,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"tau_type\": \"iqn\",\n",
      "    \"use_automatic_entropy_tuning\": false,\n",
      "    \"zf_lr\": 0.0003,\n",
      "    \"expert_lambda\": 10,\n",
      "    \"expert_lambda_lr\": 0.0001,\n",
      "    \"tune_expert_lambda\": true,\n",
      "    \"policy_lambda\": 5,\n",
      "    \"policy_lambda_lr\": 0.0001,\n",
      "    \"tune_policy_lambda\": false\n",
      "  },\n",
      "  \"version\": \"normal-iqn-neutral\"\n",
      "}\n",
      "/home/eddie/venvs/LSIQ/lib/python3.10/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2024-10-25 21:11:49.475858 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 0 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 15000\n",
      "trainer/ZF1 Loss                      35.7611\n",
      "trainer/ZF2 Loss                      36.2348\n",
      "trainer/ZF Expert Reward              -0.498529\n",
      "trainer/ZF Policy Reward              -0.571716\n",
      "trainer/ZF CHI2 Term                  35.5001\n",
      "trainer/Policy Loss                    0.0682178\n",
      "trainer/expert_lambda Loss            55.1335\n",
      "trainer/expert_lambda Value            9.9999\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm               0.0789937\n",
      "trainer/Policy Param Norm             17.4287\n",
      "trainer/Zf1 Grad Norm                103.216\n",
      "trainer/Zf1 Param Norm                32.0463\n",
      "trainer/Zf2 Grad Norm                102.74\n",
      "trainer/Zf2 Param Norm                32.036\n",
      "trainer/Z Expert Predictions Mean     -0.190365\n",
      "trainer/Z Expert Predictions Std       0.18118\n",
      "trainer/Z Expert Predictions Max       0.13977\n",
      "trainer/Z Expert Predictions Min      -0.804621\n",
      "trainer/Z Policy Predictions Mean     -0.315152\n",
      "trainer/Z Policy Predictions Std       0.181904\n",
      "trainer/Z Policy Predictions Max       0.0547174\n",
      "trainer/Z Policy Predictions Min      -0.79134\n",
      "trainer/Z Expert Targets Mean          0.308164\n",
      "trainer/Z Expert Targets Std           0.139588\n",
      "trainer/Z Expert Targets Max           0.548624\n",
      "trainer/Z Expert Targets Min          -0.141984\n",
      "trainer/Z Policy Targets Mean          0.256564\n",
      "trainer/Z Policy Targets Std           0.234027\n",
      "trainer/Z Policy Targets Max           0.852296\n",
      "trainer/Z Policy Targets Min          -0.414176\n",
      "trainer/Log Pis Mean                 -11.4078\n",
      "trainer/Log Pis Std                    0.949276\n",
      "trainer/Policy mu Mean                -0.00112765\n",
      "trainer/Policy mu Std                  0.0060151\n",
      "trainer/Policy log std Mean            0.000127196\n",
      "trainer/Policy log std Std             0.0047607\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        14822\n",
      "exploration/num paths total          516\n",
      "evaluation/num steps total           698\n",
      "evaluation/num paths total            10\n",
      "evaluation/path length Mean           69.8\n",
      "evaluation/path length Std            16.1481\n",
      "evaluation/path length Max           106\n",
      "evaluation/path length Min            57\n",
      "evaluation/Rewards Mean                4.71092\n",
      "evaluation/Rewards Std                 0.283223\n",
      "evaluation/Rewards Max                 5.47565\n",
      "evaluation/Rewards Min                 3.71637\n",
      "evaluation/Returns Mean              328.822\n",
      "evaluation/Returns Std                64.1535\n",
      "evaluation/Returns Max               467.877\n",
      "evaluation/Returns Min               272.517\n",
      "evaluation/Estimation Bias Mean      105.358\n",
      "evaluation/Estimation Bias Std        70.0187\n",
      "evaluation/EB/Q_True Mean             25.3308\n",
      "evaluation/EB/Q_True Std              68.2593\n",
      "evaluation/EB/Q_Pred Mean            130.689\n",
      "evaluation/EB/Q_Pred Std              16.3967\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           328.822\n",
      "evaluation/Actions Mean                0.077707\n",
      "evaluation/Actions Std                 0.824169\n",
      "evaluation/Actions Max                 0.999996\n",
      "evaluation/Actions Min                -0.999999\n",
      "time/backward_policy (s)               8.76013\n",
      "time/backward_zf1 (s)                  9.93092\n",
      "time/backward_zf2 (s)                  9.44199\n",
      "time/data sampling (s)                 1.58928\n",
      "time/data storing (s)                  0.0841861\n",
      "time/evaluation sampling (s)           0.342587\n",
      "time/exploration sampling (s)          2.75666\n",
      "time/logging (s)                       0.00169269\n",
      "time/preback_alpha (s)                 0.00630507\n",
      "time/preback_policy (s)               15.4795\n",
      "time/preback_start (s)                 1.02352\n",
      "time/preback_zf (s)                   36.8312\n",
      "time/saving (s)                        3.153e-06\n",
      "time/training (s)                     11.2835\n",
      "time/epoch (s)                        97.5315\n",
      "time/total (s)                       105.121\n",
      "Epoch                                  0\n",
      "---------------------------------  ---------------\n",
      "2024-10-25 21:13:26.790883 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 1 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 20000\n",
      "trainer/ZF1 Loss                       8.66539\n",
      "trainer/ZF2 Loss                       5.62353\n",
      "trainer/ZF Expert Reward              14.2714\n",
      "trainer/ZF Policy Reward               5.47404\n",
      "trainer/ZF CHI2 Term                  20.0518\n",
      "trainer/Policy Loss                 -129.988\n",
      "trainer/expert_lambda Loss            23.2435\n",
      "trainer/expert_lambda Value           10.3662\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              16.0916\n",
      "trainer/Policy Param Norm             18.6437\n",
      "trainer/Zf1 Grad Norm               1414.43\n",
      "trainer/Zf1 Param Norm                41.9114\n",
      "trainer/Zf2 Grad Norm                945.895\n",
      "trainer/Zf2 Param Norm                41.9646\n",
      "trainer/Z Expert Predictions Mean    144.064\n",
      "trainer/Z Expert Predictions Std      16.9605\n",
      "trainer/Z Expert Predictions Max     171.319\n",
      "trainer/Z Expert Predictions Min      29.1045\n",
      "trainer/Z Policy Predictions Mean    123.466\n",
      "trainer/Z Policy Predictions Std      46.7381\n",
      "trainer/Z Policy Predictions Max     157.707\n",
      "trainer/Z Policy Predictions Min       2.01311\n",
      "trainer/Z Expert Targets Mean        129.792\n",
      "trainer/Z Expert Targets Std          19.2856\n",
      "trainer/Z Expert Targets Max         164.892\n",
      "trainer/Z Expert Targets Min          14.7504\n",
      "trainer/Z Policy Targets Mean        117.992\n",
      "trainer/Z Policy Targets Std          47.4822\n",
      "trainer/Z Policy Targets Max         154.762\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  36.3494\n",
      "trainer/Log Pis Std                   14.5462\n",
      "trainer/Policy mu Mean                 0.485058\n",
      "trainer/Policy mu Std                  2.2587\n",
      "trainer/Policy log std Mean           -0.714119\n",
      "trainer/Policy log std Std             0.699373\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        19770\n",
      "exploration/num paths total          631\n",
      "evaluation/num steps total          1255\n",
      "evaluation/num paths total            20\n",
      "evaluation/path length Mean           55.7\n",
      "evaluation/path length Std             4.17253\n",
      "evaluation/path length Max            61\n",
      "evaluation/path length Min            49\n",
      "evaluation/Rewards Mean                4.7842\n",
      "evaluation/Rewards Std                 0.146357\n",
      "evaluation/Rewards Max                 5.0786\n",
      "evaluation/Rewards Min                 4.31766\n",
      "evaluation/Returns Mean              266.48\n",
      "evaluation/Returns Std                19.4074\n",
      "evaluation/Returns Max               290.388\n",
      "evaluation/Returns Min               231.776\n",
      "evaluation/Estimation Bias Mean      144.802\n",
      "evaluation/Estimation Bias Std        67.8586\n",
      "evaluation/EB/Q_True Mean             13.1726\n",
      "evaluation/EB/Q_True Std              43.0213\n",
      "evaluation/EB/Q_Pred Mean            157.975\n",
      "evaluation/EB/Q_Pred Std              57.9179\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           266.48\n",
      "evaluation/Actions Mean               -0.0463627\n",
      "evaluation/Actions Std                 0.79257\n",
      "evaluation/Actions Max                 0.999941\n",
      "evaluation/Actions Min                -0.999994\n",
      "time/backward_policy (s)               8.47915\n",
      "time/backward_zf1 (s)                  9.92288\n",
      "time/backward_zf2 (s)                  9.52092\n",
      "time/data sampling (s)                 1.60547\n",
      "time/data storing (s)                  0.0844169\n",
      "time/evaluation sampling (s)           0.291719\n",
      "time/exploration sampling (s)          2.51298\n",
      "time/logging (s)                       0.00282099\n",
      "time/preback_alpha (s)                 0.00623491\n",
      "time/preback_policy (s)               15.5651\n",
      "time/preback_start (s)                 1.00114\n",
      "time/preback_zf (s)                   36.8667\n",
      "time/saving (s)                        5.08e-06\n",
      "time/training (s)                     11.2426\n",
      "time/epoch (s)                        97.1022\n",
      "time/total (s)                       202.226\n",
      "Epoch                                  1\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:15:03.304505 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 2 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 25000\n",
      "trainer/ZF1 Loss                      11.6948\n",
      "trainer/ZF2 Loss                       9.33593\n",
      "trainer/ZF Expert Reward              11.4591\n",
      "trainer/ZF Policy Reward               3.57217\n",
      "trainer/ZF CHI2 Term                  20.1997\n",
      "trainer/Policy Loss                 -164.958\n",
      "trainer/expert_lambda Loss            17.4587\n",
      "trainer/expert_lambda Value           10.7841\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              22.5109\n",
      "trainer/Policy Param Norm             19.1054\n",
      "trainer/Zf1 Grad Norm               2306.79\n",
      "trainer/Zf1 Param Norm                45.7746\n",
      "trainer/Zf2 Grad Norm               2112.81\n",
      "trainer/Zf2 Param Norm                45.7431\n",
      "trainer/Z Expert Predictions Mean    178.257\n",
      "trainer/Z Expert Predictions Std      23.8608\n",
      "trainer/Z Expert Predictions Max     233.547\n",
      "trainer/Z Expert Predictions Min      11.965\n",
      "trainer/Z Policy Predictions Mean    160.268\n",
      "trainer/Z Policy Predictions Std      69.5256\n",
      "trainer/Z Policy Predictions Max     218.468\n",
      "trainer/Z Policy Predictions Min       4.29669\n",
      "trainer/Z Expert Targets Mean        166.798\n",
      "trainer/Z Expert Targets Std          24.981\n",
      "trainer/Z Expert Targets Max         227.466\n",
      "trainer/Z Expert Targets Min           0\n",
      "trainer/Z Policy Targets Mean        156.696\n",
      "trainer/Z Policy Targets Std          68.9771\n",
      "trainer/Z Policy Targets Max         215.039\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  36.1267\n",
      "trainer/Log Pis Std                   16.7046\n",
      "trainer/Policy mu Mean                 0.0186854\n",
      "trainer/Policy mu Std                  2.243\n",
      "trainer/Policy log std Mean           -0.87549\n",
      "trainer/Policy log std Std             0.602909\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        24710\n",
      "exploration/num paths total          706\n",
      "evaluation/num steps total          1863\n",
      "evaluation/num paths total            30\n",
      "evaluation/path length Mean           60.8\n",
      "evaluation/path length Std             8.47113\n",
      "evaluation/path length Max            79\n",
      "evaluation/path length Min            53\n",
      "evaluation/Rewards Mean                4.62649\n",
      "evaluation/Rewards Std                 0.224036\n",
      "evaluation/Rewards Max                 4.91334\n",
      "evaluation/Rewards Min                 3.84813\n",
      "evaluation/Returns Mean              281.291\n",
      "evaluation/Returns Std                39.0435\n",
      "evaluation/Returns Max               363.701\n",
      "evaluation/Returns Min               243.776\n",
      "evaluation/Estimation Bias Mean      184\n",
      "evaluation/Estimation Bias Std        84.4309\n",
      "evaluation/EB/Q_True Mean             18.2139\n",
      "evaluation/EB/Q_True Std              54.2815\n",
      "evaluation/EB/Q_Pred Mean            202.214\n",
      "evaluation/EB/Q_Pred Std              72.8286\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           281.291\n",
      "evaluation/Actions Mean                0.148459\n",
      "evaluation/Actions Std                 0.784561\n",
      "evaluation/Actions Max                 0.999992\n",
      "evaluation/Actions Min                -0.999975\n",
      "time/backward_policy (s)               8.689\n",
      "time/backward_zf1 (s)                 10.0059\n",
      "time/backward_zf2 (s)                  9.77068\n",
      "time/data sampling (s)                 1.55025\n",
      "time/data storing (s)                  0.0819851\n",
      "time/evaluation sampling (s)           0.372966\n",
      "time/exploration sampling (s)          2.52519\n",
      "time/logging (s)                       0.00273713\n",
      "time/preback_alpha (s)                 0.00607439\n",
      "time/preback_policy (s)               15.0312\n",
      "time/preback_start (s)                 0.969164\n",
      "time/preback_zf (s)                   36.5051\n",
      "time/saving (s)                        5.19e-06\n",
      "time/training (s)                     10.795\n",
      "time/epoch (s)                        96.3052\n",
      "time/total (s)                       298.534\n",
      "Epoch                                  2\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:16:41.208512 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 3 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 30000\n",
      "trainer/ZF1 Loss                      11.8006\n",
      "trainer/ZF2 Loss                       9.96845\n",
      "trainer/ZF Expert Reward              13.3083\n",
      "trainer/ZF Policy Reward               4.7892\n",
      "trainer/ZF CHI2 Term                  22.2602\n",
      "trainer/Policy Loss                 -188.376\n",
      "trainer/expert_lambda Loss            17.0314\n",
      "trainer/expert_lambda Value           11.1958\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              29.9648\n",
      "trainer/Policy Param Norm             19.587\n",
      "trainer/Zf1 Grad Norm               2143.57\n",
      "trainer/Zf1 Param Norm                49.4726\n",
      "trainer/Zf2 Grad Norm               1715.13\n",
      "trainer/Zf2 Param Norm                49.2753\n",
      "trainer/Z Expert Predictions Mean    199.491\n",
      "trainer/Z Expert Predictions Std      24.7415\n",
      "trainer/Z Expert Predictions Max     289.495\n",
      "trainer/Z Expert Predictions Min     147.662\n",
      "trainer/Z Policy Predictions Mean    183.414\n",
      "trainer/Z Policy Predictions Std      92.199\n",
      "trainer/Z Policy Predictions Max     271.771\n",
      "trainer/Z Policy Predictions Min       1.08862\n",
      "trainer/Z Expert Targets Mean        186.183\n",
      "trainer/Z Expert Targets Std          25.4703\n",
      "trainer/Z Expert Targets Max         284.785\n",
      "trainer/Z Expert Targets Min         134.4\n",
      "trainer/Z Policy Targets Mean        178.624\n",
      "trainer/Z Policy Targets Std          92.7015\n",
      "trainer/Z Policy Targets Max         267.419\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  34.7661\n",
      "trainer/Log Pis Std                   15.0276\n",
      "trainer/Policy mu Mean                 0.484537\n",
      "trainer/Policy mu Std                  2.12946\n",
      "trainer/Policy log std Mean           -1.01723\n",
      "trainer/Policy log std Std             0.507859\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        29515\n",
      "exploration/num paths total          766\n",
      "evaluation/num steps total          2476\n",
      "evaluation/num paths total            40\n",
      "evaluation/path length Mean           61.3\n",
      "evaluation/path length Std             7.12811\n",
      "evaluation/path length Max            74\n",
      "evaluation/path length Min            53\n",
      "evaluation/Rewards Mean                4.56286\n",
      "evaluation/Rewards Std                 0.380345\n",
      "evaluation/Rewards Max                 5.11738\n",
      "evaluation/Rewards Min                 3.33978\n",
      "evaluation/Returns Mean              279.703\n",
      "evaluation/Returns Std                35.0794\n",
      "evaluation/Returns Max               350.478\n",
      "evaluation/Returns Min               237.996\n",
      "evaluation/Estimation Bias Mean      208.796\n",
      "evaluation/Estimation Bias Std        99.3236\n",
      "evaluation/EB/Q_True Mean             16.7528\n",
      "evaluation/EB/Q_True Std              51.7851\n",
      "evaluation/EB/Q_Pred Mean            225.549\n",
      "evaluation/EB/Q_Pred Std              88.3094\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           279.703\n",
      "evaluation/Actions Mean                0.224912\n",
      "evaluation/Actions Std                 0.7328\n",
      "evaluation/Actions Max                 0.999989\n",
      "evaluation/Actions Min                -0.999991\n",
      "time/backward_policy (s)               8.72475\n",
      "time/backward_zf1 (s)                 10.1564\n",
      "time/backward_zf2 (s)                  9.81326\n",
      "time/data sampling (s)                 1.61056\n",
      "time/data storing (s)                  0.0841353\n",
      "time/evaluation sampling (s)           0.357684\n",
      "time/exploration sampling (s)          2.38833\n",
      "time/logging (s)                       0.00365744\n",
      "time/preback_alpha (s)                 0.00626392\n",
      "time/preback_policy (s)               15.3935\n",
      "time/preback_start (s)                 0.995773\n",
      "time/preback_zf (s)                   36.9666\n",
      "time/saving (s)                        5.983e-06\n",
      "time/training (s)                     11.1893\n",
      "time/epoch (s)                        97.6902\n",
      "time/total (s)                       396.227\n",
      "Epoch                                  3\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:18:17.443903 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 4 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 35000\n",
      "trainer/ZF1 Loss                      14.6269\n",
      "trainer/ZF2 Loss                      19.384\n",
      "trainer/ZF Expert Reward              14.1401\n",
      "trainer/ZF Policy Reward               5.81317\n",
      "trainer/ZF CHI2 Term                  28.6702\n",
      "trainer/Policy Loss                 -180.598\n",
      "trainer/expert_lambda Loss            20.4809\n",
      "trainer/expert_lambda Value           11.6089\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              31.633\n",
      "trainer/Policy Param Norm             20.0951\n",
      "trainer/Zf1 Grad Norm               1699.92\n",
      "trainer/Zf1 Param Norm                52.8183\n",
      "trainer/Zf2 Grad Norm               2731.37\n",
      "trainer/Zf2 Param Norm                52.5869\n",
      "trainer/Z Expert Predictions Mean    249.628\n",
      "trainer/Z Expert Predictions Std      33.015\n",
      "trainer/Z Expert Predictions Max     330.566\n",
      "trainer/Z Expert Predictions Min      18.9693\n",
      "trainer/Z Policy Predictions Mean    176.394\n",
      "trainer/Z Policy Predictions Std     100.733\n",
      "trainer/Z Policy Predictions Max     294.073\n",
      "trainer/Z Policy Predictions Min      -3.11858\n",
      "trainer/Z Expert Targets Mean        235.488\n",
      "trainer/Z Expert Targets Std          34.1777\n",
      "trainer/Z Expert Targets Max         322.241\n",
      "trainer/Z Expert Targets Min           0\n",
      "trainer/Z Policy Targets Mean        170.581\n",
      "trainer/Z Policy Targets Std         101.502\n",
      "trainer/Z Policy Targets Max         292.008\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  34.4068\n",
      "trainer/Log Pis Std                   15.2737\n",
      "trainer/Policy mu Mean                 0.484952\n",
      "trainer/Policy mu Std                  2.02276\n",
      "trainer/Policy log std Mean           -1.17342\n",
      "trainer/Policy log std Std             0.455157\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        34430\n",
      "exploration/num paths total          819\n",
      "evaluation/num steps total          3495\n",
      "evaluation/num paths total            50\n",
      "evaluation/path length Mean          101.9\n",
      "evaluation/path length Std            15.2279\n",
      "evaluation/path length Max           134\n",
      "evaluation/path length Min            87\n",
      "evaluation/Rewards Mean                5.0918\n",
      "evaluation/Rewards Std                 0.450146\n",
      "evaluation/Rewards Max                 6.45121\n",
      "evaluation/Rewards Min                 3.7613\n",
      "evaluation/Returns Mean              518.854\n",
      "evaluation/Returns Std                58.389\n",
      "evaluation/Returns Max               620.777\n",
      "evaluation/Returns Min               448.755\n",
      "evaluation/Estimation Bias Mean      150.536\n",
      "evaluation/Estimation Bias Std       120.451\n",
      "evaluation/EB/Q_True Mean             26.1874\n",
      "evaluation/EB/Q_True Std              75.5369\n",
      "evaluation/EB/Q_Pred Mean            176.724\n",
      "evaluation/EB/Q_Pred Std             101.837\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           518.854\n",
      "evaluation/Actions Mean                0.113689\n",
      "evaluation/Actions Std                 0.725782\n",
      "evaluation/Actions Max                 0.999996\n",
      "evaluation/Actions Min                -0.999996\n",
      "time/backward_policy (s)               7.76204\n",
      "time/backward_zf1 (s)                  9.11859\n",
      "time/backward_zf2 (s)                  8.70467\n",
      "time/data sampling (s)                 1.56535\n",
      "time/data storing (s)                  0.0825786\n",
      "time/evaluation sampling (s)           0.673643\n",
      "time/exploration sampling (s)          2.4372\n",
      "time/logging (s)                       0.00294324\n",
      "time/preback_alpha (s)                 0.00627102\n",
      "time/preback_policy (s)               16.5392\n",
      "time/preback_start (s)                 0.962444\n",
      "time/preback_zf (s)                   36.4866\n",
      "time/saving (s)                        3.618e-06\n",
      "time/training (s)                     11.6836\n",
      "time/epoch (s)                        96.0252\n",
      "time/total (s)                       492.255\n",
      "Epoch                                  4\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:19:54.207904 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 5 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 40000\n",
      "trainer/ZF1 Loss                      14.9021\n",
      "trainer/ZF2 Loss                      17.2583\n",
      "trainer/ZF Expert Reward              12.2401\n",
      "trainer/ZF Policy Reward               3.84244\n",
      "trainer/ZF CHI2 Term                  26.2757\n",
      "trainer/Policy Loss                 -178.087\n",
      "trainer/expert_lambda Loss            16.996\n",
      "trainer/expert_lambda Value           12.0157\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              31.55\n",
      "trainer/Policy Param Norm             20.6665\n",
      "trainer/Zf1 Grad Norm               2532.27\n",
      "trainer/Zf1 Param Norm                55.9834\n",
      "trainer/Zf2 Grad Norm               4123.97\n",
      "trainer/Zf2 Param Norm                55.9686\n",
      "trainer/Z Expert Predictions Mean    214.741\n",
      "trainer/Z Expert Predictions Std      43.8989\n",
      "trainer/Z Expert Predictions Max     370.084\n",
      "trainer/Z Expert Predictions Min      97.9833\n",
      "trainer/Z Policy Predictions Mean    173.401\n",
      "trainer/Z Policy Predictions Std     109.041\n",
      "trainer/Z Policy Predictions Max     322.595\n",
      "trainer/Z Policy Predictions Min       4.27277\n",
      "trainer/Z Expert Targets Mean        202.5\n",
      "trainer/Z Expert Targets Std          43.8404\n",
      "trainer/Z Expert Targets Max         363.29\n",
      "trainer/Z Expert Targets Min          86.9586\n",
      "trainer/Z Policy Targets Mean        169.558\n",
      "trainer/Z Policy Targets Std         109.502\n",
      "trainer/Z Policy Targets Max         312.155\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  32.5703\n",
      "trainer/Log Pis Std                   14.9357\n",
      "trainer/Policy mu Mean                 0.321189\n",
      "trainer/Policy mu Std                  1.9603\n",
      "trainer/Policy log std Mean           -1.2799\n",
      "trainer/Policy log std Std             0.397499\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        39718\n",
      "exploration/num paths total          876\n",
      "evaluation/num steps total          4440\n",
      "evaluation/num paths total            60\n",
      "evaluation/path length Mean           94.5\n",
      "evaluation/path length Std            19.6685\n",
      "evaluation/path length Max           138\n",
      "evaluation/path length Min            65\n",
      "evaluation/Rewards Mean                4.94151\n",
      "evaluation/Rewards Std                 0.358145\n",
      "evaluation/Rewards Max                 6.42521\n",
      "evaluation/Rewards Min                 3.34715\n",
      "evaluation/Returns Mean              466.973\n",
      "evaluation/Returns Std                96.4924\n",
      "evaluation/Returns Max               638.27\n",
      "evaluation/Returns Min               312.347\n",
      "evaluation/Estimation Bias Mean      185.936\n",
      "evaluation/Estimation Bias Std       117.696\n",
      "evaluation/EB/Q_True Mean             30.2413\n",
      "evaluation/EB/Q_True Std              83.2942\n",
      "evaluation/EB/Q_Pred Mean            216.178\n",
      "evaluation/EB/Q_Pred Std              98.3832\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           466.973\n",
      "evaluation/Actions Mean                0.0882745\n",
      "evaluation/Actions Std                 0.719982\n",
      "evaluation/Actions Max                 0.999997\n",
      "evaluation/Actions Min                -0.999998\n",
      "time/backward_policy (s)               8.77173\n",
      "time/backward_zf1 (s)                 10.0327\n",
      "time/backward_zf2 (s)                  9.81712\n",
      "time/data sampling (s)                 1.59514\n",
      "time/data storing (s)                  0.0835236\n",
      "time/evaluation sampling (s)           0.632317\n",
      "time/exploration sampling (s)          2.31761\n",
      "time/logging (s)                       0.0024706\n",
      "time/preback_alpha (s)                 0.00613361\n",
      "time/preback_policy (s)               14.982\n",
      "time/preback_start (s)                 0.970787\n",
      "time/preback_zf (s)                   36.5177\n",
      "time/saving (s)                        2.957e-06\n",
      "time/training (s)                     10.8252\n",
      "time/epoch (s)                        96.5544\n",
      "time/total (s)                       588.813\n",
      "Epoch                                  5\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:21:31.221830 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 6 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 45000\n",
      "trainer/ZF1 Loss                       9.86727\n",
      "trainer/ZF2 Loss                       7.78671\n",
      "trainer/ZF Expert Reward              14.5174\n",
      "trainer/ZF Policy Reward               6.07238\n",
      "trainer/ZF CHI2 Term                  21.3886\n",
      "trainer/Policy Loss                 -183.256\n",
      "trainer/expert_lambda Loss            11.2555\n",
      "trainer/expert_lambda Value           12.4344\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              33.2733\n",
      "trainer/Policy Param Norm             21.1582\n",
      "trainer/Zf1 Grad Norm               2043.08\n",
      "trainer/Zf1 Param Norm                58.1268\n",
      "trainer/Zf2 Grad Norm               1738.36\n",
      "trainer/Zf2 Param Norm                58.3104\n",
      "trainer/Z Expert Predictions Mean    201.323\n",
      "trainer/Z Expert Predictions Std      37.7912\n",
      "trainer/Z Expert Predictions Max     380.813\n",
      "trainer/Z Expert Predictions Min      13.1178\n",
      "trainer/Z Policy Predictions Mean    181.765\n",
      "trainer/Z Policy Predictions Std     108.055\n",
      "trainer/Z Policy Predictions Max     329.487\n",
      "trainer/Z Policy Predictions Min     -10.016\n",
      "trainer/Z Expert Targets Mean        186.806\n",
      "trainer/Z Expert Targets Std          38.1448\n",
      "trainer/Z Expert Targets Max         370.347\n",
      "trainer/Z Expert Targets Min           0\n",
      "trainer/Z Policy Targets Mean        175.692\n",
      "trainer/Z Policy Targets Std         107.597\n",
      "trainer/Z Policy Targets Max         309.171\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  32.3761\n",
      "trainer/Log Pis Std                   14.3882\n",
      "trainer/Policy mu Mean                 0.345272\n",
      "trainer/Policy mu Std                  1.91807\n",
      "trainer/Policy log std Mean           -1.3307\n",
      "trainer/Policy log std Std             0.409383\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        44691\n",
      "exploration/num paths total          940\n",
      "evaluation/num steps total          5164\n",
      "evaluation/num paths total            70\n",
      "evaluation/path length Mean           72.4\n",
      "evaluation/path length Std             3.41174\n",
      "evaluation/path length Max            78\n",
      "evaluation/path length Min            67\n",
      "evaluation/Rewards Mean                5.17296\n",
      "evaluation/Rewards Std                 0.331049\n",
      "evaluation/Rewards Max                 6.55983\n",
      "evaluation/Rewards Min                 4.79467\n",
      "evaluation/Returns Mean              374.523\n",
      "evaluation/Returns Std                20.2566\n",
      "evaluation/Returns Max               408.683\n",
      "evaluation/Returns Min               348.254\n",
      "evaluation/Estimation Bias Mean      180.963\n",
      "evaluation/Estimation Bias Std       111.818\n",
      "evaluation/EB/Q_True Mean             17.6979\n",
      "evaluation/EB/Q_True Std              57.1316\n",
      "evaluation/EB/Q_Pred Mean            198.661\n",
      "evaluation/EB/Q_Pred Std             100.482\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           374.523\n",
      "evaluation/Actions Mean                0.246157\n",
      "evaluation/Actions Std                 0.700431\n",
      "evaluation/Actions Max                 0.999998\n",
      "evaluation/Actions Min                -0.999995\n",
      "time/backward_policy (s)               8.38819\n",
      "time/backward_zf1 (s)                  9.78185\n",
      "time/backward_zf2 (s)                  9.46811\n",
      "time/data sampling (s)                 1.6239\n",
      "time/data storing (s)                  0.0833424\n",
      "time/evaluation sampling (s)           0.332482\n",
      "time/exploration sampling (s)          2.41508\n",
      "time/logging (s)                       0.00331177\n",
      "time/preback_alpha (s)                 0.0062262\n",
      "time/preback_policy (s)               15.7047\n",
      "time/preback_start (s)                 0.982727\n",
      "time/preback_zf (s)                   36.7699\n",
      "time/saving (s)                        6.175e-06\n",
      "time/training (s)                     11.2415\n",
      "time/epoch (s)                        96.8014\n",
      "time/total (s)                       685.617\n",
      "Epoch                                  6\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:23:07.947004 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 7 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 50000\n",
      "trainer/ZF1 Loss                       9.59936\n",
      "trainer/ZF2 Loss                       6.34959\n",
      "trainer/ZF Expert Reward              17.2595\n",
      "trainer/ZF Policy Reward               4.51471\n",
      "trainer/ZF CHI2 Term                  23.1837\n",
      "trainer/Policy Loss                 -164.704\n",
      "trainer/expert_lambda Loss            19.3545\n",
      "trainer/expert_lambda Value           12.8702\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              36.5898\n",
      "trainer/Policy Param Norm             21.5739\n",
      "trainer/Zf1 Grad Norm               2212.29\n",
      "trainer/Zf1 Param Norm                60.2444\n",
      "trainer/Zf2 Grad Norm               2530.13\n",
      "trainer/Zf2 Param Norm                60.4202\n",
      "trainer/Z Expert Predictions Mean    218.35\n",
      "trainer/Z Expert Predictions Std      29.5344\n",
      "trainer/Z Expert Predictions Max     371.424\n",
      "trainer/Z Expert Predictions Min     162.408\n",
      "trainer/Z Policy Predictions Mean    161.108\n",
      "trainer/Z Policy Predictions Std     102.067\n",
      "trainer/Z Policy Predictions Max     334.783\n",
      "trainer/Z Policy Predictions Min      -3.20317\n",
      "trainer/Z Expert Targets Mean        201.091\n",
      "trainer/Z Expert Targets Std          30.0787\n",
      "trainer/Z Expert Targets Max         355.671\n",
      "trainer/Z Expert Targets Min         145.535\n",
      "trainer/Z Policy Targets Mean        156.594\n",
      "trainer/Z Policy Targets Std         102.409\n",
      "trainer/Z Policy Targets Max         316.333\n",
      "trainer/Z Policy Targets Min          -2.56658\n",
      "trainer/Log Pis Mean                  34.3912\n",
      "trainer/Log Pis Std                   14.5757\n",
      "trainer/Policy mu Mean                 0.718266\n",
      "trainer/Policy mu Std                  1.95137\n",
      "trainer/Policy log std Mean           -1.24776\n",
      "trainer/Policy log std Std             0.437268\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        49709\n",
      "exploration/num paths total         1007\n",
      "evaluation/num steps total          6113\n",
      "evaluation/num paths total            80\n",
      "evaluation/path length Mean           94.9\n",
      "evaluation/path length Std            20.6662\n",
      "evaluation/path length Max           138\n",
      "evaluation/path length Min            68\n",
      "evaluation/Rewards Mean                5.26492\n",
      "evaluation/Rewards Std                 0.463357\n",
      "evaluation/Rewards Max                 6.4848\n",
      "evaluation/Rewards Min                 3.52742\n",
      "evaluation/Returns Mean              499.641\n",
      "evaluation/Returns Std               109.441\n",
      "evaluation/Returns Max               730.723\n",
      "evaluation/Returns Min               372.699\n",
      "evaluation/Estimation Bias Mean      154.199\n",
      "evaluation/Estimation Bias Std       132.757\n",
      "evaluation/EB/Q_True Mean             36.569\n",
      "evaluation/EB/Q_True Std              97.4693\n",
      "evaluation/EB/Q_Pred Mean            190.768\n",
      "evaluation/EB/Q_Pred Std              89.6476\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           499.641\n",
      "evaluation/Actions Mean                0.117057\n",
      "evaluation/Actions Std                 0.652542\n",
      "evaluation/Actions Max                 1\n",
      "evaluation/Actions Min                -0.999934\n",
      "time/backward_policy (s)               8.14212\n",
      "time/backward_zf1 (s)                  9.4829\n",
      "time/backward_zf2 (s)                  9.11233\n",
      "time/data sampling (s)                 1.5857\n",
      "time/data storing (s)                  0.0831168\n",
      "time/evaluation sampling (s)           0.631127\n",
      "time/exploration sampling (s)          2.33521\n",
      "time/logging (s)                       0.00175358\n",
      "time/preback_alpha (s)                 0.00617568\n",
      "time/preback_policy (s)               16.0698\n",
      "time/preback_start (s)                 0.969719\n",
      "time/preback_zf (s)                   36.6374\n",
      "time/saving (s)                        2.672e-06\n",
      "time/training (s)                     11.4569\n",
      "time/epoch (s)                        96.5142\n",
      "time/total (s)                       782.135\n",
      "Epoch                                  7\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:24:44.325361 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 8 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 55000\n",
      "trainer/ZF1 Loss                       3.21688\n",
      "trainer/ZF2 Loss                       3.76041\n",
      "trainer/ZF Expert Reward              16.5801\n",
      "trainer/ZF Policy Reward               5.62778\n",
      "trainer/ZF CHI2 Term                  17.855\n",
      "trainer/Policy Loss                 -174.136\n",
      "trainer/expert_lambda Loss            11.5599\n",
      "trainer/expert_lambda Value           13.3127\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              31.4882\n",
      "trainer/Policy Param Norm             21.9561\n",
      "trainer/Zf1 Grad Norm               1254.17\n",
      "trainer/Zf1 Param Norm                62.4707\n",
      "trainer/Zf2 Grad Norm               1590.98\n",
      "trainer/Zf2 Param Norm                62.4765\n",
      "trainer/Z Expert Predictions Mean    228.479\n",
      "trainer/Z Expert Predictions Std      30.474\n",
      "trainer/Z Expert Predictions Max     387.524\n",
      "trainer/Z Expert Predictions Min      88.454\n",
      "trainer/Z Policy Predictions Mean    171.205\n",
      "trainer/Z Policy Predictions Std      94.5523\n",
      "trainer/Z Policy Predictions Max     330.55\n",
      "trainer/Z Policy Predictions Min       2.13826\n",
      "trainer/Z Expert Targets Mean        211.899\n",
      "trainer/Z Expert Targets Std          31.3148\n",
      "trainer/Z Expert Targets Max         373.401\n",
      "trainer/Z Expert Targets Min          67.0841\n",
      "trainer/Z Policy Targets Mean        165.577\n",
      "trainer/Z Policy Targets Std          94.5168\n",
      "trainer/Z Policy Targets Max         323.602\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  29.9259\n",
      "trainer/Log Pis Std                   14.2419\n",
      "trainer/Policy mu Mean                 0.471015\n",
      "trainer/Policy mu Std                  1.81872\n",
      "trainer/Policy log std Mean           -1.3574\n",
      "trainer/Policy log std Std             0.513979\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        54736\n",
      "exploration/num paths total         1065\n",
      "evaluation/num steps total          6828\n",
      "evaluation/num paths total            90\n",
      "evaluation/path length Mean           71.5\n",
      "evaluation/path length Std             2.97489\n",
      "evaluation/path length Max            77\n",
      "evaluation/path length Min            67\n",
      "evaluation/Rewards Mean                4.95387\n",
      "evaluation/Rewards Std                 0.161901\n",
      "evaluation/Rewards Max                 5.47008\n",
      "evaluation/Rewards Min                 4.56077\n",
      "evaluation/Returns Mean              354.201\n",
      "evaluation/Returns Std                 7.87145\n",
      "evaluation/Returns Max               365.869\n",
      "evaluation/Returns Min               339.307\n",
      "evaluation/Estimation Bias Mean      178.595\n",
      "evaluation/Estimation Bias Std       103.786\n",
      "evaluation/EB/Q_True Mean             15.5451\n",
      "evaluation/EB/Q_True Std              50.8175\n",
      "evaluation/EB/Q_Pred Mean            194.14\n",
      "evaluation/EB/Q_Pred Std              94.3994\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           354.201\n",
      "evaluation/Actions Mean                0.110248\n",
      "evaluation/Actions Std                 0.665667\n",
      "evaluation/Actions Max                 1\n",
      "evaluation/Actions Min                -0.999982\n",
      "time/backward_policy (s)               8.16535\n",
      "time/backward_zf1 (s)                  9.55413\n",
      "time/backward_zf2 (s)                  9.15027\n",
      "time/data sampling (s)                 1.61105\n",
      "time/data storing (s)                  0.0839988\n",
      "time/evaluation sampling (s)           0.374238\n",
      "time/exploration sampling (s)          2.41209\n",
      "time/logging (s)                       0.0027198\n",
      "time/preback_alpha (s)                 0.00620746\n",
      "time/preback_policy (s)               15.8829\n",
      "time/preback_start (s)                 0.979852\n",
      "time/preback_zf (s)                   36.586\n",
      "time/saving (s)                        3.639e-06\n",
      "time/training (s)                     11.361\n",
      "time/epoch (s)                        96.1699\n",
      "time/total (s)                       878.307\n",
      "Epoch                                  8\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:26:20.858555 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 9 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 60000\n",
      "trainer/ZF1 Loss                       2.39619\n",
      "trainer/ZF2 Loss                       2.30702\n",
      "trainer/ZF Expert Reward              16.9127\n",
      "trainer/ZF Policy Reward               6.03879\n",
      "trainer/ZF CHI2 Term                  16.3663\n",
      "trainer/Policy Loss                 -176.64\n",
      "trainer/expert_lambda Loss            10.8737\n",
      "trainer/expert_lambda Value           13.7658\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              31.4144\n",
      "trainer/Policy Param Norm             22.2286\n",
      "trainer/Zf1 Grad Norm               2141.6\n",
      "trainer/Zf1 Param Norm                64.8697\n",
      "trainer/Zf2 Grad Norm               1398.11\n",
      "trainer/Zf2 Param Norm                64.5469\n",
      "trainer/Z Expert Predictions Mean    298.766\n",
      "trainer/Z Expert Predictions Std      23.9168\n",
      "trainer/Z Expert Predictions Max     362.841\n",
      "trainer/Z Expert Predictions Min     224.473\n",
      "trainer/Z Policy Predictions Mean    174.222\n",
      "trainer/Z Policy Predictions Std      83.1188\n",
      "trainer/Z Policy Predictions Max     304.057\n",
      "trainer/Z Policy Predictions Min      -3.2233\n",
      "trainer/Z Expert Targets Mean        281.854\n",
      "trainer/Z Expert Targets Std          24.5712\n",
      "trainer/Z Expert Targets Max         347.964\n",
      "trainer/Z Expert Targets Min         211.519\n",
      "trainer/Z Policy Targets Mean        168.183\n",
      "trainer/Z Policy Targets Std          83.5152\n",
      "trainer/Z Policy Targets Max         298.64\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  27.6768\n",
      "trainer/Log Pis Std                   11.4382\n",
      "trainer/Policy mu Mean                 0.323343\n",
      "trainer/Policy mu Std                  1.70197\n",
      "trainer/Policy log std Mean           -1.36196\n",
      "trainer/Policy log std Std             0.55061\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        59614\n",
      "exploration/num paths total         1119\n",
      "evaluation/num steps total          7915\n",
      "evaluation/num paths total           100\n",
      "evaluation/path length Mean          108.7\n",
      "evaluation/path length Std            21.0098\n",
      "evaluation/path length Max           147\n",
      "evaluation/path length Min            80\n",
      "evaluation/Rewards Mean                5.13682\n",
      "evaluation/Rewards Std                 0.570301\n",
      "evaluation/Rewards Max                 6.66709\n",
      "evaluation/Rewards Min                 3.31655\n",
      "evaluation/Returns Mean              558.372\n",
      "evaluation/Returns Std                99.2582\n",
      "evaluation/Returns Max               713.705\n",
      "evaluation/Returns Min               430.847\n",
      "evaluation/Estimation Bias Mean      158.267\n",
      "evaluation/Estimation Bias Std       124.405\n",
      "evaluation/EB/Q_True Mean             29.4725\n",
      "evaluation/EB/Q_True Std              84.5866\n",
      "evaluation/EB/Q_Pred Mean            187.74\n",
      "evaluation/EB/Q_Pred Std              98.53\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           558.372\n",
      "evaluation/Actions Mean                0.113336\n",
      "evaluation/Actions Std                 0.58721\n",
      "evaluation/Actions Max                 1\n",
      "evaluation/Actions Min                -0.997904\n",
      "time/backward_policy (s)               8.43808\n",
      "time/backward_zf1 (s)                  9.71606\n",
      "time/backward_zf2 (s)                  9.45442\n",
      "time/data sampling (s)                 1.56965\n",
      "time/data storing (s)                  0.0821574\n",
      "time/evaluation sampling (s)           0.789045\n",
      "time/exploration sampling (s)          2.49929\n",
      "time/logging (s)                       0.00514919\n",
      "time/preback_alpha (s)                 0.00608682\n",
      "time/preback_policy (s)               15.392\n",
      "time/preback_start (s)                 0.959751\n",
      "time/preback_zf (s)                   36.4246\n",
      "time/saving (s)                        6.372e-06\n",
      "time/training (s)                     10.9915\n",
      "time/epoch (s)                        96.3278\n",
      "time/total (s)                       974.638\n",
      "Epoch                                  9\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:27:57.337656 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 10 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 65000\n",
      "trainer/ZF1 Loss                       0.300549\n",
      "trainer/ZF2 Loss                       0.320475\n",
      "trainer/ZF Expert Reward              16.1887\n",
      "trainer/ZF Policy Reward               5.60269\n",
      "trainer/ZF CHI2 Term                  13.9849\n",
      "trainer/Policy Loss                 -174.416\n",
      "trainer/expert_lambda Loss             9.154\n",
      "trainer/expert_lambda Value           14.22\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              31.6495\n",
      "trainer/Policy Param Norm             22.5631\n",
      "trainer/Zf1 Grad Norm               1102.1\n",
      "trainer/Zf1 Param Norm                67.0776\n",
      "trainer/Zf2 Grad Norm               1338.52\n",
      "trainer/Zf2 Param Norm                66.6319\n",
      "trainer/Z Expert Predictions Mean    260.405\n",
      "trainer/Z Expert Predictions Std      37.7088\n",
      "trainer/Z Expert Predictions Max     363.612\n",
      "trainer/Z Expert Predictions Min      38.5414\n",
      "trainer/Z Policy Predictions Mean    172.594\n",
      "trainer/Z Policy Predictions Std      91.4926\n",
      "trainer/Z Policy Predictions Max     304.839\n",
      "trainer/Z Policy Predictions Min      -1.85418\n",
      "trainer/Z Expert Targets Mean        244.216\n",
      "trainer/Z Expert Targets Std          38.6712\n",
      "trainer/Z Expert Targets Max         348.688\n",
      "trainer/Z Expert Targets Min          19.6927\n",
      "trainer/Z Policy Targets Mean        166.991\n",
      "trainer/Z Policy Targets Std          91.9403\n",
      "trainer/Z Policy Targets Max         303.752\n",
      "trainer/Z Policy Targets Min          -3.22511\n",
      "trainer/Log Pis Mean                  25.6777\n",
      "trainer/Log Pis Std                   12.532\n",
      "trainer/Policy mu Mean                 0.370037\n",
      "trainer/Policy mu Std                  1.58294\n",
      "trainer/Policy log std Mean           -1.42823\n",
      "trainer/Policy log std Std             0.552146\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        64517\n",
      "exploration/num paths total         1165\n",
      "evaluation/num steps total          8825\n",
      "evaluation/num paths total           110\n",
      "evaluation/path length Mean           91\n",
      "evaluation/path length Std            23.9541\n",
      "evaluation/path length Max           152\n",
      "evaluation/path length Min            74\n",
      "evaluation/Rewards Mean                5.13627\n",
      "evaluation/Rewards Std                 0.405947\n",
      "evaluation/Rewards Max                 6.43862\n",
      "evaluation/Rewards Min                 3.90112\n",
      "evaluation/Returns Mean              467.4\n",
      "evaluation/Returns Std               107.957\n",
      "evaluation/Returns Max               719.8\n",
      "evaluation/Returns Min               378.812\n",
      "evaluation/Estimation Bias Mean      154.706\n",
      "evaluation/Estimation Bias Std       105.393\n",
      "evaluation/EB/Q_True Mean             24.6115\n",
      "evaluation/EB/Q_True Std              62.8804\n",
      "evaluation/EB/Q_Pred Mean            179.318\n",
      "evaluation/EB/Q_Pred Std              94.1669\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           467.4\n",
      "evaluation/Actions Mean                0.13614\n",
      "evaluation/Actions Std                 0.615724\n",
      "evaluation/Actions Max                 1\n",
      "evaluation/Actions Min                -0.999979\n",
      "time/backward_policy (s)               8.47318\n",
      "time/backward_zf1 (s)                  9.75062\n",
      "time/backward_zf2 (s)                  9.5093\n",
      "time/data sampling (s)                 1.58314\n",
      "time/data storing (s)                  0.0828591\n",
      "time/evaluation sampling (s)           0.795191\n",
      "time/exploration sampling (s)          2.42134\n",
      "time/logging (s)                       0.00299002\n",
      "time/preback_alpha (s)                 0.00612692\n",
      "time/preback_policy (s)               15.3221\n",
      "time/preback_start (s)                 0.958991\n",
      "time/preback_zf (s)                   36.4201\n",
      "time/saving (s)                        4.33e-06\n",
      "time/training (s)                     10.9424\n",
      "time/epoch (s)                        96.2684\n",
      "time/total (s)                      1070.91\n",
      "Epoch                                 10\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:29:34.066026 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 11 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 70000\n",
      "trainer/ZF1 Loss                       1.82636\n",
      "trainer/ZF2 Loss                       1.25683\n",
      "trainer/ZF Expert Reward              17.0102\n",
      "trainer/ZF Policy Reward               5.04053\n",
      "trainer/ZF CHI2 Term                  15.6776\n",
      "trainer/Policy Loss                 -187.229\n",
      "trainer/expert_lambda Loss             8.18596\n",
      "trainer/expert_lambda Value           14.6707\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              30.4746\n",
      "trainer/Policy Param Norm             22.9687\n",
      "trainer/Zf1 Grad Norm               2020.93\n",
      "trainer/Zf1 Param Norm                69.3046\n",
      "trainer/Zf2 Grad Norm               1649.78\n",
      "trainer/Zf2 Param Norm                68.8871\n",
      "trainer/Z Expert Predictions Mean    296.467\n",
      "trainer/Z Expert Predictions Std      29.9655\n",
      "trainer/Z Expert Predictions Max     360.608\n",
      "trainer/Z Expert Predictions Min     200.616\n",
      "trainer/Z Policy Predictions Mean    184.241\n",
      "trainer/Z Policy Predictions Std      90.1903\n",
      "trainer/Z Policy Predictions Max     324.894\n",
      "trainer/Z Policy Predictions Min      -6.08865\n",
      "trainer/Z Expert Targets Mean        279.457\n",
      "trainer/Z Expert Targets Std          30.5996\n",
      "trainer/Z Expert Targets Max         348.356\n",
      "trainer/Z Expert Targets Min         172.794\n",
      "trainer/Z Policy Targets Mean        179.2\n",
      "trainer/Z Policy Targets Std          90.7586\n",
      "trainer/Z Policy Targets Max         324.173\n",
      "trainer/Z Policy Targets Min          -1.27484\n",
      "trainer/Log Pis Mean                  24.1025\n",
      "trainer/Log Pis Std                   11.3917\n",
      "trainer/Policy mu Mean                 0.400934\n",
      "trainer/Policy mu Std                  1.44274\n",
      "trainer/Policy log std Mean           -1.53362\n",
      "trainer/Policy log std Std             0.561024\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        69349\n",
      "exploration/num paths total         1208\n",
      "evaluation/num steps total          9985\n",
      "evaluation/num paths total           120\n",
      "evaluation/path length Mean          116\n",
      "evaluation/path length Std            24.4499\n",
      "evaluation/path length Max           169\n",
      "evaluation/path length Min            82\n",
      "evaluation/Rewards Mean                4.90501\n",
      "evaluation/Rewards Std                 0.528837\n",
      "evaluation/Rewards Max                 6.27638\n",
      "evaluation/Rewards Min                 3.1623\n",
      "evaluation/Returns Mean              568.981\n",
      "evaluation/Returns Std               141.544\n",
      "evaluation/Returns Max               878.102\n",
      "evaluation/Returns Min               361.588\n",
      "evaluation/Estimation Bias Mean      167.004\n",
      "evaluation/Estimation Bias Std       164.789\n",
      "evaluation/EB/Q_True Mean             40.0312\n",
      "evaluation/EB/Q_True Std             106.379\n",
      "evaluation/EB/Q_Pred Mean            207.035\n",
      "evaluation/EB/Q_Pred Std             111.107\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           568.981\n",
      "evaluation/Actions Mean                0.105002\n",
      "evaluation/Actions Std                 0.635622\n",
      "evaluation/Actions Max                 1\n",
      "evaluation/Actions Min                -0.999995\n",
      "time/backward_policy (s)               8.56319\n",
      "time/backward_zf1 (s)                  9.84161\n",
      "time/backward_zf2 (s)                  9.6169\n",
      "time/data sampling (s)                 1.6073\n",
      "time/data storing (s)                  0.0833738\n",
      "time/evaluation sampling (s)           0.78742\n",
      "time/exploration sampling (s)          2.48308\n",
      "time/logging (s)                       0.00190666\n",
      "time/preback_alpha (s)                 0.00610821\n",
      "time/preback_policy (s)               15.1882\n",
      "time/preback_start (s)                 0.964229\n",
      "time/preback_zf (s)                   36.4743\n",
      "time/saving (s)                        2.449e-06\n",
      "time/training (s)                     10.9008\n",
      "time/epoch (s)                        96.5184\n",
      "time/total (s)                      1167.43\n",
      "Epoch                                 11\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:31:10.745278 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 12 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 75000\n",
      "trainer/ZF1 Loss                       7.01842\n",
      "trainer/ZF2 Loss                       8.41339\n",
      "trainer/ZF Expert Reward              18.2648\n",
      "trainer/ZF Policy Reward               6.68129\n",
      "trainer/ZF CHI2 Term                  22.394\n",
      "trainer/Policy Loss                 -185.787\n",
      "trainer/expert_lambda Loss            11.4965\n",
      "trainer/expert_lambda Value           15.1209\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              38.6155\n",
      "trainer/Policy Param Norm             23.3751\n",
      "trainer/Zf1 Grad Norm               2991.72\n",
      "trainer/Zf1 Param Norm                71.8146\n",
      "trainer/Zf2 Grad Norm               3078.1\n",
      "trainer/Zf2 Param Norm                71.3474\n",
      "trainer/Z Expert Predictions Mean    365.631\n",
      "trainer/Z Expert Predictions Std      29.2152\n",
      "trainer/Z Expert Predictions Max     407.839\n",
      "trainer/Z Expert Predictions Min      62.0635\n",
      "trainer/Z Policy Predictions Mean    183.441\n",
      "trainer/Z Policy Predictions Std      93.3608\n",
      "trainer/Z Policy Predictions Max     341.878\n",
      "trainer/Z Policy Predictions Min      -5.93733\n",
      "trainer/Z Expert Targets Mean        347.367\n",
      "trainer/Z Expert Targets Std          29.8462\n",
      "trainer/Z Expert Targets Max         392.601\n",
      "trainer/Z Expert Targets Min          37.019\n",
      "trainer/Z Policy Targets Mean        176.76\n",
      "trainer/Z Policy Targets Std          92.736\n",
      "trainer/Z Policy Targets Max         337.45\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  24.8421\n",
      "trainer/Log Pis Std                   11.3447\n",
      "trainer/Policy mu Mean                 0.327961\n",
      "trainer/Policy mu Std                  1.43698\n",
      "trainer/Policy log std Mean           -1.59965\n",
      "trainer/Policy log std Std             0.586526\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        74219\n",
      "exploration/num paths total         1249\n",
      "evaluation/num steps total         10922\n",
      "evaluation/num paths total           130\n",
      "evaluation/path length Mean           93.7\n",
      "evaluation/path length Std            12.9232\n",
      "evaluation/path length Max           116\n",
      "evaluation/path length Min            69\n",
      "evaluation/Rewards Mean                5.36775\n",
      "evaluation/Rewards Std                 0.374974\n",
      "evaluation/Rewards Max                 6.6409\n",
      "evaluation/Rewards Min                 4.87707\n",
      "evaluation/Returns Mean              502.958\n",
      "evaluation/Returns Std                65.8969\n",
      "evaluation/Returns Max               621.79\n",
      "evaluation/Returns Min               374.926\n",
      "evaluation/Estimation Bias Mean      186.709\n",
      "evaluation/Estimation Bias Std       137.414\n",
      "evaluation/EB/Q_True Mean             28.1735\n",
      "evaluation/EB/Q_True Std              82.9851\n",
      "evaluation/EB/Q_Pred Mean            214.882\n",
      "evaluation/EB/Q_Pred Std             107.615\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           502.958\n",
      "evaluation/Actions Mean                0.127693\n",
      "evaluation/Actions Std                 0.562476\n",
      "evaluation/Actions Max                 0.999996\n",
      "evaluation/Actions Min                -0.999994\n",
      "time/backward_policy (s)               8.48857\n",
      "time/backward_zf1 (s)                  9.78465\n",
      "time/backward_zf2 (s)                  9.5115\n",
      "time/data sampling (s)                 1.63923\n",
      "time/data storing (s)                  0.0831193\n",
      "time/evaluation sampling (s)           0.522445\n",
      "time/exploration sampling (s)          2.47168\n",
      "time/logging (s)                       0.00170936\n",
      "time/preback_alpha (s)                 0.00613815\n",
      "time/preback_policy (s)               15.3671\n",
      "time/preback_start (s)                 0.974376\n",
      "time/preback_zf (s)                   36.4872\n",
      "time/saving (s)                        3.17e-06\n",
      "time/training (s)                     11.1339\n",
      "time/epoch (s)                        96.4716\n",
      "time/total (s)                      1263.9\n",
      "Epoch                                 12\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:32:47.087322 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 13 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 80000\n",
      "trainer/ZF1 Loss                       4.84882\n",
      "trainer/ZF2 Loss                       6.45555\n",
      "trainer/ZF Expert Reward              18.0724\n",
      "trainer/ZF Policy Reward               5.45754\n",
      "trainer/ZF CHI2 Term                  19.5908\n",
      "trainer/Policy Loss                 -172.351\n",
      "trainer/expert_lambda Loss             8.00545\n",
      "trainer/expert_lambda Value           15.5766\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              52.6152\n",
      "trainer/Policy Param Norm             23.8589\n",
      "trainer/Zf1 Grad Norm               3110.06\n",
      "trainer/Zf1 Param Norm                74.334\n",
      "trainer/Zf2 Grad Norm               2506.71\n",
      "trainer/Zf2 Param Norm                73.8414\n",
      "trainer/Z Expert Predictions Mean    421.573\n",
      "trainer/Z Expert Predictions Std      35.062\n",
      "trainer/Z Expert Predictions Max     467.36\n",
      "trainer/Z Expert Predictions Min      44.252\n",
      "trainer/Z Policy Predictions Mean    171.71\n",
      "trainer/Z Policy Predictions Std     107.186\n",
      "trainer/Z Policy Predictions Max     357.587\n",
      "trainer/Z Policy Predictions Min      -3.3239\n",
      "trainer/Z Expert Targets Mean        403.501\n",
      "trainer/Z Expert Targets Std          35.4156\n",
      "trainer/Z Expert Targets Max         451.758\n",
      "trainer/Z Expert Targets Min          19.4035\n",
      "trainer/Z Policy Targets Mean        166.252\n",
      "trainer/Z Policy Targets Std         107.57\n",
      "trainer/Z Policy Targets Max         353.109\n",
      "trainer/Z Policy Targets Min          -1.62004\n",
      "trainer/Log Pis Mean                  29.7626\n",
      "trainer/Log Pis Std                   11.5154\n",
      "trainer/Policy mu Mean                 0.476504\n",
      "trainer/Policy mu Std                  1.71122\n",
      "trainer/Policy log std Mean           -1.55792\n",
      "trainer/Policy log std Std             0.648488\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        79144\n",
      "exploration/num paths total         1290\n",
      "evaluation/num steps total         12233\n",
      "evaluation/num paths total           140\n",
      "evaluation/path length Mean          131.1\n",
      "evaluation/path length Std            22.8055\n",
      "evaluation/path length Max           165\n",
      "evaluation/path length Min            96\n",
      "evaluation/Rewards Mean                5.21896\n",
      "evaluation/Rewards Std                 0.405274\n",
      "evaluation/Rewards Max                 6.70544\n",
      "evaluation/Rewards Min                 3.35806\n",
      "evaluation/Returns Mean              684.205\n",
      "evaluation/Returns Std               127.693\n",
      "evaluation/Returns Max               869.649\n",
      "evaluation/Returns Min               500.332\n",
      "evaluation/Estimation Bias Mean      217.376\n",
      "evaluation/Estimation Bias Std       148.933\n",
      "evaluation/EB/Q_True Mean             34.8395\n",
      "evaluation/EB/Q_True Std             100.362\n",
      "evaluation/EB/Q_Pred Mean            252.216\n",
      "evaluation/EB/Q_Pred Std             117.091\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           684.205\n",
      "evaluation/Actions Mean                0.0652532\n",
      "evaluation/Actions Std                 0.597036\n",
      "evaluation/Actions Max                 1\n",
      "evaluation/Actions Min                -0.999981\n",
      "time/backward_policy (s)               8.67928\n",
      "time/backward_zf1 (s)                  9.97063\n",
      "time/backward_zf2 (s)                  9.72585\n",
      "time/data sampling (s)                 1.61345\n",
      "time/data storing (s)                  0.0823363\n",
      "time/evaluation sampling (s)           0.749979\n",
      "time/exploration sampling (s)          2.45042\n",
      "time/logging (s)                       0.00405389\n",
      "time/preback_alpha (s)                 0.00601608\n",
      "time/preback_policy (s)               14.8616\n",
      "time/preback_start (s)                 0.971011\n",
      "time/preback_zf (s)                   36.3253\n",
      "time/saving (s)                        6.45e-06\n",
      "time/training (s)                     10.6995\n",
      "time/epoch (s)                        96.1395\n",
      "time/total (s)                      1360.05\n",
      "Epoch                                 13\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:34:24.070989 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 14 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 85000\n",
      "trainer/ZF1 Loss                       7.91872\n",
      "trainer/ZF2 Loss                       7.7757\n",
      "trainer/ZF Expert Reward              18.4013\n",
      "trainer/ZF Policy Reward               5.58017\n",
      "trainer/ZF CHI2 Term                  21.8936\n",
      "trainer/Policy Loss                 -193.863\n",
      "trainer/expert_lambda Loss             8.2695\n",
      "trainer/expert_lambda Value           16.0293\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              44.984\n",
      "trainer/Policy Param Norm             24.3585\n",
      "trainer/Zf1 Grad Norm               3032.5\n",
      "trainer/Zf1 Param Norm                76.5853\n",
      "trainer/Zf2 Grad Norm               2015.51\n",
      "trainer/Zf2 Param Norm                76.0951\n",
      "trainer/Z Expert Predictions Mean    445.33\n",
      "trainer/Z Expert Predictions Std      34.7901\n",
      "trainer/Z Expert Predictions Max     499.236\n",
      "trainer/Z Expert Predictions Min      45.9354\n",
      "trainer/Z Policy Predictions Mean    191.815\n",
      "trainer/Z Policy Predictions Std     113.884\n",
      "trainer/Z Policy Predictions Max     376.615\n",
      "trainer/Z Policy Predictions Min       5.61333\n",
      "trainer/Z Expert Targets Mean        426.929\n",
      "trainer/Z Expert Targets Std          35.421\n",
      "trainer/Z Expert Targets Max         481.784\n",
      "trainer/Z Expert Targets Min          14.6151\n",
      "trainer/Z Policy Targets Mean        186.235\n",
      "trainer/Z Policy Targets Std         113.924\n",
      "trainer/Z Policy Targets Max         372.853\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  29.16\n",
      "trainer/Log Pis Std                   12.3466\n",
      "trainer/Policy mu Mean                 0.420108\n",
      "trainer/Policy mu Std                  1.69372\n",
      "trainer/Policy log std Mean           -1.6193\n",
      "trainer/Policy log std Std             0.664868\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        84183\n",
      "exploration/num paths total         1323\n",
      "evaluation/num steps total         13644\n",
      "evaluation/num paths total           150\n",
      "evaluation/path length Mean          141.1\n",
      "evaluation/path length Std            26.6024\n",
      "evaluation/path length Max           171\n",
      "evaluation/path length Min            92\n",
      "evaluation/Rewards Mean                4.77869\n",
      "evaluation/Rewards Std                 0.395693\n",
      "evaluation/Rewards Max                 5.87594\n",
      "evaluation/Rewards Min                 2.96508\n",
      "evaluation/Returns Mean              674.273\n",
      "evaluation/Returns Std               143.591\n",
      "evaluation/Returns Max               850.961\n",
      "evaluation/Returns Min               415.874\n",
      "evaluation/Estimation Bias Mean      183.581\n",
      "evaluation/Estimation Bias Std       166.816\n",
      "evaluation/EB/Q_True Mean             31.7215\n",
      "evaluation/EB/Q_True Std              94.2101\n",
      "evaluation/EB/Q_Pred Mean            215.302\n",
      "evaluation/EB/Q_Pred Std             128.129\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           674.273\n",
      "evaluation/Actions Mean                0.183835\n",
      "evaluation/Actions Std                 0.557809\n",
      "evaluation/Actions Max                 1\n",
      "evaluation/Actions Min                -0.999989\n",
      "time/backward_policy (s)               8.6328\n",
      "time/backward_zf1 (s)                  9.92008\n",
      "time/backward_zf2 (s)                  9.6907\n",
      "time/data sampling (s)                 1.63507\n",
      "time/data storing (s)                  0.0830201\n",
      "time/evaluation sampling (s)           0.750477\n",
      "time/exploration sampling (s)          2.54759\n",
      "time/logging (s)                       0.00311469\n",
      "time/preback_alpha (s)                 0.00614205\n",
      "time/preback_policy (s)               15.1205\n",
      "time/preback_start (s)                 0.972132\n",
      "time/preback_zf (s)                   36.4751\n",
      "time/saving (s)                        2.957e-06\n",
      "time/training (s)                     10.9363\n",
      "time/epoch (s)                        96.773\n",
      "time/total (s)                      1456.82\n",
      "Epoch                                 14\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:36:02.525461 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 15 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 90000\n",
      "trainer/ZF1 Loss                      10.8301\n",
      "trainer/ZF2 Loss                       9.30105\n",
      "trainer/ZF Expert Reward              17.3948\n",
      "trainer/ZF Policy Reward               4.9773\n",
      "trainer/ZF CHI2 Term                  22.985\n",
      "trainer/Policy Loss                 -193.576\n",
      "trainer/expert_lambda Loss             6.86183\n",
      "trainer/expert_lambda Value           16.4821\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              42.806\n",
      "trainer/Policy Param Norm             24.8606\n",
      "trainer/Zf1 Grad Norm               3136.25\n",
      "trainer/Zf1 Param Norm                78.9202\n",
      "trainer/Zf2 Grad Norm               2484.16\n",
      "trainer/Zf2 Param Norm                78.4427\n",
      "trainer/Z Expert Predictions Mean    454.617\n",
      "trainer/Z Expert Predictions Std      37.556\n",
      "trainer/Z Expert Predictions Max     529.959\n",
      "trainer/Z Expert Predictions Min     352.289\n",
      "trainer/Z Policy Predictions Mean    190.089\n",
      "trainer/Z Policy Predictions Std     117.672\n",
      "trainer/Z Policy Predictions Max     400.791\n",
      "trainer/Z Policy Predictions Min      -1.29802\n",
      "trainer/Z Expert Targets Mean        437.222\n",
      "trainer/Z Expert Targets Std          37.6066\n",
      "trainer/Z Expert Targets Max         513.732\n",
      "trainer/Z Expert Targets Min         332.739\n",
      "trainer/Z Policy Targets Mean        185.112\n",
      "trainer/Z Policy Targets Std         117.338\n",
      "trainer/Z Policy Targets Max         403.987\n",
      "trainer/Z Policy Targets Min          -3.98002\n",
      "trainer/Log Pis Mean                  27.4472\n",
      "trainer/Log Pis Std                   11.185\n",
      "trainer/Policy mu Mean                 0.406342\n",
      "trainer/Policy mu Std                  1.64945\n",
      "trainer/Policy log std Mean           -1.60591\n",
      "trainer/Policy log std Std             0.658879\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        89071\n",
      "exploration/num paths total         1353\n",
      "evaluation/num steps total         15321\n",
      "evaluation/num paths total           160\n",
      "evaluation/path length Mean          167.7\n",
      "evaluation/path length Std            20.1745\n",
      "evaluation/path length Max           195\n",
      "evaluation/path length Min           125\n",
      "evaluation/Rewards Mean                5.05096\n",
      "evaluation/Rewards Std                 0.42449\n",
      "evaluation/Rewards Max                 6.56507\n",
      "evaluation/Rewards Min                 3.0757\n",
      "evaluation/Returns Mean              847.045\n",
      "evaluation/Returns Std                75.5683\n",
      "evaluation/Returns Max               953.053\n",
      "evaluation/Returns Min               664.088\n",
      "evaluation/Estimation Bias Mean      221.535\n",
      "evaluation/Estimation Bias Std       157.189\n",
      "evaluation/EB/Q_True Mean             31.6028\n",
      "evaluation/EB/Q_True Std              96.6372\n",
      "evaluation/EB/Q_Pred Mean            253.138\n",
      "evaluation/EB/Q_Pred Std             130.026\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           847.045\n",
      "evaluation/Actions Mean                0.109599\n",
      "evaluation/Actions Std                 0.578986\n",
      "evaluation/Actions Max                 1\n",
      "evaluation/Actions Min                -0.99999\n",
      "time/backward_policy (s)               8.81522\n",
      "time/backward_zf1 (s)                 10.2004\n",
      "time/backward_zf2 (s)                  9.945\n",
      "time/data sampling (s)                 1.67769\n",
      "time/data storing (s)                  0.0850934\n",
      "time/evaluation sampling (s)           0.966898\n",
      "time/exploration sampling (s)          2.62073\n",
      "time/logging (s)                       0.00546403\n",
      "time/preback_alpha (s)                 0.00621248\n",
      "time/preback_policy (s)               15.1257\n",
      "time/preback_start (s)                 0.984865\n",
      "time/preback_zf (s)                   36.6797\n",
      "time/saving (s)                        5.941e-06\n",
      "time/training (s)                     11.1311\n",
      "time/epoch (s)                        98.2441\n",
      "time/total (s)                      1555.07\n",
      "Epoch                                 15\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:37:39.261545 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 16 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 95000\n",
      "trainer/ZF1 Loss                       8.17854\n",
      "trainer/ZF2 Loss                      10.516\n",
      "trainer/ZF Expert Reward              18.5771\n",
      "trainer/ZF Policy Reward               5.70507\n",
      "trainer/ZF CHI2 Term                  23.2329\n",
      "trainer/Policy Loss                 -223.453\n",
      "trainer/expert_lambda Loss             5.94222\n",
      "trainer/expert_lambda Value           16.9322\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              55.0118\n",
      "trainer/Policy Param Norm             25.3296\n",
      "trainer/Zf1 Grad Norm               2709.3\n",
      "trainer/Zf1 Param Norm                81.1082\n",
      "trainer/Zf2 Grad Norm               2664.45\n",
      "trainer/Zf2 Param Norm                80.6318\n",
      "trainer/Z Expert Predictions Mean    480.133\n",
      "trainer/Z Expert Predictions Std      27.5234\n",
      "trainer/Z Expert Predictions Max     530.058\n",
      "trainer/Z Expert Predictions Min     395.018\n",
      "trainer/Z Policy Predictions Mean    218.723\n",
      "trainer/Z Policy Predictions Std     127.027\n",
      "trainer/Z Policy Predictions Max     451.07\n",
      "trainer/Z Policy Predictions Min       0.536321\n",
      "trainer/Z Expert Targets Mean        461.556\n",
      "trainer/Z Expert Targets Std          27.8102\n",
      "trainer/Z Expert Targets Max         516.545\n",
      "trainer/Z Expert Targets Min         376.593\n",
      "trainer/Z Policy Targets Mean        213.018\n",
      "trainer/Z Policy Targets Std         126.919\n",
      "trainer/Z Policy Targets Max         419.584\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  30.0187\n",
      "trainer/Log Pis Std                   12.0237\n",
      "trainer/Policy mu Mean                 0.334101\n",
      "trainer/Policy mu Std                  1.71465\n",
      "trainer/Policy log std Mean           -1.79245\n",
      "trainer/Policy log std Std             0.794722\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        94178\n",
      "exploration/num paths total         1376\n",
      "evaluation/num steps total         16219\n",
      "evaluation/num paths total           170\n",
      "evaluation/path length Mean           89.8\n",
      "evaluation/path length Std            17.6057\n",
      "evaluation/path length Max           126\n",
      "evaluation/path length Min            65\n",
      "evaluation/Rewards Mean                5.27949\n",
      "evaluation/Rewards Std                 0.356334\n",
      "evaluation/Rewards Max                 6.44527\n",
      "evaluation/Rewards Min                 4.83936\n",
      "evaluation/Returns Mean              474.098\n",
      "evaluation/Returns Std                91.7893\n",
      "evaluation/Returns Max               645.264\n",
      "evaluation/Returns Min               329.836\n",
      "evaluation/Estimation Bias Mean      226.773\n",
      "evaluation/Estimation Bias Std       152.941\n",
      "evaluation/EB/Q_True Mean             21.0791\n",
      "evaluation/EB/Q_True Std              60.0251\n",
      "evaluation/EB/Q_Pred Mean            247.852\n",
      "evaluation/EB/Q_Pred Std             140.266\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           474.098\n",
      "evaluation/Actions Mean                0.0785663\n",
      "evaluation/Actions Std                 0.612225\n",
      "evaluation/Actions Max                 1\n",
      "evaluation/Actions Min                -0.999984\n",
      "time/backward_policy (s)               8.53708\n",
      "time/backward_zf1 (s)                  9.81269\n",
      "time/backward_zf2 (s)                  9.56526\n",
      "time/data sampling (s)                 1.64476\n",
      "time/data storing (s)                  0.0824162\n",
      "time/evaluation sampling (s)           0.72315\n",
      "time/exploration sampling (s)          2.51559\n",
      "time/logging (s)                       0.00395299\n",
      "time/preback_alpha (s)                 0.00615065\n",
      "time/preback_policy (s)               15.2525\n",
      "time/preback_start (s)                 0.966377\n",
      "time/preback_zf (s)                   36.4526\n",
      "time/saving (s)                        5.452e-06\n",
      "time/training (s)                     10.9637\n",
      "time/epoch (s)                        96.5262\n",
      "time/total (s)                      1651.6\n",
      "Epoch                                 16\n",
      "---------------------------------  --------------\n",
      "2024-10-25 21:39:15.994067 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 17 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 100000\n",
      "trainer/ZF1 Loss                       12.6771\n",
      "trainer/ZF2 Loss                       12.5948\n",
      "trainer/ZF Expert Reward               17.2565\n",
      "trainer/ZF Policy Reward                3.88514\n",
      "trainer/ZF CHI2 Term                   25.0068\n",
      "trainer/Policy Loss                  -223.604\n",
      "trainer/expert_lambda Loss              6.47722\n",
      "trainer/expert_lambda Value            17.379\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               63.9475\n",
      "trainer/Policy Param Norm              25.8058\n",
      "trainer/Zf1 Grad Norm                4983.78\n",
      "trainer/Zf1 Param Norm                 83.4968\n",
      "trainer/Zf2 Grad Norm                5586.74\n",
      "trainer/Zf2 Param Norm                 83.0968\n",
      "trainer/Z Expert Predictions Mean     501.059\n",
      "trainer/Z Expert Predictions Std       23.1088\n",
      "trainer/Z Expert Predictions Max      549.713\n",
      "trainer/Z Expert Predictions Min      426.385\n",
      "trainer/Z Policy Predictions Mean     219.986\n",
      "trainer/Z Policy Predictions Std      133.51\n",
      "trainer/Z Policy Predictions Max      438.65\n",
      "trainer/Z Policy Predictions Min        2.43248\n",
      "trainer/Z Expert Targets Mean         483.803\n",
      "trainer/Z Expert Targets Std           22.9672\n",
      "trainer/Z Expert Targets Max          535.875\n",
      "trainer/Z Expert Targets Min          413.536\n",
      "trainer/Z Policy Targets Mean         216.101\n",
      "trainer/Z Policy Targets Std          134.046\n",
      "trainer/Z Policy Targets Max          427.684\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   29.8802\n",
      "trainer/Log Pis Std                    11.5867\n",
      "trainer/Policy mu Mean                  0.374622\n",
      "trainer/Policy mu Std                   1.75936\n",
      "trainer/Policy log std Mean            -1.75834\n",
      "trainer/Policy log std Std              0.75336\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total         98342\n",
      "exploration/num paths total          1401\n",
      "evaluation/num steps total          18167\n",
      "evaluation/num paths total            180\n",
      "evaluation/path length Mean           194.8\n",
      "evaluation/path length Std             43.053\n",
      "evaluation/path length Max            244\n",
      "evaluation/path length Min            114\n",
      "evaluation/Rewards Mean                 5.24835\n",
      "evaluation/Rewards Std                  0.295427\n",
      "evaluation/Rewards Max                  6.49652\n",
      "evaluation/Rewards Min                  4.14046\n",
      "evaluation/Returns Mean              1022.38\n",
      "evaluation/Returns Std                227.671\n",
      "evaluation/Returns Max               1309.04\n",
      "evaluation/Returns Min                585.929\n",
      "evaluation/Estimation Bias Mean       307.163\n",
      "evaluation/Estimation Bias Std        175.361\n",
      "evaluation/EB/Q_True Mean              31.4848\n",
      "evaluation/EB/Q_True Std               93.4319\n",
      "evaluation/EB/Q_Pred Mean             338.648\n",
      "evaluation/EB/Q_Pred Std              148.179\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           1022.38\n",
      "evaluation/Actions Mean                 0.039949\n",
      "evaluation/Actions Std                  0.588076\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999787\n",
      "time/backward_policy (s)                8.11878\n",
      "time/backward_zf1 (s)                   9.38385\n",
      "time/backward_zf2 (s)                   9.08832\n",
      "time/data sampling (s)                  1.61368\n",
      "time/data storing (s)                   0.0821416\n",
      "time/evaluation sampling (s)            1.44726\n",
      "time/exploration sampling (s)           2.45837\n",
      "time/logging (s)                        0.0050564\n",
      "time/preback_alpha (s)                  0.00618483\n",
      "time/preback_policy (s)                15.8179\n",
      "time/preback_start (s)                  0.95465\n",
      "time/preback_zf (s)                    36.3361\n",
      "time/saving (s)                         4.338e-06\n",
      "time/training (s)                      11.2136\n",
      "time/epoch (s)                         96.526\n",
      "time/total (s)                       1748.13\n",
      "Epoch                                  17\n",
      "---------------------------------  ---------------\n",
      "2024-10-25 21:40:54.099784 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 18 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 105000\n",
      "trainer/ZF1 Loss                       12.6574\n",
      "trainer/ZF2 Loss                       11.044\n",
      "trainer/ZF Expert Reward               20.4202\n",
      "trainer/ZF Policy Reward                5.51647\n",
      "trainer/ZF CHI2 Term                   26.8323\n",
      "trainer/Policy Loss                  -243.733\n",
      "trainer/expert_lambda Loss              8.51226\n",
      "trainer/expert_lambda Value            17.8222\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               56.8025\n",
      "trainer/Policy Param Norm              26.2571\n",
      "trainer/Zf1 Grad Norm                5281.44\n",
      "trainer/Zf1 Param Norm                 86.1034\n",
      "trainer/Zf2 Grad Norm                3229.22\n",
      "trainer/Zf2 Param Norm                 85.6955\n",
      "trainer/Z Expert Predictions Mean     553.98\n",
      "trainer/Z Expert Predictions Std       51.3271\n",
      "trainer/Z Expert Predictions Max      596.634\n",
      "trainer/Z Expert Predictions Min       18.6778\n",
      "trainer/Z Policy Predictions Mean     241.437\n",
      "trainer/Z Policy Predictions Std      149.685\n",
      "trainer/Z Policy Predictions Max      485.155\n",
      "trainer/Z Policy Predictions Min       -2.94211\n",
      "trainer/Z Expert Targets Mean         533.559\n",
      "trainer/Z Expert Targets Std           52.3137\n",
      "trainer/Z Expert Targets Max          579.512\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean         235.92\n",
      "trainer/Z Policy Targets Std          148.907\n",
      "trainer/Z Policy Targets Max          474.187\n",
      "trainer/Z Policy Targets Min           -6.46425\n",
      "trainer/Log Pis Mean                   29.7786\n",
      "trainer/Log Pis Std                    10.6296\n",
      "trainer/Policy mu Mean                  0.385671\n",
      "trainer/Policy mu Std                   1.77443\n",
      "trainer/Policy log std Mean            -1.77523\n",
      "trainer/Policy log std Std              0.785445\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        103080\n",
      "exploration/num paths total          1416\n",
      "evaluation/num steps total          20928\n",
      "evaluation/num paths total            190\n",
      "evaluation/path length Mean           276.1\n",
      "evaluation/path length Std             53.741\n",
      "evaluation/path length Max            372\n",
      "evaluation/path length Min            217\n",
      "evaluation/Rewards Mean                 5.27858\n",
      "evaluation/Rewards Std                  0.31345\n",
      "evaluation/Rewards Max                  6.37056\n",
      "evaluation/Rewards Min                  3.56278\n",
      "evaluation/Returns Mean              1457.42\n",
      "evaluation/Returns Std                267.87\n",
      "evaluation/Returns Max               1973.19\n",
      "evaluation/Returns Min               1151.53\n",
      "evaluation/Estimation Bias Mean       326.629\n",
      "evaluation/Estimation Bias Std        201.14\n",
      "evaluation/EB/Q_True Mean              53.3913\n",
      "evaluation/EB/Q_True Std              143.488\n",
      "evaluation/EB/Q_Pred Mean             380.021\n",
      "evaluation/EB/Q_Pred Std              152.334\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           1457.42\n",
      "evaluation/Actions Mean                 0.0852862\n",
      "evaluation/Actions Std                  0.579257\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999942\n",
      "time/backward_policy (s)                8.66208\n",
      "time/backward_zf1 (s)                   9.95656\n",
      "time/backward_zf2 (s)                   9.7173\n",
      "time/data sampling (s)                  1.67481\n",
      "time/data storing (s)                   0.0828558\n",
      "time/evaluation sampling (s)            1.71572\n",
      "time/exploration sampling (s)           2.54141\n",
      "time/logging (s)                        0.00449492\n",
      "time/preback_alpha (s)                  0.00614722\n",
      "time/preback_policy (s)                15.0617\n",
      "time/preback_start (s)                  0.971671\n",
      "time/preback_zf (s)                    36.5479\n",
      "time/saving (s)                         2.973e-06\n",
      "time/training (s)                      10.9544\n",
      "time/epoch (s)                         97.897\n",
      "time/total (s)                       1846.03\n",
      "Epoch                                  18\n",
      "---------------------------------  ---------------\n",
      "2024-10-25 21:42:33.126340 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 19 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 110000\n",
      "trainer/ZF1 Loss                       19.713\n",
      "trainer/ZF2 Loss                       21.9494\n",
      "trainer/ZF Expert Reward               19.5481\n",
      "trainer/ZF Policy Reward                4.58106\n",
      "trainer/ZF CHI2 Term                   34.4036\n",
      "trainer/Policy Loss                  -248.098\n",
      "trainer/expert_lambda Loss              6.96852\n",
      "trainer/expert_lambda Value            18.2639\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               61.1709\n",
      "trainer/Policy Param Norm              26.7022\n",
      "trainer/Zf1 Grad Norm                5056.52\n",
      "trainer/Zf1 Param Norm                 88.9461\n",
      "trainer/Zf2 Grad Norm                5171.16\n",
      "trainer/Zf2 Param Norm                 88.5398\n",
      "trainer/Z Expert Predictions Mean     608.159\n",
      "trainer/Z Expert Predictions Std       37.7871\n",
      "trainer/Z Expert Predictions Max      657.993\n",
      "trainer/Z Expert Predictions Min      216.059\n",
      "trainer/Z Policy Predictions Mean     243.781\n",
      "trainer/Z Policy Predictions Std      157.291\n",
      "trainer/Z Policy Predictions Max      544.506\n",
      "trainer/Z Policy Predictions Min        6.57606\n",
      "trainer/Z Expert Targets Mean         588.611\n",
      "trainer/Z Expert Targets Std           36.5148\n",
      "trainer/Z Expert Targets Max          641.411\n",
      "trainer/Z Expert Targets Min          218.532\n",
      "trainer/Z Policy Targets Mean         239.2\n",
      "trainer/Z Policy Targets Std          159.149\n",
      "trainer/Z Policy Targets Max          541.496\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   33.5007\n",
      "trainer/Log Pis Std                    12.9383\n",
      "trainer/Policy mu Mean                  0.493707\n",
      "trainer/Policy mu Std                   1.97885\n",
      "trainer/Policy log std Mean            -1.77465\n",
      "trainer/Policy log std Std              0.857191\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        108911\n",
      "exploration/num paths total          1430\n",
      "evaluation/num steps total          24052\n",
      "evaluation/num paths total            200\n",
      "evaluation/path length Mean           312.4\n",
      "evaluation/path length Std            163.119\n",
      "evaluation/path length Max            547\n",
      "evaluation/path length Min             86\n",
      "evaluation/Rewards Mean                 5.30236\n",
      "evaluation/Rewards Std                  0.23391\n",
      "evaluation/Rewards Max                  6.5995\n",
      "evaluation/Rewards Min                  4.88476\n",
      "evaluation/Returns Mean              1656.46\n",
      "evaluation/Returns Std                858.507\n",
      "evaluation/Returns Max               2889.51\n",
      "evaluation/Returns Min                444.742\n",
      "evaluation/Estimation Bias Mean       358.941\n",
      "evaluation/Estimation Bias Std        240.207\n",
      "evaluation/EB/Q_True Mean              76.1788\n",
      "evaluation/EB/Q_True Std              172.866\n",
      "evaluation/EB/Q_Pred Mean             435.119\n",
      "evaluation/EB/Q_Pred Std              155.702\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           1656.46\n",
      "evaluation/Actions Mean                 0.0678664\n",
      "evaluation/Actions Std                  0.554206\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.99996\n",
      "time/backward_policy (s)                8.35278\n",
      "time/backward_zf1 (s)                   9.64795\n",
      "time/backward_zf2 (s)                   9.33575\n",
      "time/data sampling (s)                  1.69944\n",
      "time/data storing (s)                   0.083032\n",
      "time/evaluation sampling (s)            2.80975\n",
      "time/exploration sampling (s)           2.50474\n",
      "time/logging (s)                        0.00831454\n",
      "time/preback_alpha (s)                  0.00617041\n",
      "time/preback_policy (s)                15.6896\n",
      "time/preback_start (s)                  0.96581\n",
      "time/preback_zf (s)                    36.4589\n",
      "time/saving (s)                         5.158e-06\n",
      "time/training (s)                      11.258\n",
      "time/epoch (s)                         98.8202\n",
      "time/total (s)                       1944.85\n",
      "Epoch                                  19\n",
      "---------------------------------  ---------------\n",
      "2024-10-25 21:44:11.878032 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 20 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 115000\n",
      "trainer/ZF1 Loss                       29.7503\n",
      "trainer/ZF2 Loss                       33.4617\n",
      "trainer/ZF Expert Reward               20.3445\n",
      "trainer/ZF Policy Reward                6.60334\n",
      "trainer/ZF CHI2 Term                   45.2761\n",
      "trainer/Policy Loss                  -287.027\n",
      "trainer/expert_lambda Loss             10.1309\n",
      "trainer/expert_lambda Value            18.6982\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               70.2214\n",
      "trainer/Policy Param Norm              27.1356\n",
      "trainer/Zf1 Grad Norm                4057.35\n",
      "trainer/Zf1 Param Norm                 91.6356\n",
      "trainer/Zf2 Grad Norm                4861.15\n",
      "trainer/Zf2 Param Norm                 91.3415\n",
      "trainer/Z Expert Predictions Mean     676.578\n",
      "trainer/Z Expert Predictions Std       40.4785\n",
      "trainer/Z Expert Predictions Max      727.635\n",
      "trainer/Z Expert Predictions Min      229.81\n",
      "trainer/Z Policy Predictions Mean     284.429\n",
      "trainer/Z Policy Predictions Std      179.883\n",
      "trainer/Z Policy Predictions Max      652.953\n",
      "trainer/Z Policy Predictions Min       12.3298\n",
      "trainer/Z Expert Targets Mean         656.234\n",
      "trainer/Z Expert Targets Std           40.978\n",
      "trainer/Z Expert Targets Max          709.717\n",
      "trainer/Z Expert Targets Min          205.119\n",
      "trainer/Z Policy Targets Mean         277.826\n",
      "trainer/Z Policy Targets Std          179.013\n",
      "trainer/Z Policy Targets Max          645.41\n",
      "trainer/Z Policy Targets Min           -4.76384\n",
      "trainer/Log Pis Mean                   34.8398\n",
      "trainer/Log Pis Std                    12.4966\n",
      "trainer/Policy mu Mean                  0.496806\n",
      "trainer/Policy mu Std                   1.91604\n",
      "trainer/Policy log std Mean            -1.86368\n",
      "trainer/Policy log std Std              0.871972\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        109217\n",
      "exploration/num paths total          1431\n",
      "evaluation/num steps total          27327\n",
      "evaluation/num paths total            210\n",
      "evaluation/path length Mean           327.5\n",
      "evaluation/path length Std            126.328\n",
      "evaluation/path length Max            602\n",
      "evaluation/path length Min            205\n",
      "evaluation/Rewards Mean                 5.34021\n",
      "evaluation/Rewards Std                  0.201494\n",
      "evaluation/Rewards Max                  6.52908\n",
      "evaluation/Rewards Min                  4.86194\n",
      "evaluation/Returns Mean              1748.92\n",
      "evaluation/Returns Std                670.683\n",
      "evaluation/Returns Max               3209.76\n",
      "evaluation/Returns Min               1092.98\n",
      "evaluation/Estimation Bias Mean       466.34\n",
      "evaluation/Estimation Bias Std        288.82\n",
      "evaluation/EB/Q_True Mean              82.2581\n",
      "evaluation/EB/Q_True Std              180.988\n",
      "evaluation/EB/Q_Pred Mean             548.598\n",
      "evaluation/EB/Q_Pred Std              206.092\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           1748.92\n",
      "evaluation/Actions Mean                 0.0485894\n",
      "evaluation/Actions Std                  0.557016\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999846\n",
      "time/backward_policy (s)                8.52191\n",
      "time/backward_zf1 (s)                   9.79331\n",
      "time/backward_zf2 (s)                   9.51055\n",
      "time/data sampling (s)                  1.63425\n",
      "time/data storing (s)                   0.0819566\n",
      "time/evaluation sampling (s)            2.86737\n",
      "time/exploration sampling (s)           2.49413\n",
      "time/logging (s)                        0.007267\n",
      "time/preback_alpha (s)                  0.00610769\n",
      "time/preback_policy (s)                15.2657\n",
      "time/preback_start (s)                  0.970169\n",
      "time/preback_zf (s)                    36.4175\n",
      "time/saving (s)                         5.573e-06\n",
      "time/training (s)                      10.9714\n",
      "time/epoch (s)                         98.5416\n",
      "time/total (s)                       2043.39\n",
      "Epoch                                  20\n",
      "---------------------------------  ---------------\n",
      "2024-10-25 21:45:52.901917 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 21 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 120000\n",
      "trainer/ZF1 Loss                       35.1517\n",
      "trainer/ZF2 Loss                       36.0676\n",
      "trainer/ZF Expert Reward               20.3667\n",
      "trainer/ZF Policy Reward                4.5723\n",
      "trainer/ZF CHI2 Term                   48.7578\n",
      "trainer/Policy Loss                  -333.623\n",
      "trainer/expert_lambda Loss              6.83488\n",
      "trainer/expert_lambda Value            19.1297\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               94.2714\n",
      "trainer/Policy Param Norm              27.5829\n",
      "trainer/Zf1 Grad Norm                7508.59\n",
      "trainer/Zf1 Param Norm                 94.0975\n",
      "trainer/Zf2 Grad Norm                7393.31\n",
      "trainer/Zf2 Param Norm                 93.8783\n",
      "trainer/Z Expert Predictions Mean     731.432\n",
      "trainer/Z Expert Predictions Std       36.7606\n",
      "trainer/Z Expert Predictions Max      785.611\n",
      "trainer/Z Expert Predictions Min      545.573\n",
      "trainer/Z Policy Predictions Mean     327.321\n",
      "trainer/Z Policy Predictions Std      190.554\n",
      "trainer/Z Policy Predictions Max      690.39\n",
      "trainer/Z Policy Predictions Min        2.56704\n",
      "trainer/Z Expert Targets Mean         711.066\n",
      "trainer/Z Expert Targets Std           36.6174\n",
      "trainer/Z Expert Targets Max          770.178\n",
      "trainer/Z Expert Targets Min          531.986\n",
      "trainer/Z Policy Targets Mean         322.749\n",
      "trainer/Z Policy Targets Std          191.256\n",
      "trainer/Z Policy Targets Max          685.564\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   34.9867\n",
      "trainer/Log Pis Std                    12.2685\n",
      "trainer/Policy mu Mean                  0.383173\n",
      "trainer/Policy mu Std                   1.81054\n",
      "trainer/Policy log std Mean            -1.95591\n",
      "trainer/Policy log std Std              0.880929\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        117759\n",
      "exploration/num paths total          1441\n",
      "evaluation/num steps total          36906\n",
      "evaluation/num paths total            220\n",
      "evaluation/path length Mean           957.9\n",
      "evaluation/path length Std            126.3\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            579\n",
      "evaluation/Rewards Mean                 5.341\n",
      "evaluation/Rewards Std                  0.0852194\n",
      "evaluation/Rewards Max                  6.26144\n",
      "evaluation/Rewards Min                  4.89747\n",
      "evaluation/Returns Mean              5116.14\n",
      "evaluation/Returns Std                668.641\n",
      "evaluation/Returns Max               5342.74\n",
      "evaluation/Returns Min               3110.23\n",
      "evaluation/Estimation Bias Mean       582.496\n",
      "evaluation/Estimation Bias Std        174.374\n",
      "evaluation/EB/Q_True Mean              50.3245\n",
      "evaluation/EB/Q_True Std              151.348\n",
      "evaluation/EB/Q_Pred Mean             632.82\n",
      "evaluation/EB/Q_Pred Std               73.8468\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5116.14\n",
      "evaluation/Actions Mean                 0.0416274\n",
      "evaluation/Actions Std                  0.534646\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999952\n",
      "time/backward_policy (s)                8.37962\n",
      "time/backward_zf1 (s)                   9.70598\n",
      "time/backward_zf2 (s)                   9.42061\n",
      "time/data sampling (s)                  1.76029\n",
      "time/data storing (s)                   0.0832872\n",
      "time/evaluation sampling (s)            4.59077\n",
      "time/exploration sampling (s)           2.53932\n",
      "time/logging (s)                        0.0123606\n",
      "time/preback_alpha (s)                  0.00621193\n",
      "time/preback_policy (s)                15.6131\n",
      "time/preback_start (s)                  0.983095\n",
      "time/preback_zf (s)                    36.5177\n",
      "time/saving (s)                         3.479e-06\n",
      "time/training (s)                      11.2074\n",
      "time/epoch (s)                        100.82\n",
      "time/total (s)                       2144.22\n",
      "Epoch                                  21\n",
      "---------------------------------  ---------------\n",
      "2024-10-25 21:47:31.311782 +0330 | [humanoid_2024_10_25_21_10_05_0000--s-0] Epoch 22 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 125000\n",
      "trainer/ZF1 Loss                       47.2421\n",
      "trainer/ZF2 Loss                       38.1633\n",
      "trainer/ZF Expert Reward               18.3854\n",
      "trainer/ZF Policy Reward                2.11346\n",
      "trainer/ZF CHI2 Term                   53.0546\n",
      "trainer/Policy Loss                  -353.033\n",
      "trainer/expert_lambda Loss             11.0183\n",
      "trainer/expert_lambda Value            19.5435\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              102.92\n",
      "trainer/Policy Param Norm              28.066\n",
      "trainer/Zf1 Grad Norm                8538.6\n",
      "trainer/Zf1 Param Norm                 96.7642\n",
      "trainer/Zf2 Grad Norm                7553.67\n",
      "trainer/Zf2 Param Norm                 96.5675\n",
      "trainer/Z Expert Predictions Mean     811.934\n",
      "trainer/Z Expert Predictions Std       46.3828\n",
      "trainer/Z Expert Predictions Max      868.875\n",
      "trainer/Z Expert Predictions Min      581.879\n",
      "trainer/Z Policy Predictions Mean     347.739\n",
      "trainer/Z Policy Predictions Std      226.513\n",
      "trainer/Z Policy Predictions Max      776.108\n",
      "trainer/Z Policy Predictions Min       -7.85489\n",
      "trainer/Z Expert Targets Mean         793.549\n",
      "trainer/Z Expert Targets Std           45.5552\n",
      "trainer/Z Expert Targets Max          854.057\n",
      "trainer/Z Expert Targets Min          570.173\n",
      "trainer/Z Policy Targets Mean         345.626\n",
      "trainer/Z Policy Targets Std          228.377\n",
      "trainer/Z Policy Targets Max          791.196\n",
      "trainer/Z Policy Targets Min           -4.72979\n",
      "trainer/Log Pis Mean                   35.9798\n",
      "trainer/Log Pis Std                    13.8292\n",
      "trainer/Policy mu Mean                  0.465308\n",
      "trainer/Policy mu Std                   1.93695\n",
      "trainer/Policy log std Mean            -1.96209\n",
      "trainer/Policy log std Std              0.925513\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        118480\n",
      "exploration/num paths total          1442\n",
      "evaluation/num steps total          40156\n",
      "evaluation/num paths total            230\n",
      "evaluation/path length Mean           325\n",
      "evaluation/path length Std             20.6349\n",
      "evaluation/path length Max            350\n",
      "evaluation/path length Min            276\n",
      "evaluation/Rewards Mean                 5.29701\n",
      "evaluation/Rewards Std                  0.201969\n",
      "evaluation/Rewards Max                  6.52735\n",
      "evaluation/Rewards Min                  4.85881\n",
      "evaluation/Returns Mean              1721.53\n",
      "evaluation/Returns Std                106.014\n",
      "evaluation/Returns Max               1843.54\n",
      "evaluation/Returns Min               1462.15\n",
      "evaluation/Estimation Bias Mean       617.65\n",
      "evaluation/Estimation Bias Std        312.001\n",
      "evaluation/EB/Q_True Mean              41.3715\n",
      "evaluation/EB/Q_True Std              126.821\n",
      "evaluation/EB/Q_Pred Mean             659.022\n",
      "evaluation/EB/Q_Pred Std              284.387\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           1721.53\n",
      "evaluation/Actions Mean                 0.0751711\n",
      "evaluation/Actions Std                  0.558259\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999998\n",
      "time/backward_policy (s)                7.97427\n",
      "time/backward_zf1 (s)                   9.41288\n",
      "time/backward_zf2 (s)                   8.91409\n",
      "time/data sampling (s)                  1.76843\n",
      "time/data storing (s)                   0.0841886\n",
      "time/evaluation sampling (s)            1.87271\n",
      "time/exploration sampling (s)           2.48676\n",
      "time/logging (s)                        0.00868941\n",
      "time/preback_alpha (s)                  0.00632654\n",
      "time/preback_policy (s)                16.3783\n",
      "time/preback_start (s)                  0.979657\n",
      "time/preback_zf (s)                    36.6902\n",
      "time/saving (s)                         4.568e-06\n",
      "time/training (s)                      11.615\n",
      "time/epoch (s)                         98.1915\n",
      "time/total (s)                       2242.41\n",
      "Epoch                                  22\n",
      "---------------------------------  ---------------\n"
     ]
    }
   ],
   "source": [
    "!python train.py --env humanoid --demos 1 --loss 'v0' --alpha 0.01 --seed 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f88cbb-71c0-4916-b464-8af1fa527ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
