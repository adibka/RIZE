{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225624e4-7513-4478-bec3-6e447ae0749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No personal conf_private.py found.\n",
      "doodad not detected\n",
      "2024-11-18 22:17:50.368812 +0330 | Variant:\n",
      "2024-11-18 22:17:50.369073 +0330 | {\n",
      "  \"algorithm_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"min_num_steps_before_training\": 10000,\n",
      "    \"num_epochs\": 60,\n",
      "    \"num_eval_paths_per_epoch\": 10,\n",
      "    \"num_expl_steps_per_train_loop\": 5000,\n",
      "    \"num_trains_per_train_loop\": 5000\n",
      "  },\n",
      "  \"iq_kwargs\": {\n",
      "    \"demos\": 0.256,\n",
      "    \"regularize\": \"TD_both\",\n",
      "    \"loss\": \"v0\",\n",
      "    \"chi\": 0.5,\n",
      "    \"expert_path\": \"experts/Humanoid-v2_25.pkl\",\n",
      "    \"subsample_freq\": 1,\n",
      "    \"reward_type\": \"sparse\",\n",
      "    \"noise_std\": 1.0,\n",
      "    \"sparse_prob\": 0.5,\n",
      "    \"sparse_type\": \"empty\"\n",
      "  },\n",
      "  \"env\": \"Humanoid-v2\",\n",
      "  \"seed\": 4,\n",
      "  \"expectation_z\": true,\n",
      "  \"use_policy_expert_obs\": false,\n",
      "  \"eval_env_num\": 10,\n",
      "  \"expl_env_num\": 10,\n",
      "  \"layer_size\": 256,\n",
      "  \"num_quantiles\": 24,\n",
      "  \"replay_buffer_size\": 1000000,\n",
      "  \"trainer_kwargs\": {\n",
      "    \"alpha\": 0.05,\n",
      "    \"discount\": 0.99,\n",
      "    \"policy_lr\": 1e-05,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"tau_type\": \"iqn\",\n",
      "    \"use_automatic_entropy_tuning\": false,\n",
      "    \"zf_lr\": 0.0003,\n",
      "    \"expert_lambda\": 10,\n",
      "    \"expert_lambda_lr\": 0.0001,\n",
      "    \"tune_expert_lambda\": true,\n",
      "    \"policy_lambda\": 5,\n",
      "    \"policy_lambda_lr\": 0.0001,\n",
      "    \"tune_policy_lambda\": false\n",
      "  },\n",
      "  \"version\": \"normal-iqn-neutral\"\n",
      "}\n",
      "/home/eddie/venvs/LSIQ/lib/python3.10/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2024-11-18 22:19:32.456889 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 0 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 15000\n",
      "trainer/ZF1 Loss                      29.2815\n",
      "trainer/ZF2 Loss                      28.904\n",
      "trainer/ZF Expert Reward              -0.639691\n",
      "trainer/ZF Policy Reward              -0.625573\n",
      "trainer/ZF CHI2 Term                  28.4502\n",
      "trainer/Policy Loss                   -0.547695\n",
      "trainer/expert_lambda Loss            56.6323\n",
      "trainer/expert_lambda Value            9.9999\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm               0.45157\n",
      "trainer/Policy Param Norm             17.4353\n",
      "trainer/Zf1 Grad Norm                 87.3893\n",
      "trainer/Zf1 Param Norm                32.039\n",
      "trainer/Zf2 Grad Norm                 82.105\n",
      "trainer/Zf2 Param Norm                32.036\n",
      "trainer/Z Expert Predictions Mean     -0.286128\n",
      "trainer/Z Expert Predictions Std       0.125176\n",
      "trainer/Z Expert Predictions Max       0.0485872\n",
      "trainer/Z Expert Predictions Min      -0.556613\n",
      "trainer/Z Policy Predictions Mean     -0.218198\n",
      "trainer/Z Policy Predictions Std       0.188273\n",
      "trainer/Z Policy Predictions Max       0.193468\n",
      "trainer/Z Policy Predictions Min      -0.805313\n",
      "trainer/Z Expert Targets Mean          0.353563\n",
      "trainer/Z Expert Targets Std           0.216348\n",
      "trainer/Z Expert Targets Max           0.813912\n",
      "trainer/Z Expert Targets Min          -0.061882\n",
      "trainer/Z Policy Targets Mean          0.407376\n",
      "trainer/Z Policy Targets Std           0.235997\n",
      "trainer/Z Policy Targets Max           0.974308\n",
      "trainer/Z Policy Targets Min          -0.194816\n",
      "trainer/Log Pis Mean                 -11.3485\n",
      "trainer/Log Pis Std                    0.913593\n",
      "trainer/Policy mu Mean                -0.000240836\n",
      "trainer/Policy mu Std                  0.00629745\n",
      "trainer/Policy log std Mean            8.08263e-05\n",
      "trainer/Policy log std Std             0.00555132\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        14650\n",
      "exploration/num paths total          517\n",
      "evaluation/num steps total           494\n",
      "evaluation/num paths total            10\n",
      "evaluation/path length Mean           49.4\n",
      "evaluation/path length Std             1.0198\n",
      "evaluation/path length Max            51\n",
      "evaluation/path length Min            48\n",
      "evaluation/Rewards Mean                4.91601\n",
      "evaluation/Rewards Std                 0.127226\n",
      "evaluation/Rewards Max                 5.19578\n",
      "evaluation/Rewards Min                 4.61215\n",
      "evaluation/Returns Mean              242.851\n",
      "evaluation/Returns Std                 5.12118\n",
      "evaluation/Returns Max               250.3\n",
      "evaluation/Returns Min               233.237\n",
      "evaluation/Estimation Bias Mean      112.674\n",
      "evaluation/Estimation Bias Std        54.0575\n",
      "evaluation/EB/Q_True Mean             11.1931\n",
      "evaluation/EB/Q_True Std              37.6477\n",
      "evaluation/EB/Q_Pred Mean            123.867\n",
      "evaluation/EB/Q_Pred Std              42.8165\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           242.851\n",
      "evaluation/Actions Mean                0.0381058\n",
      "evaluation/Actions Std                 0.565086\n",
      "evaluation/Actions Max                 0.998145\n",
      "evaluation/Actions Min                -0.999154\n",
      "time/backward_policy (s)               8.58681\n",
      "time/backward_zf1 (s)                  9.74414\n",
      "time/backward_zf2 (s)                  9.32319\n",
      "time/data sampling (s)                 1.52041\n",
      "time/data storing (s)                  0.0863367\n",
      "time/evaluation sampling (s)           0.215582\n",
      "time/exploration sampling (s)          2.73781\n",
      "time/logging (s)                       0.00202622\n",
      "time/preback_alpha (s)                 0.005298\n",
      "time/preback_policy (s)               15.3655\n",
      "time/preback_start (s)                 0.933736\n",
      "time/preback_zf (s)                   36.3039\n",
      "time/saving (s)                        4.161e-06\n",
      "time/training (s)                     11.232\n",
      "time/epoch (s)                        96.0568\n",
      "time/total (s)                       103.476\n",
      "Epoch                                  0\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 22:21:08.788742 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 1 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 20000\n",
      "trainer/ZF1 Loss                      19.8995\n",
      "trainer/ZF2 Loss                      18.4515\n",
      "trainer/ZF Expert Reward              12.209\n",
      "trainer/ZF Policy Reward               3.64976\n",
      "trainer/ZF CHI2 Term                  29.4191\n",
      "trainer/Policy Loss                 -115.551\n",
      "trainer/expert_lambda Loss             3.86707\n",
      "trainer/expert_lambda Value           10.3542\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              12.7836\n",
      "trainer/Policy Param Norm             18.4656\n",
      "trainer/Zf1 Grad Norm                653.385\n",
      "trainer/Zf1 Param Norm                42.5857\n",
      "trainer/Zf2 Grad Norm                479.094\n",
      "trainer/Zf2 Param Norm                42.4822\n",
      "trainer/Z Expert Predictions Mean    202.37\n",
      "trainer/Z Expert Predictions Std       5.26926\n",
      "trainer/Z Expert Predictions Max     209.1\n",
      "trainer/Z Expert Predictions Min     182.511\n",
      "trainer/Z Policy Predictions Mean    109.619\n",
      "trainer/Z Policy Predictions Std      45.3604\n",
      "trainer/Z Policy Predictions Max     175.386\n",
      "trainer/Z Policy Predictions Min      -2.92186\n",
      "trainer/Z Expert Targets Mean        190.161\n",
      "trainer/Z Expert Targets Std           5.94596\n",
      "trainer/Z Expert Targets Max         199.451\n",
      "trainer/Z Expert Targets Min         166.109\n",
      "trainer/Z Policy Targets Mean        105.969\n",
      "trainer/Z Policy Targets Std          46.5233\n",
      "trainer/Z Policy Targets Max         172.206\n",
      "trainer/Z Policy Targets Min          -3.67285\n",
      "trainer/Log Pis Mean                  12.4743\n",
      "trainer/Log Pis Std                    9.46261\n",
      "trainer/Policy mu Mean                 0.225426\n",
      "trainer/Policy mu Std                  1.35718\n",
      "trainer/Policy log std Mean           -0.810032\n",
      "trainer/Policy log std Std             0.275577\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        19463\n",
      "exploration/num paths total          588\n",
      "evaluation/num steps total          1109\n",
      "evaluation/num paths total            20\n",
      "evaluation/path length Mean           61.5\n",
      "evaluation/path length Std             1.36015\n",
      "evaluation/path length Max            64\n",
      "evaluation/path length Min            59\n",
      "evaluation/Rewards Mean                5.16213\n",
      "evaluation/Rewards Std                 0.250141\n",
      "evaluation/Rewards Max                 5.84193\n",
      "evaluation/Rewards Min                 4.69536\n",
      "evaluation/Returns Mean              317.471\n",
      "evaluation/Returns Std                 9.70395\n",
      "evaluation/Returns Max               330.171\n",
      "evaluation/Returns Min               300.372\n",
      "evaluation/Estimation Bias Mean      164.657\n",
      "evaluation/Estimation Bias Std        92.4872\n",
      "evaluation/EB/Q_True Mean             14.4843\n",
      "evaluation/EB/Q_True Std              48.0275\n",
      "evaluation/EB/Q_Pred Mean            179.142\n",
      "evaluation/EB/Q_Pred Std              83.9832\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           317.471\n",
      "evaluation/Actions Mean                0.171069\n",
      "evaluation/Actions Std                 0.515605\n",
      "evaluation/Actions Max                 0.978911\n",
      "evaluation/Actions Min                -0.954091\n",
      "time/backward_policy (s)               8.44781\n",
      "time/backward_zf1 (s)                  9.73519\n",
      "time/backward_zf2 (s)                  9.4366\n",
      "time/data sampling (s)                 1.52465\n",
      "time/data storing (s)                  0.0879596\n",
      "time/evaluation sampling (s)           0.341455\n",
      "time/exploration sampling (s)          2.58741\n",
      "time/logging (s)                       0.00125274\n",
      "time/preback_alpha (s)                 0.00526372\n",
      "time/preback_policy (s)               15.4366\n",
      "time/preback_start (s)                 0.918207\n",
      "time/preback_zf (s)                   36.4876\n",
      "time/saving (s)                        2.736e-06\n",
      "time/training (s)                     11.115\n",
      "time/epoch (s)                        96.125\n",
      "time/total (s)                       199.603\n",
      "Epoch                                  1\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:22:45.228358 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 2 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 25000\n",
      "trainer/ZF1 Loss                       4.56259\n",
      "trainer/ZF2 Loss                       4.16453\n",
      "trainer/ZF Expert Reward              13.1015\n",
      "trainer/ZF Policy Reward               2.55096\n",
      "trainer/ZF CHI2 Term                  14.5021\n",
      "trainer/Policy Loss                 -151.925\n",
      "trainer/expert_lambda Loss             7.99116\n",
      "trainer/expert_lambda Value           10.8461\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              16.0406\n",
      "trainer/Policy Param Norm             19.319\n",
      "trainer/Zf1 Grad Norm               1011.47\n",
      "trainer/Zf1 Param Norm                46.4788\n",
      "trainer/Zf2 Grad Norm                818.498\n",
      "trainer/Zf2 Param Norm                46.2389\n",
      "trainer/Z Expert Predictions Mean    303.522\n",
      "trainer/Z Expert Predictions Std      20.106\n",
      "trainer/Z Expert Predictions Max     324.915\n",
      "trainer/Z Expert Predictions Min     214.371\n",
      "trainer/Z Policy Predictions Mean    145.718\n",
      "trainer/Z Policy Predictions Std      66.3963\n",
      "trainer/Z Policy Predictions Max     277.411\n",
      "trainer/Z Policy Predictions Min      -1.56616\n",
      "trainer/Z Expert Targets Mean        290.421\n",
      "trainer/Z Expert Targets Std          20.9096\n",
      "trainer/Z Expert Targets Max         315.006\n",
      "trainer/Z Expert Targets Min         203.371\n",
      "trainer/Z Policy Targets Mean        143.167\n",
      "trainer/Z Policy Targets Std          66.5801\n",
      "trainer/Z Policy Targets Max         276.156\n",
      "trainer/Z Policy Targets Min          -2.29471\n",
      "trainer/Log Pis Mean                  10.866\n",
      "trainer/Log Pis Std                    9.06792\n",
      "trainer/Policy mu Mean                 0.327365\n",
      "trainer/Policy mu Std                  1.18239\n",
      "trainer/Policy log std Mean           -0.909167\n",
      "trainer/Policy log std Std             0.427325\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        24501\n",
      "exploration/num paths total          656\n",
      "evaluation/num steps total          2059\n",
      "evaluation/num paths total            30\n",
      "evaluation/path length Mean           95\n",
      "evaluation/path length Std            14.5808\n",
      "evaluation/path length Max           121\n",
      "evaluation/path length Min            74\n",
      "evaluation/Rewards Mean                5.09108\n",
      "evaluation/Rewards Std                 0.316038\n",
      "evaluation/Rewards Max                 6.10372\n",
      "evaluation/Rewards Min                 4.10632\n",
      "evaluation/Returns Mean              483.653\n",
      "evaluation/Returns Std                59.2147\n",
      "evaluation/Returns Max               597.406\n",
      "evaluation/Returns Min               393.558\n",
      "evaluation/Estimation Bias Mean      162.216\n",
      "evaluation/Estimation Bias Std        98.1722\n",
      "evaluation/EB/Q_True Mean             26.4849\n",
      "evaluation/EB/Q_True Std              78.0087\n",
      "evaluation/EB/Q_Pred Mean            188.7\n",
      "evaluation/EB/Q_Pred Std              67.4405\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           483.653\n",
      "evaluation/Actions Mean                0.071106\n",
      "evaluation/Actions Std                 0.539691\n",
      "evaluation/Actions Max                 0.997216\n",
      "evaluation/Actions Min                -0.992352\n",
      "time/backward_policy (s)               8.45793\n",
      "time/backward_zf1 (s)                  9.78595\n",
      "time/backward_zf2 (s)                  9.49803\n",
      "time/data sampling (s)                 1.52581\n",
      "time/data storing (s)                  0.0865239\n",
      "time/evaluation sampling (s)           0.550442\n",
      "time/exploration sampling (s)          2.45586\n",
      "time/logging (s)                       0.00250981\n",
      "time/preback_alpha (s)                 0.00520014\n",
      "time/preback_policy (s)               15.4088\n",
      "time/preback_start (s)                 0.91011\n",
      "time/preback_zf (s)                   36.492\n",
      "time/saving (s)                        3.86e-06\n",
      "time/training (s)                     11.0578\n",
      "time/epoch (s)                        96.237\n",
      "time/total (s)                       295.842\n",
      "Epoch                                  2\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:24:21.295696 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 3 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 30000\n",
      "trainer/ZF1 Loss                       8.31251\n",
      "trainer/ZF2 Loss                      11.9392\n",
      "trainer/ZF Expert Reward              14.3881\n",
      "trainer/ZF Policy Reward               0.953792\n",
      "trainer/ZF CHI2 Term                  21.3412\n",
      "trainer/Policy Loss                 -159.319\n",
      "trainer/expert_lambda Loss            10.7431\n",
      "trainer/expert_lambda Value           11.3094\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              32.1683\n",
      "trainer/Policy Param Norm             20.0799\n",
      "trainer/Zf1 Grad Norm               1452.14\n",
      "trainer/Zf1 Param Norm                49.4008\n",
      "trainer/Zf2 Grad Norm               2911.71\n",
      "trainer/Zf2 Param Norm                49.1727\n",
      "trainer/Z Expert Predictions Mean    327.562\n",
      "trainer/Z Expert Predictions Std      26.1123\n",
      "trainer/Z Expert Predictions Max     364.764\n",
      "trainer/Z Expert Predictions Min     265.608\n",
      "trainer/Z Policy Predictions Mean    153.692\n",
      "trainer/Z Policy Predictions Std      80.1469\n",
      "trainer/Z Policy Predictions Max     271.577\n",
      "trainer/Z Policy Predictions Min      -5.99446\n",
      "trainer/Z Expert Targets Mean        313.174\n",
      "trainer/Z Expert Targets Std          26.3284\n",
      "trainer/Z Expert Targets Max         353.686\n",
      "trainer/Z Expert Targets Min         250.032\n",
      "trainer/Z Policy Targets Mean        152.738\n",
      "trainer/Z Policy Targets Std          80.6811\n",
      "trainer/Z Policy Targets Max         260.611\n",
      "trainer/Z Policy Targets Min          -2.62633\n",
      "trainer/Log Pis Mean                  10.8018\n",
      "trainer/Log Pis Std                    9.65027\n",
      "trainer/Policy mu Mean                 0.116319\n",
      "trainer/Policy mu Std                  1.22398\n",
      "trainer/Policy log std Mean           -0.926864\n",
      "trainer/Policy log std Std             0.487174\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        29630\n",
      "exploration/num paths total          717\n",
      "evaluation/num steps total          2945\n",
      "evaluation/num paths total            40\n",
      "evaluation/path length Mean           88.6\n",
      "evaluation/path length Std             9.29731\n",
      "evaluation/path length Max           107\n",
      "evaluation/path length Min            78\n",
      "evaluation/Rewards Mean                5.31246\n",
      "evaluation/Rewards Std                 0.373603\n",
      "evaluation/Rewards Max                 6.57254\n",
      "evaluation/Rewards Min                 4.73027\n",
      "evaluation/Returns Mean              470.684\n",
      "evaluation/Returns Std                42.7005\n",
      "evaluation/Returns Max               562.1\n",
      "evaluation/Returns Min               416.717\n",
      "evaluation/Estimation Bias Mean      115.664\n",
      "evaluation/Estimation Bias Std       116.473\n",
      "evaluation/EB/Q_True Mean             25.2316\n",
      "evaluation/EB/Q_True Std              75.859\n",
      "evaluation/EB/Q_Pred Mean            140.896\n",
      "evaluation/EB/Q_Pred Std              89.8364\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           470.684\n",
      "evaluation/Actions Mean                0.0826837\n",
      "evaluation/Actions Std                 0.535096\n",
      "evaluation/Actions Max                 0.999998\n",
      "evaluation/Actions Min                -0.995089\n",
      "time/backward_policy (s)               8.62964\n",
      "time/backward_zf1 (s)                  9.88655\n",
      "time/backward_zf2 (s)                  9.68646\n",
      "time/data sampling (s)                 1.51835\n",
      "time/data storing (s)                  0.0863819\n",
      "time/evaluation sampling (s)           0.53303\n",
      "time/exploration sampling (s)          2.41612\n",
      "time/logging (s)                       0.00262538\n",
      "time/preback_alpha (s)                 0.00523013\n",
      "time/preback_policy (s)               15.0131\n",
      "time/preback_start (s)                 0.904001\n",
      "time/preback_zf (s)                   36.3927\n",
      "time/saving (s)                        3.75e-06\n",
      "time/training (s)                     10.7878\n",
      "time/epoch (s)                        95.862\n",
      "time/total (s)                       391.707\n",
      "Epoch                                  3\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:25:57.652912 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 4 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 35000\n",
      "trainer/ZF1 Loss                      11.5134\n",
      "trainer/ZF2 Loss                       8.29563\n",
      "trainer/ZF Expert Reward              14.3676\n",
      "trainer/ZF Policy Reward               2.88554\n",
      "trainer/ZF CHI2 Term                  21.539\n",
      "trainer/Policy Loss                 -122.463\n",
      "trainer/expert_lambda Loss            11.6102\n",
      "trainer/expert_lambda Value           11.7618\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              25.6059\n",
      "trainer/Policy Param Norm             20.5622\n",
      "trainer/Zf1 Grad Norm               2143.04\n",
      "trainer/Zf1 Param Norm                51.5757\n",
      "trainer/Zf2 Grad Norm               1345.08\n",
      "trainer/Zf2 Param Norm                51.3318\n",
      "trainer/Z Expert Predictions Mean    282.115\n",
      "trainer/Z Expert Predictions Std      30.9181\n",
      "trainer/Z Expert Predictions Max     344.47\n",
      "trainer/Z Expert Predictions Min     217.335\n",
      "trainer/Z Policy Predictions Mean    119.04\n",
      "trainer/Z Policy Predictions Std      76.1123\n",
      "trainer/Z Policy Predictions Max     218.418\n",
      "trainer/Z Policy Predictions Min      -7.87623\n",
      "trainer/Z Expert Targets Mean        267.747\n",
      "trainer/Z Expert Targets Std          31.5748\n",
      "trainer/Z Expert Targets Max         334.233\n",
      "trainer/Z Expert Targets Min         203.915\n",
      "trainer/Z Policy Targets Mean        116.154\n",
      "trainer/Z Policy Targets Std          75.7177\n",
      "trainer/Z Policy Targets Max         214.266\n",
      "trainer/Z Policy Targets Min         -10.9893\n",
      "trainer/Log Pis Mean                   8.16483\n",
      "trainer/Log Pis Std                    8.96999\n",
      "trainer/Policy mu Mean                 0.165003\n",
      "trainer/Policy mu Std                  1.16437\n",
      "trainer/Policy log std Mean           -0.844822\n",
      "trainer/Policy log std Std             0.458297\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        34503\n",
      "exploration/num paths total          771\n",
      "evaluation/num steps total          3555\n",
      "evaluation/num paths total            50\n",
      "evaluation/path length Mean           61\n",
      "evaluation/path length Std             5.94979\n",
      "evaluation/path length Max            74\n",
      "evaluation/path length Min            53\n",
      "evaluation/Rewards Mean                5.0839\n",
      "evaluation/Rewards Std                 0.385048\n",
      "evaluation/Rewards Max                 6.09064\n",
      "evaluation/Rewards Min                 4.46944\n",
      "evaluation/Returns Mean              310.118\n",
      "evaluation/Returns Std                32.3126\n",
      "evaluation/Returns Max               380.451\n",
      "evaluation/Returns Min               266.435\n",
      "evaluation/Estimation Bias Mean       94.2834\n",
      "evaluation/Estimation Bias Std        81.3575\n",
      "evaluation/EB/Q_True Mean             19.2686\n",
      "evaluation/EB/Q_True Std              57.9473\n",
      "evaluation/EB/Q_Pred Mean            113.552\n",
      "evaluation/EB/Q_Pred Std              64.5747\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           310.118\n",
      "evaluation/Actions Mean                0.0857475\n",
      "evaluation/Actions Std                 0.559445\n",
      "evaluation/Actions Max                 0.998667\n",
      "evaluation/Actions Min                -0.999001\n",
      "time/backward_policy (s)               8.7991\n",
      "time/backward_zf1 (s)                 10.1\n",
      "time/backward_zf2 (s)                  9.88044\n",
      "time/data sampling (s)                 1.54828\n",
      "time/data storing (s)                  0.0863989\n",
      "time/evaluation sampling (s)           0.333697\n",
      "time/exploration sampling (s)          2.47021\n",
      "time/logging (s)                       0.00298513\n",
      "time/preback_alpha (s)                 0.00521694\n",
      "time/preback_policy (s)               14.8387\n",
      "time/preback_start (s)                 0.907856\n",
      "time/preback_zf (s)                   36.3673\n",
      "time/saving (s)                        4.134e-06\n",
      "time/training (s)                     10.8126\n",
      "time/epoch (s)                        96.1527\n",
      "time/total (s)                       487.863\n",
      "Epoch                                  4\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:27:33.207013 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 5 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 40000\n",
      "trainer/ZF1 Loss                       4.70044\n",
      "trainer/ZF2 Loss                       7.50399\n",
      "trainer/ZF Expert Reward              15.0134\n",
      "trainer/ZF Policy Reward               4.29242\n",
      "trainer/ZF CHI2 Term                  18.4263\n",
      "trainer/Policy Loss                 -115.818\n",
      "trainer/expert_lambda Loss            12.1361\n",
      "trainer/expert_lambda Value           12.2061\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              19.6897\n",
      "trainer/Policy Param Norm             21.0057\n",
      "trainer/Zf1 Grad Norm                960.421\n",
      "trainer/Zf1 Param Norm                53.9514\n",
      "trainer/Zf2 Grad Norm                845.417\n",
      "trainer/Zf2 Param Norm                53.6254\n",
      "trainer/Z Expert Predictions Mean    279.005\n",
      "trainer/Z Expert Predictions Std      33.3575\n",
      "trainer/Z Expert Predictions Max     349.138\n",
      "trainer/Z Expert Predictions Min     214.618\n",
      "trainer/Z Policy Predictions Mean    114.02\n",
      "trainer/Z Policy Predictions Std      70.6259\n",
      "trainer/Z Policy Predictions Max     207.507\n",
      "trainer/Z Policy Predictions Min       3.0233\n",
      "trainer/Z Expert Targets Mean        263.992\n",
      "trainer/Z Expert Targets Std          33.9417\n",
      "trainer/Z Expert Targets Max         341.264\n",
      "trainer/Z Expert Targets Min         197.563\n",
      "trainer/Z Policy Targets Mean        109.727\n",
      "trainer/Z Policy Targets Std          70.4359\n",
      "trainer/Z Policy Targets Max         195.623\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                   6.61734\n",
      "trainer/Log Pis Std                    8.68534\n",
      "trainer/Policy mu Mean                 0.113496\n",
      "trainer/Policy mu Std                  1.09746\n",
      "trainer/Policy log std Mean           -0.843756\n",
      "trainer/Policy log std Std             0.488492\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        39482\n",
      "exploration/num paths total          826\n",
      "evaluation/num steps total          4362\n",
      "evaluation/num paths total            60\n",
      "evaluation/path length Mean           80.7\n",
      "evaluation/path length Std             6.19758\n",
      "evaluation/path length Max            91\n",
      "evaluation/path length Min            71\n",
      "evaluation/Rewards Mean                5.34821\n",
      "evaluation/Rewards Std                 0.332985\n",
      "evaluation/Rewards Max                 6.25657\n",
      "evaluation/Rewards Min                 4.86376\n",
      "evaluation/Returns Mean              431.601\n",
      "evaluation/Returns Std                31.7392\n",
      "evaluation/Returns Max               483.733\n",
      "evaluation/Returns Min               379.781\n",
      "evaluation/Estimation Bias Mean       89.818\n",
      "evaluation/Estimation Bias Std        92.4227\n",
      "evaluation/EB/Q_True Mean             21.4631\n",
      "evaluation/EB/Q_True Std              67.1839\n",
      "evaluation/EB/Q_Pred Mean            111.281\n",
      "evaluation/EB/Q_Pred Std              65.6792\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           431.601\n",
      "evaluation/Actions Mean                0.115384\n",
      "evaluation/Actions Std                 0.586193\n",
      "evaluation/Actions Max                 0.996737\n",
      "evaluation/Actions Min                -0.995007\n",
      "time/backward_policy (s)               8.24905\n",
      "time/backward_zf1 (s)                  9.49695\n",
      "time/backward_zf2 (s)                  9.24673\n",
      "time/data sampling (s)                 1.52658\n",
      "time/data storing (s)                  0.0857268\n",
      "time/evaluation sampling (s)           0.389961\n",
      "time/exploration sampling (s)          2.43232\n",
      "time/logging (s)                       0.00154702\n",
      "time/preback_alpha (s)                 0.00514829\n",
      "time/preback_policy (s)               15.6078\n",
      "time/preback_start (s)                 0.895023\n",
      "time/preback_zf (s)                   36.2608\n",
      "time/saving (s)                        2.603e-06\n",
      "time/training (s)                     11.1522\n",
      "time/epoch (s)                        95.3499\n",
      "time/total (s)                       583.216\n",
      "Epoch                                  5\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:29:09.976543 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 6 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 45000\n",
      "trainer/ZF1 Loss                       4.53226\n",
      "trainer/ZF2 Loss                       5.57743\n",
      "trainer/ZF Expert Reward              14.7694\n",
      "trainer/ZF Policy Reward               2.56651\n",
      "trainer/ZF CHI2 Term                  16.8752\n",
      "trainer/Policy Loss                 -102.376\n",
      "trainer/expert_lambda Loss            11.7983\n",
      "trainer/expert_lambda Value           12.6538\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              30.0201\n",
      "trainer/Policy Param Norm             21.4477\n",
      "trainer/Zf1 Grad Norm                952.948\n",
      "trainer/Zf1 Param Norm                56.2774\n",
      "trainer/Zf2 Grad Norm               1227.16\n",
      "trainer/Zf2 Param Norm                55.7565\n",
      "trainer/Z Expert Predictions Mean    303.502\n",
      "trainer/Z Expert Predictions Std      36.3504\n",
      "trainer/Z Expert Predictions Max     362.685\n",
      "trainer/Z Expert Predictions Min     231.366\n",
      "trainer/Z Policy Predictions Mean     99.7168\n",
      "trainer/Z Policy Predictions Std      67.1041\n",
      "trainer/Z Policy Predictions Max     228.246\n",
      "trainer/Z Policy Predictions Min      -2.31919\n",
      "trainer/Z Expert Targets Mean        288.733\n",
      "trainer/Z Expert Targets Std          37.2268\n",
      "trainer/Z Expert Targets Max         353.221\n",
      "trainer/Z Expert Targets Min         202.063\n",
      "trainer/Z Policy Targets Mean         97.1503\n",
      "trainer/Z Policy Targets Std          67.2756\n",
      "trainer/Z Policy Targets Max         230.871\n",
      "trainer/Z Policy Targets Min          -1.85097\n",
      "trainer/Log Pis Mean                   6.90226\n",
      "trainer/Log Pis Std                    8.20131\n",
      "trainer/Policy mu Mean                 0.187226\n",
      "trainer/Policy mu Std                  1.09385\n",
      "trainer/Policy log std Mean           -0.842329\n",
      "trainer/Policy log std Std             0.450324\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        44298\n",
      "exploration/num paths total          881\n",
      "evaluation/num steps total          5317\n",
      "evaluation/num paths total            70\n",
      "evaluation/path length Mean           95.5\n",
      "evaluation/path length Std            12.7299\n",
      "evaluation/path length Max           121\n",
      "evaluation/path length Min            84\n",
      "evaluation/Rewards Mean                5.17277\n",
      "evaluation/Rewards Std                 0.431348\n",
      "evaluation/Rewards Max                 6.42981\n",
      "evaluation/Rewards Min                 3.7094\n",
      "evaluation/Returns Mean              493.999\n",
      "evaluation/Returns Std                65.0964\n",
      "evaluation/Returns Max               628.397\n",
      "evaluation/Returns Min               388.849\n",
      "evaluation/Estimation Bias Mean       94.0953\n",
      "evaluation/Estimation Bias Std       108.979\n",
      "evaluation/EB/Q_True Mean             28.4513\n",
      "evaluation/EB/Q_True Std              82.8066\n",
      "evaluation/EB/Q_Pred Mean            122.547\n",
      "evaluation/EB/Q_Pred Std              86.4979\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           493.999\n",
      "evaluation/Actions Mean                0.0203969\n",
      "evaluation/Actions Std                 0.498986\n",
      "evaluation/Actions Max                 0.999913\n",
      "evaluation/Actions Min                -0.998489\n",
      "time/backward_policy (s)               8.84476\n",
      "time/backward_zf1 (s)                 10.109\n",
      "time/backward_zf2 (s)                  9.89322\n",
      "time/data sampling (s)                 1.55915\n",
      "time/data storing (s)                  0.0868428\n",
      "time/evaluation sampling (s)           0.531774\n",
      "time/exploration sampling (s)          2.47754\n",
      "time/logging (s)                       0.00192077\n",
      "time/preback_alpha (s)                 0.00523907\n",
      "time/preback_policy (s)               14.9064\n",
      "time/preback_start (s)                 0.907058\n",
      "time/preback_zf (s)                   36.3706\n",
      "time/saving (s)                        2.413e-06\n",
      "time/training (s)                     10.873\n",
      "time/epoch (s)                        96.5665\n",
      "time/total (s)                       679.784\n",
      "Epoch                                  6\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:30:45.825607 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 7 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 50000\n",
      "trainer/ZF1 Loss                      15.1005\n",
      "trainer/ZF2 Loss                      12.1974\n",
      "trainer/ZF Expert Reward              14.3729\n",
      "trainer/ZF Policy Reward               4.26766\n",
      "trainer/ZF CHI2 Term                  24.586\n",
      "trainer/Policy Loss                 -101.054\n",
      "trainer/expert_lambda Loss            14.3572\n",
      "trainer/expert_lambda Value           13.107\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              34.1519\n",
      "trainer/Policy Param Norm             21.9582\n",
      "trainer/Zf1 Grad Norm               2057.99\n",
      "trainer/Zf1 Param Norm                58.882\n",
      "trainer/Zf2 Grad Norm               1794.05\n",
      "trainer/Zf2 Param Norm                58.4065\n",
      "trainer/Z Expert Predictions Mean    350.081\n",
      "trainer/Z Expert Predictions Std      46.9722\n",
      "trainer/Z Expert Predictions Max     397.009\n",
      "trainer/Z Expert Predictions Min     224.177\n",
      "trainer/Z Policy Predictions Mean     99.8749\n",
      "trainer/Z Policy Predictions Std      67.7605\n",
      "trainer/Z Policy Predictions Max     255.979\n",
      "trainer/Z Policy Predictions Min      -1.93412\n",
      "trainer/Z Expert Targets Mean        335.708\n",
      "trainer/Z Expert Targets Std          47.1336\n",
      "trainer/Z Expert Targets Max         387.406\n",
      "trainer/Z Expert Targets Min         208.971\n",
      "trainer/Z Policy Targets Mean         95.6073\n",
      "trainer/Z Policy Targets Std          67.1974\n",
      "trainer/Z Policy Targets Max         244.871\n",
      "trainer/Z Policy Targets Min          -5.13105\n",
      "trainer/Log Pis Mean                   8.92721\n",
      "trainer/Log Pis Std                    8.22621\n",
      "trainer/Policy mu Mean                 0.0911285\n",
      "trainer/Policy mu Std                  1.16385\n",
      "trainer/Policy log std Mean           -0.937817\n",
      "trainer/Policy log std Std             0.533184\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        49542\n",
      "exploration/num paths total          934\n",
      "evaluation/num steps total          6535\n",
      "evaluation/num paths total            80\n",
      "evaluation/path length Mean          121.8\n",
      "evaluation/path length Std            21.3157\n",
      "evaluation/path length Max           150\n",
      "evaluation/path length Min            92\n",
      "evaluation/Rewards Mean                4.872\n",
      "evaluation/Rewards Std                 0.522433\n",
      "evaluation/Rewards Max                 6.23058\n",
      "evaluation/Rewards Min                 3.26252\n",
      "evaluation/Returns Mean              593.41\n",
      "evaluation/Returns Std               126.153\n",
      "evaluation/Returns Max               748.144\n",
      "evaluation/Returns Min               418.71\n",
      "evaluation/Estimation Bias Mean      105.244\n",
      "evaluation/Estimation Bias Std       126.357\n",
      "evaluation/EB/Q_True Mean             26.9705\n",
      "evaluation/EB/Q_True Std              81.6767\n",
      "evaluation/EB/Q_Pred Mean            132.214\n",
      "evaluation/EB/Q_Pred Std              93.4153\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           593.41\n",
      "evaluation/Actions Mean                0.0878438\n",
      "evaluation/Actions Std                 0.549069\n",
      "evaluation/Actions Max                 1\n",
      "evaluation/Actions Min                -0.99862\n",
      "time/backward_policy (s)               8.14133\n",
      "time/backward_zf1 (s)                  9.43483\n",
      "time/backward_zf2 (s)                  9.1411\n",
      "time/data sampling (s)                 1.5491\n",
      "time/data storing (s)                  0.0867908\n",
      "time/evaluation sampling (s)           0.602215\n",
      "time/exploration sampling (s)          2.47885\n",
      "time/logging (s)                       0.00356994\n",
      "time/preback_alpha (s)                 0.00515175\n",
      "time/preback_policy (s)               15.7525\n",
      "time/preback_start (s)                 0.901578\n",
      "time/preback_zf (s)                   36.2988\n",
      "time/saving (s)                        3.599e-06\n",
      "time/training (s)                     11.253\n",
      "time/epoch (s)                        95.6487\n",
      "time/total (s)                       775.435\n",
      "Epoch                                  7\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:32:21.998735 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 8 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 55000\n",
      "trainer/ZF1 Loss                      14.2487\n",
      "trainer/ZF2 Loss                      11.9769\n",
      "trainer/ZF Expert Reward              14.3598\n",
      "trainer/ZF Policy Reward               1.70526\n",
      "trainer/ZF CHI2 Term                  23.1978\n",
      "trainer/Policy Loss                  -94.7358\n",
      "trainer/expert_lambda Loss            11.6135\n",
      "trainer/expert_lambda Value           13.5539\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              26.6037\n",
      "trainer/Policy Param Norm             22.4827\n",
      "trainer/Zf1 Grad Norm               2961.64\n",
      "trainer/Zf1 Param Norm                62.1175\n",
      "trainer/Zf2 Grad Norm               2439.75\n",
      "trainer/Zf2 Param Norm                61.6682\n",
      "trainer/Z Expert Predictions Mean    438.883\n",
      "trainer/Z Expert Predictions Std      68.4961\n",
      "trainer/Z Expert Predictions Max     508.047\n",
      "trainer/Z Expert Predictions Min     254.211\n",
      "trainer/Z Policy Predictions Mean     91.3413\n",
      "trainer/Z Policy Predictions Std      75.9773\n",
      "trainer/Z Policy Predictions Max     304.787\n",
      "trainer/Z Policy Predictions Min       1.16441\n",
      "trainer/Z Expert Targets Mean        424.523\n",
      "trainer/Z Expert Targets Std          68.0814\n",
      "trainer/Z Expert Targets Max         498.397\n",
      "trainer/Z Expert Targets Min         234.46\n",
      "trainer/Z Policy Targets Mean         89.636\n",
      "trainer/Z Policy Targets Std          76.7587\n",
      "trainer/Z Policy Targets Max         297.734\n",
      "trainer/Z Policy Targets Min          -0.943063\n",
      "trainer/Log Pis Mean                   8.80441\n",
      "trainer/Log Pis Std                    9.4501\n",
      "trainer/Policy mu Mean                 0.0114269\n",
      "trainer/Policy mu Std                  1.14814\n",
      "trainer/Policy log std Mean           -0.914708\n",
      "trainer/Policy log std Std             0.623836\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        54659\n",
      "exploration/num paths total          995\n",
      "evaluation/num steps total          7759\n",
      "evaluation/num paths total            90\n",
      "evaluation/path length Mean          122.4\n",
      "evaluation/path length Std            28.2531\n",
      "evaluation/path length Max           179\n",
      "evaluation/path length Min            86\n",
      "evaluation/Rewards Mean                4.81584\n",
      "evaluation/Rewards Std                 0.448552\n",
      "evaluation/Rewards Max                 6.42678\n",
      "evaluation/Rewards Min                 3.4031\n",
      "evaluation/Returns Mean              589.459\n",
      "evaluation/Returns Std               142.677\n",
      "evaluation/Returns Max               918.757\n",
      "evaluation/Returns Min               408.456\n",
      "evaluation/Estimation Bias Mean       66.4229\n",
      "evaluation/Estimation Bias Std       128.37\n",
      "evaluation/EB/Q_True Mean             40.9192\n",
      "evaluation/EB/Q_True Std             108.287\n",
      "evaluation/EB/Q_Pred Mean            107.342\n",
      "evaluation/EB/Q_Pred Std              76.6255\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           589.459\n",
      "evaluation/Actions Mean                0.141373\n",
      "evaluation/Actions Std                 0.52545\n",
      "evaluation/Actions Max                 1\n",
      "evaluation/Actions Min                -0.999949\n",
      "time/backward_policy (s)               8.32361\n",
      "time/backward_zf1 (s)                  9.58315\n",
      "time/backward_zf2 (s)                  9.32536\n",
      "time/data sampling (s)                 1.56027\n",
      "time/data storing (s)                  0.0860217\n",
      "time/evaluation sampling (s)           0.753368\n",
      "time/exploration sampling (s)          2.38171\n",
      "time/logging (s)                       0.00317688\n",
      "time/preback_alpha (s)                 0.00516591\n",
      "time/preback_policy (s)               15.5234\n",
      "time/preback_start (s)                 0.903515\n",
      "time/preback_zf (s)                   36.3248\n",
      "time/saving (s)                        4.127e-06\n",
      "time/training (s)                     11.1959\n",
      "time/epoch (s)                        95.9695\n",
      "time/total (s)                       871.408\n",
      "Epoch                                  8\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:33:57.357178 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 9 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 60000\n",
      "trainer/ZF1 Loss                       4.41772\n",
      "trainer/ZF2 Loss                       9.3277\n",
      "trainer/ZF Expert Reward              17.8889\n",
      "trainer/ZF Policy Reward               2.88534\n",
      "trainer/ZF CHI2 Term                  21.5725\n",
      "trainer/Policy Loss                  -97.5312\n",
      "trainer/expert_lambda Loss            23.7013\n",
      "trainer/expert_lambda Value           13.998\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              18.8893\n",
      "trainer/Policy Param Norm             22.9994\n",
      "trainer/Zf1 Grad Norm               1017.59\n",
      "trainer/Zf1 Param Norm                64.4311\n",
      "trainer/Zf2 Grad Norm               1411.35\n",
      "trainer/Zf2 Param Norm                64.0363\n",
      "trainer/Z Expert Predictions Mean    331.018\n",
      "trainer/Z Expert Predictions Std      51.993\n",
      "trainer/Z Expert Predictions Max     424.294\n",
      "trainer/Z Expert Predictions Min     216.793\n",
      "trainer/Z Policy Predictions Mean     95.6279\n",
      "trainer/Z Policy Predictions Std      73.5016\n",
      "trainer/Z Policy Predictions Max     222.212\n",
      "trainer/Z Policy Predictions Min      -0.695573\n",
      "trainer/Z Expert Targets Mean        313.129\n",
      "trainer/Z Expert Targets Std          51.923\n",
      "trainer/Z Expert Targets Max         407.49\n",
      "trainer/Z Expert Targets Min         202.637\n",
      "trainer/Z Policy Targets Mean         92.7425\n",
      "trainer/Z Policy Targets Std          73.6821\n",
      "trainer/Z Policy Targets Max         220.218\n",
      "trainer/Z Policy Targets Min          -4.46749\n",
      "trainer/Log Pis Mean                   7.77443\n",
      "trainer/Log Pis Std                    9.0504\n",
      "trainer/Policy mu Mean                 0.17959\n",
      "trainer/Policy mu Std                  1.18786\n",
      "trainer/Policy log std Mean           -0.792741\n",
      "trainer/Policy log std Std             0.428602\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        59611\n",
      "exploration/num paths total         1057\n",
      "evaluation/num steps total          8749\n",
      "evaluation/num paths total           100\n",
      "evaluation/path length Mean           99\n",
      "evaluation/path length Std            20.4793\n",
      "evaluation/path length Max           144\n",
      "evaluation/path length Min            71\n",
      "evaluation/Rewards Mean                5.3733\n",
      "evaluation/Rewards Std                 0.379988\n",
      "evaluation/Rewards Max                 6.63412\n",
      "evaluation/Rewards Min                 4.84522\n",
      "evaluation/Returns Mean              531.957\n",
      "evaluation/Returns Std               105.645\n",
      "evaluation/Returns Max               754.552\n",
      "evaluation/Returns Min               378.253\n",
      "evaluation/Estimation Bias Mean       56.7017\n",
      "evaluation/Estimation Bias Std       104.848\n",
      "evaluation/EB/Q_True Mean             36.7389\n",
      "evaluation/EB/Q_True Std              98.2382\n",
      "evaluation/EB/Q_Pred Mean             93.4405\n",
      "evaluation/EB/Q_Pred Std              53.5517\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           531.957\n",
      "evaluation/Actions Mean                0.0772804\n",
      "evaluation/Actions Std                 0.506235\n",
      "evaluation/Actions Max                 0.988889\n",
      "evaluation/Actions Min                -0.996498\n",
      "time/backward_policy (s)               7.72817\n",
      "time/backward_zf1 (s)                  8.99124\n",
      "time/backward_zf2 (s)                  8.65619\n",
      "time/data sampling (s)                 1.52842\n",
      "time/data storing (s)                  0.0854455\n",
      "time/evaluation sampling (s)           0.704552\n",
      "time/exploration sampling (s)          2.36124\n",
      "time/logging (s)                       0.0033648\n",
      "time/preback_alpha (s)                 0.00509454\n",
      "time/preback_policy (s)               16.3998\n",
      "time/preback_start (s)                 0.888964\n",
      "time/preback_zf (s)                   36.1982\n",
      "time/saving (s)                        4.659e-06\n",
      "time/training (s)                     11.6079\n",
      "time/epoch (s)                        95.1587\n",
      "time/total (s)                       966.569\n",
      "Epoch                                  9\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:35:33.932912 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 10 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 65000\n",
      "trainer/ZF1 Loss                       3.21518\n",
      "trainer/ZF2 Loss                       1.08878\n",
      "trainer/ZF Expert Reward              16.0094\n",
      "trainer/ZF Policy Reward               1.81631\n",
      "trainer/ZF CHI2 Term                  15.866\n",
      "trainer/Policy Loss                  -85.4072\n",
      "trainer/expert_lambda Loss            19.0306\n",
      "trainer/expert_lambda Value           14.4329\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              18.5444\n",
      "trainer/Policy Param Norm             23.4277\n",
      "trainer/Zf1 Grad Norm                940.25\n",
      "trainer/Zf1 Param Norm                66.3044\n",
      "trainer/Zf2 Grad Norm                946.879\n",
      "trainer/Zf2 Param Norm                65.901\n",
      "trainer/Z Expert Predictions Mean    243.941\n",
      "trainer/Z Expert Predictions Std      31.8469\n",
      "trainer/Z Expert Predictions Max     306.777\n",
      "trainer/Z Expert Predictions Min     165.248\n",
      "trainer/Z Policy Predictions Mean     84.5871\n",
      "trainer/Z Policy Predictions Std      56.1011\n",
      "trainer/Z Policy Predictions Max     197.757\n",
      "trainer/Z Policy Predictions Min      -2.0871\n",
      "trainer/Z Expert Targets Mean        227.932\n",
      "trainer/Z Expert Targets Std          31.9204\n",
      "trainer/Z Expert Targets Max         296.155\n",
      "trainer/Z Expert Targets Min         145.311\n",
      "trainer/Z Policy Targets Mean         82.7708\n",
      "trainer/Z Policy Targets Std          56.5475\n",
      "trainer/Z Policy Targets Max         202.919\n",
      "trainer/Z Policy Targets Min          -3.55899\n",
      "trainer/Log Pis Mean                   6.17955\n",
      "trainer/Log Pis Std                    8.01782\n",
      "trainer/Policy mu Mean                 0.161098\n",
      "trainer/Policy mu Std                  1.11926\n",
      "trainer/Policy log std Mean           -0.736699\n",
      "trainer/Policy log std Std             0.396353\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        64511\n",
      "exploration/num paths total         1105\n",
      "evaluation/num steps total          9677\n",
      "evaluation/num paths total           110\n",
      "evaluation/path length Mean           92.8\n",
      "evaluation/path length Std            11.6688\n",
      "evaluation/path length Max           115\n",
      "evaluation/path length Min            79\n",
      "evaluation/Rewards Mean                5.40466\n",
      "evaluation/Rewards Std                 0.447433\n",
      "evaluation/Rewards Max                 6.93148\n",
      "evaluation/Rewards Min                 4.84776\n",
      "evaluation/Returns Mean              501.553\n",
      "evaluation/Returns Std                49.9096\n",
      "evaluation/Returns Max               592.407\n",
      "evaluation/Returns Min               437.386\n",
      "evaluation/Estimation Bias Mean       67.4924\n",
      "evaluation/Estimation Bias Std        93.531\n",
      "evaluation/EB/Q_True Mean             26.6556\n",
      "evaluation/EB/Q_True Std              78.8286\n",
      "evaluation/EB/Q_Pred Mean             94.1481\n",
      "evaluation/EB/Q_Pred Std              57.0128\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           501.553\n",
      "evaluation/Actions Mean                0.0824066\n",
      "evaluation/Actions Std                 0.479604\n",
      "evaluation/Actions Max                 0.990263\n",
      "evaluation/Actions Min                -0.974004\n",
      "time/backward_policy (s)               8.51663\n",
      "time/backward_zf1 (s)                  9.7954\n",
      "time/backward_zf2 (s)                  9.55986\n",
      "time/data sampling (s)                 1.57602\n",
      "time/data storing (s)                  0.0861496\n",
      "time/evaluation sampling (s)           0.473053\n",
      "time/exploration sampling (s)          2.45741\n",
      "time/logging (s)                       0.00418567\n",
      "time/preback_alpha (s)                 0.00523005\n",
      "time/preback_policy (s)               15.3941\n",
      "time/preback_start (s)                 0.909505\n",
      "time/preback_zf (s)                   36.4188\n",
      "time/saving (s)                        5.922e-06\n",
      "time/training (s)                     11.175\n",
      "time/epoch (s)                        96.3713\n",
      "time/total (s)                      1062.94\n",
      "Epoch                                 10\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:37:08.949422 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 11 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 70000\n",
      "trainer/ZF1 Loss                       1.28694\n",
      "trainer/ZF2 Loss                      -0.20566\n",
      "trainer/ZF Expert Reward              15.6729\n",
      "trainer/ZF Policy Reward               1.2523\n",
      "trainer/ZF CHI2 Term                  14.6897\n",
      "trainer/Policy Loss                  -77.9079\n",
      "trainer/expert_lambda Loss            16.807\n",
      "trainer/expert_lambda Value           14.8772\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              12.3655\n",
      "trainer/Policy Param Norm             23.9002\n",
      "trainer/Zf1 Grad Norm                865.46\n",
      "trainer/Zf1 Param Norm                68.1757\n",
      "trainer/Zf2 Grad Norm                700.004\n",
      "trainer/Zf2 Param Norm                67.7866\n",
      "trainer/Z Expert Predictions Mean    166.429\n",
      "trainer/Z Expert Predictions Std      37.6682\n",
      "trainer/Z Expert Predictions Max     288.287\n",
      "trainer/Z Expert Predictions Min     114.653\n",
      "trainer/Z Policy Predictions Mean     76.3125\n",
      "trainer/Z Policy Predictions Std      51.817\n",
      "trainer/Z Policy Predictions Max     197.381\n",
      "trainer/Z Policy Predictions Min       0.930935\n",
      "trainer/Z Expert Targets Mean        150.756\n",
      "trainer/Z Expert Targets Std          38.487\n",
      "trainer/Z Expert Targets Max         273.367\n",
      "trainer/Z Expert Targets Min          96.3177\n",
      "trainer/Z Policy Targets Mean         75.0602\n",
      "trainer/Z Policy Targets Std          51.9218\n",
      "trainer/Z Policy Targets Max         199.233\n",
      "trainer/Z Policy Targets Min          -1.24796\n",
      "trainer/Log Pis Mean                   4.27961\n",
      "trainer/Log Pis Std                    7.40835\n",
      "trainer/Policy mu Mean                 0.238856\n",
      "trainer/Policy mu Std                  1.03299\n",
      "trainer/Policy log std Mean           -0.671504\n",
      "trainer/Policy log std Std             0.381539\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        69473\n",
      "exploration/num paths total         1154\n",
      "evaluation/num steps total         10560\n",
      "evaluation/num paths total           120\n",
      "evaluation/path length Mean           88.3\n",
      "evaluation/path length Std            16.371\n",
      "evaluation/path length Max           116\n",
      "evaluation/path length Min            63\n",
      "evaluation/Rewards Mean                4.74082\n",
      "evaluation/Rewards Std                 0.48224\n",
      "evaluation/Rewards Max                 5.33646\n",
      "evaluation/Rewards Min                 3.24521\n",
      "evaluation/Returns Mean              418.614\n",
      "evaluation/Returns Std                65.843\n",
      "evaluation/Returns Max               534.307\n",
      "evaluation/Returns Min               322.503\n",
      "evaluation/Estimation Bias Mean       65.3397\n",
      "evaluation/Estimation Bias Std        69.9145\n",
      "evaluation/EB/Q_True Mean             23.885\n",
      "evaluation/EB/Q_True Std              70.5646\n",
      "evaluation/EB/Q_Pred Mean             89.2247\n",
      "evaluation/EB/Q_Pred Std              42.4161\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           418.614\n",
      "evaluation/Actions Mean                0.154999\n",
      "evaluation/Actions Std                 0.48873\n",
      "evaluation/Actions Max                 0.999146\n",
      "evaluation/Actions Min                -0.981687\n",
      "time/backward_policy (s)               7.5508\n",
      "time/backward_zf1 (s)                  8.82346\n",
      "time/backward_zf2 (s)                  8.46167\n",
      "time/data sampling (s)                 1.53778\n",
      "time/data storing (s)                  0.085377\n",
      "time/evaluation sampling (s)           0.575035\n",
      "time/exploration sampling (s)          2.31884\n",
      "time/logging (s)                       0.00306288\n",
      "time/preback_alpha (s)                 0.00506951\n",
      "time/preback_policy (s)               16.6461\n",
      "time/preback_start (s)                 0.886759\n",
      "time/preback_zf (s)                   36.1663\n",
      "time/saving (s)                        4.396e-06\n",
      "time/training (s)                     11.7537\n",
      "time/epoch (s)                        94.814\n",
      "time/total (s)                      1157.76\n",
      "Epoch                                 11\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:38:44.325417 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 12 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 75000\n",
      "trainer/ZF1 Loss                     -10.2761\n",
      "trainer/ZF2 Loss                      -8.88297\n",
      "trainer/ZF Expert Reward              17.8115\n",
      "trainer/ZF Policy Reward               1.79601\n",
      "trainer/ZF CHI2 Term                   7.20796\n",
      "trainer/Policy Loss                  -77.664\n",
      "trainer/expert_lambda Loss             6.78697\n",
      "trainer/expert_lambda Value           15.3341\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              15.4604\n",
      "trainer/Policy Param Norm             24.2628\n",
      "trainer/Zf1 Grad Norm                213.504\n",
      "trainer/Zf1 Param Norm                69.9134\n",
      "trainer/Zf2 Grad Norm                269.861\n",
      "trainer/Zf2 Param Norm                69.5988\n",
      "trainer/Z Expert Predictions Mean    117.73\n",
      "trainer/Z Expert Predictions Std      13.3386\n",
      "trainer/Z Expert Predictions Max     165.641\n",
      "trainer/Z Expert Predictions Min      99.6968\n",
      "trainer/Z Policy Predictions Mean     76.7968\n",
      "trainer/Z Policy Predictions Std      42.3138\n",
      "trainer/Z Policy Predictions Max     155.032\n",
      "trainer/Z Policy Predictions Min       1.5905\n",
      "trainer/Z Expert Targets Mean         99.9181\n",
      "trainer/Z Expert Targets Std          13.6328\n",
      "trainer/Z Expert Targets Max         146.511\n",
      "trainer/Z Expert Targets Min          79.8047\n",
      "trainer/Z Policy Targets Mean         75.0008\n",
      "trainer/Z Policy Targets Std          42.3938\n",
      "trainer/Z Policy Targets Max         152.878\n",
      "trainer/Z Policy Targets Min           0.652538\n",
      "trainer/Log Pis Mean                   2.60883\n",
      "trainer/Log Pis Std                    8.73447\n",
      "trainer/Policy mu Mean                 0.224989\n",
      "trainer/Policy mu Std                  0.972626\n",
      "trainer/Policy log std Mean           -0.614399\n",
      "trainer/Policy log std Std             0.527541\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        74545\n",
      "exploration/num paths total         1231\n",
      "evaluation/num steps total         11779\n",
      "evaluation/num paths total           131\n",
      "evaluation/path length Mean          110.818\n",
      "evaluation/path length Std            25.2544\n",
      "evaluation/path length Max           149\n",
      "evaluation/path length Min            63\n",
      "evaluation/Rewards Mean                4.86651\n",
      "evaluation/Rewards Std                 0.469336\n",
      "evaluation/Rewards Max                 6.43374\n",
      "evaluation/Rewards Min                 3.27104\n",
      "evaluation/Returns Mean              539.298\n",
      "evaluation/Returns Std               115.246\n",
      "evaluation/Returns Max               748.976\n",
      "evaluation/Returns Min               338.365\n",
      "evaluation/Estimation Bias Mean       63.9953\n",
      "evaluation/Estimation Bias Std        93.5487\n",
      "evaluation/EB/Q_True Mean             28.6338\n",
      "evaluation/EB/Q_True Std              86.0473\n",
      "evaluation/EB/Q_Pred Mean             92.6291\n",
      "evaluation/EB/Q_Pred Std              48.6651\n",
      "evaluation/Num Paths                  11\n",
      "evaluation/Average Returns           539.298\n",
      "evaluation/Actions Mean                0.073014\n",
      "evaluation/Actions Std                 0.469509\n",
      "evaluation/Actions Max                 0.995917\n",
      "evaluation/Actions Min                -0.993863\n",
      "time/backward_policy (s)               8.00974\n",
      "time/backward_zf1 (s)                  9.26925\n",
      "time/backward_zf2 (s)                  8.97001\n",
      "time/data sampling (s)                 1.5483\n",
      "time/data storing (s)                  0.0858308\n",
      "time/evaluation sampling (s)           0.643665\n",
      "time/exploration sampling (s)          2.37985\n",
      "time/logging (s)                       0.00477827\n",
      "time/preback_alpha (s)                 0.00511385\n",
      "time/preback_policy (s)               15.9019\n",
      "time/preback_start (s)                 0.893564\n",
      "time/preback_zf (s)                   36.1539\n",
      "time/saving (s)                        5.5e-06\n",
      "time/training (s)                     11.3118\n",
      "time/epoch (s)                        95.1777\n",
      "time/total (s)                      1252.94\n",
      "Epoch                                 12\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:40:19.848579 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 13 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 80000\n",
      "trainer/ZF1 Loss                      -0.945024\n",
      "trainer/ZF2 Loss                      -2.06444\n",
      "trainer/ZF Expert Reward              17.6373\n",
      "trainer/ZF Policy Reward               2.26942\n",
      "trainer/ZF CHI2 Term                  14.9313\n",
      "trainer/Policy Loss                  -73.366\n",
      "trainer/expert_lambda Loss            19.8955\n",
      "trainer/expert_lambda Value           15.8103\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm               7.82438\n",
      "trainer/Policy Param Norm             24.6735\n",
      "trainer/Zf1 Grad Norm                473.179\n",
      "trainer/Zf1 Param Norm                71.4986\n",
      "trainer/Zf2 Grad Norm                244.144\n",
      "trainer/Zf2 Param Norm                71.2487\n",
      "trainer/Z Expert Predictions Mean    131.824\n",
      "trainer/Z Expert Predictions Std      11.1605\n",
      "trainer/Z Expert Predictions Max     156.958\n",
      "trainer/Z Expert Predictions Min     110.314\n",
      "trainer/Z Policy Predictions Mean     72.6167\n",
      "trainer/Z Policy Predictions Std      44.7949\n",
      "trainer/Z Policy Predictions Max     143.821\n",
      "trainer/Z Policy Predictions Min       1.25136\n",
      "trainer/Z Expert Targets Mean        114.186\n",
      "trainer/Z Expert Targets Std          12.9668\n",
      "trainer/Z Expert Targets Max         141.019\n",
      "trainer/Z Expert Targets Min          85.3411\n",
      "trainer/Z Policy Targets Mean         70.3472\n",
      "trainer/Z Policy Targets Std          44.8849\n",
      "trainer/Z Policy Targets Max         140.992\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  -0.556839\n",
      "trainer/Log Pis Std                    6.08971\n",
      "trainer/Policy mu Mean                 0.151174\n",
      "trainer/Policy mu Std                  0.856364\n",
      "trainer/Policy log std Mean           -0.562321\n",
      "trainer/Policy log std Std             0.296824\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        79537\n",
      "exploration/num paths total         1294\n",
      "evaluation/num steps total         12635\n",
      "evaluation/num paths total           141\n",
      "evaluation/path length Mean           85.6\n",
      "evaluation/path length Std             5.95315\n",
      "evaluation/path length Max            96\n",
      "evaluation/path length Min            80\n",
      "evaluation/Rewards Mean                5.48478\n",
      "evaluation/Rewards Std                 0.426718\n",
      "evaluation/Rewards Max                 6.64033\n",
      "evaluation/Rewards Min                 4.9141\n",
      "evaluation/Returns Mean              469.497\n",
      "evaluation/Returns Std                32.5086\n",
      "evaluation/Returns Max               526.629\n",
      "evaluation/Returns Min               434.452\n",
      "evaluation/Estimation Bias Mean       50.0536\n",
      "evaluation/Estimation Bias Std        81.0121\n",
      "evaluation/EB/Q_True Mean             23.1208\n",
      "evaluation/EB/Q_True Std              72.1163\n",
      "evaluation/EB/Q_Pred Mean             73.1743\n",
      "evaluation/EB/Q_Pred Std              46.084\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           469.497\n",
      "evaluation/Actions Mean                0.124394\n",
      "evaluation/Actions Std                 0.515981\n",
      "evaluation/Actions Max                 0.962221\n",
      "evaluation/Actions Min                -0.98221\n",
      "time/backward_policy (s)               8.54962\n",
      "time/backward_zf1 (s)                  9.79303\n",
      "time/backward_zf2 (s)                  9.5998\n",
      "time/data sampling (s)                 1.56148\n",
      "time/data storing (s)                  0.0861055\n",
      "time/evaluation sampling (s)           0.418198\n",
      "time/exploration sampling (s)          2.32299\n",
      "time/logging (s)                       0.00475482\n",
      "time/preback_alpha (s)                 0.00519926\n",
      "time/preback_policy (s)               15.0361\n",
      "time/preback_start (s)                 0.90043\n",
      "time/preback_zf (s)                   36.2854\n",
      "time/saving (s)                        6.12e-06\n",
      "time/training (s)                     10.7551\n",
      "time/epoch (s)                        95.3183\n",
      "time/total (s)                      1348.26\n",
      "Epoch                                 13\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:41:56.086478 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 14 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 85000\n",
      "trainer/ZF1 Loss                       7.89083\n",
      "trainer/ZF2 Loss                       6.70875\n",
      "trainer/ZF Expert Reward              19.3106\n",
      "trainer/ZF Policy Reward               3.38359\n",
      "trainer/ZF CHI2 Term                  24.4181\n",
      "trainer/Policy Loss                  -73.52\n",
      "trainer/expert_lambda Loss            31.8529\n",
      "trainer/expert_lambda Value           16.2704\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              16.1386\n",
      "trainer/Policy Param Norm             25.2291\n",
      "trainer/Zf1 Grad Norm                554.693\n",
      "trainer/Zf1 Param Norm                73.6344\n",
      "trainer/Zf2 Grad Norm                535.431\n",
      "trainer/Zf2 Param Norm                73.3643\n",
      "trainer/Z Expert Predictions Mean    226.381\n",
      "trainer/Z Expert Predictions Std      25.8931\n",
      "trainer/Z Expert Predictions Max     279.89\n",
      "trainer/Z Expert Predictions Min     148.076\n",
      "trainer/Z Policy Predictions Mean     72.9401\n",
      "trainer/Z Policy Predictions Std      47.6991\n",
      "trainer/Z Policy Predictions Max     190.713\n",
      "trainer/Z Policy Predictions Min       3.03798\n",
      "trainer/Z Expert Targets Mean        207.07\n",
      "trainer/Z Expert Targets Std          27.5532\n",
      "trainer/Z Expert Targets Max         259.689\n",
      "trainer/Z Expert Targets Min         120.474\n",
      "trainer/Z Policy Targets Mean         69.5565\n",
      "trainer/Z Policy Targets Std          48.159\n",
      "trainer/Z Policy Targets Max         215.873\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                   2.59685\n",
      "trainer/Log Pis Std                    6.3085\n",
      "trainer/Policy mu Mean                 0.161819\n",
      "trainer/Policy mu Std                  0.921611\n",
      "trainer/Policy log std Mean           -0.710407\n",
      "trainer/Policy log std Std             0.389088\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        84484\n",
      "exploration/num paths total         1339\n",
      "evaluation/num steps total         13754\n",
      "evaluation/num paths total           151\n",
      "evaluation/path length Mean          111.9\n",
      "evaluation/path length Std            24.4927\n",
      "evaluation/path length Max           155\n",
      "evaluation/path length Min            74\n",
      "evaluation/Rewards Mean                5.26352\n",
      "evaluation/Rewards Std                 0.376946\n",
      "evaluation/Rewards Max                 6.61742\n",
      "evaluation/Rewards Min                 4.82055\n",
      "evaluation/Returns Mean              588.988\n",
      "evaluation/Returns Std               125.309\n",
      "evaluation/Returns Max               813.66\n",
      "evaluation/Returns Min               387.821\n",
      "evaluation/Estimation Bias Mean      113.694\n",
      "evaluation/Estimation Bias Std       165.727\n",
      "evaluation/EB/Q_True Mean             36.922\n",
      "evaluation/EB/Q_True Std             100.789\n",
      "evaluation/EB/Q_Pred Mean            150.616\n",
      "evaluation/EB/Q_Pred Std             133.897\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           588.988\n",
      "evaluation/Actions Mean                0.143209\n",
      "evaluation/Actions Std                 0.495405\n",
      "evaluation/Actions Max                 0.996455\n",
      "evaluation/Actions Min                -0.986278\n",
      "time/backward_policy (s)               8.68365\n",
      "time/backward_zf1 (s)                  9.9199\n",
      "time/backward_zf2 (s)                  9.72702\n",
      "time/data sampling (s)                 1.57978\n",
      "time/data storing (s)                  0.0861863\n",
      "time/evaluation sampling (s)           0.681305\n",
      "time/exploration sampling (s)          2.4845\n",
      "time/logging (s)                       0.00428935\n",
      "time/preback_alpha (s)                 0.0052218\n",
      "time/preback_policy (s)               14.9075\n",
      "time/preback_start (s)                 0.905538\n",
      "time/preback_zf (s)                   36.3349\n",
      "time/saving (s)                        5.309e-06\n",
      "time/training (s)                     10.7127\n",
      "time/epoch (s)                        96.0325\n",
      "time/total (s)                      1444.3\n",
      "Epoch                                 14\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:43:32.004675 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 15 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 90000\n",
      "trainer/ZF1 Loss                      22.6549\n",
      "trainer/ZF2 Loss                      21.0572\n",
      "trainer/ZF Expert Reward              18.3494\n",
      "trainer/ZF Policy Reward               1.14149\n",
      "trainer/ZF CHI2 Term                  36.4798\n",
      "trainer/Policy Loss                 -103.703\n",
      "trainer/expert_lambda Loss            20.8589\n",
      "trainer/expert_lambda Value           16.7138\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              84.2776\n",
      "trainer/Policy Param Norm             25.7543\n",
      "trainer/Zf1 Grad Norm               2251.22\n",
      "trainer/Zf1 Param Norm                76.7398\n",
      "trainer/Zf2 Grad Norm               2483.05\n",
      "trainer/Zf2 Param Norm                76.4499\n",
      "trainer/Z Expert Predictions Mean    379.9\n",
      "trainer/Z Expert Predictions Std      38.6563\n",
      "trainer/Z Expert Predictions Max     452.952\n",
      "trainer/Z Expert Predictions Min     261.332\n",
      "trainer/Z Policy Predictions Mean     97.3061\n",
      "trainer/Z Policy Predictions Std      93.6911\n",
      "trainer/Z Policy Predictions Max     403.074\n",
      "trainer/Z Policy Predictions Min      -3.52847\n",
      "trainer/Z Expert Targets Mean        361.551\n",
      "trainer/Z Expert Targets Std          39.8104\n",
      "trainer/Z Expert Targets Max         437.935\n",
      "trainer/Z Expert Targets Min         232.171\n",
      "trainer/Z Policy Targets Mean         96.1646\n",
      "trainer/Z Policy Targets Std          95.1835\n",
      "trainer/Z Policy Targets Max         410.969\n",
      "trainer/Z Policy Targets Min          -6.16962\n",
      "trainer/Log Pis Mean                   8.67538\n",
      "trainer/Log Pis Std                   11.0409\n",
      "trainer/Policy mu Mean                 0.214379\n",
      "trainer/Policy mu Std                  1.00933\n",
      "trainer/Policy log std Mean           -0.971408\n",
      "trainer/Policy log std Std             0.807671\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        89595\n",
      "exploration/num paths total         1385\n",
      "evaluation/num steps total         14618\n",
      "evaluation/num paths total           161\n",
      "evaluation/path length Mean           86.4\n",
      "evaluation/path length Std             7.81281\n",
      "evaluation/path length Max           100\n",
      "evaluation/path length Min            75\n",
      "evaluation/Rewards Mean                5.41962\n",
      "evaluation/Rewards Std                 0.452848\n",
      "evaluation/Rewards Max                 6.58868\n",
      "evaluation/Rewards Min                 4.85417\n",
      "evaluation/Returns Mean              468.256\n",
      "evaluation/Returns Std                39.4014\n",
      "evaluation/Returns Max               537.021\n",
      "evaluation/Returns Min               409.108\n",
      "evaluation/Estimation Bias Mean      102.477\n",
      "evaluation/Estimation Bias Std       139.366\n",
      "evaluation/EB/Q_True Mean             23.9374\n",
      "evaluation/EB/Q_True Std              73.3123\n",
      "evaluation/EB/Q_Pred Mean            126.414\n",
      "evaluation/EB/Q_Pred Std             118.822\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           468.256\n",
      "evaluation/Actions Mean                0.0620439\n",
      "evaluation/Actions Std                 0.451328\n",
      "evaluation/Actions Max                 0.983738\n",
      "evaluation/Actions Min                -0.984229\n",
      "time/backward_policy (s)               8.54754\n",
      "time/backward_zf1 (s)                  9.79883\n",
      "time/backward_zf2 (s)                  9.57628\n",
      "time/data sampling (s)                 1.59389\n",
      "time/data storing (s)                  0.0863393\n",
      "time/evaluation sampling (s)           0.503466\n",
      "time/exploration sampling (s)          2.43062\n",
      "time/logging (s)                       0.00364407\n",
      "time/preback_alpha (s)                 0.00518386\n",
      "time/preback_policy (s)               15.0981\n",
      "time/preback_start (s)                 0.908845\n",
      "time/preback_zf (s)                   36.2946\n",
      "time/saving (s)                        5.624e-06\n",
      "time/training (s)                     10.8649\n",
      "time/epoch (s)                        95.7123\n",
      "time/total (s)                      1540.01\n",
      "Epoch                                 15\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:45:07.646522 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 16 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 95000\n",
      "trainer/ZF1 Loss                      33.6981\n",
      "trainer/ZF2 Loss                      40.3893\n",
      "trainer/ZF Expert Reward              17.4889\n",
      "trainer/ZF Policy Reward               2.27862\n",
      "trainer/ZF CHI2 Term                  50.5401\n",
      "trainer/Policy Loss                 -105.141\n",
      "trainer/expert_lambda Loss            32.7665\n",
      "trainer/expert_lambda Value           17.1584\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              71.3549\n",
      "trainer/Policy Param Norm             26.1994\n",
      "trainer/Zf1 Grad Norm               4431.95\n",
      "trainer/Zf1 Param Norm                79.3814\n",
      "trainer/Zf2 Grad Norm               4645.86\n",
      "trainer/Zf2 Param Norm                79.0228\n",
      "trainer/Z Expert Predictions Mean    412.861\n",
      "trainer/Z Expert Predictions Std      59.1171\n",
      "trainer/Z Expert Predictions Max     483.055\n",
      "trainer/Z Expert Predictions Min     220.017\n",
      "trainer/Z Policy Predictions Mean    100.507\n",
      "trainer/Z Policy Predictions Std     102.305\n",
      "trainer/Z Policy Predictions Max     385.042\n",
      "trainer/Z Policy Predictions Min      -1.63575\n",
      "trainer/Z Expert Targets Mean        395.372\n",
      "trainer/Z Expert Targets Std          59.4259\n",
      "trainer/Z Expert Targets Max         468.046\n",
      "trainer/Z Expert Targets Min         191.817\n",
      "trainer/Z Policy Targets Mean         98.2287\n",
      "trainer/Z Policy Targets Std         103.428\n",
      "trainer/Z Policy Targets Max         403.143\n",
      "trainer/Z Policy Targets Min          -4.3154\n",
      "trainer/Log Pis Mean                   9.91184\n",
      "trainer/Log Pis Std                   10.3253\n",
      "trainer/Policy mu Mean                 0.193536\n",
      "trainer/Policy mu Std                  1.06177\n",
      "trainer/Policy log std Mean           -0.998205\n",
      "trainer/Policy log std Std             0.817285\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        94005\n",
      "exploration/num paths total         1426\n",
      "evaluation/num steps total         15707\n",
      "evaluation/num paths total           171\n",
      "evaluation/path length Mean          108.9\n",
      "evaluation/path length Std            15.4172\n",
      "evaluation/path length Max           131\n",
      "evaluation/path length Min            88\n",
      "evaluation/Rewards Mean                5.32459\n",
      "evaluation/Rewards Std                 0.301002\n",
      "evaluation/Rewards Max                 6.28941\n",
      "evaluation/Rewards Min                 4.89812\n",
      "evaluation/Returns Mean              579.848\n",
      "evaluation/Returns Std                80.1344\n",
      "evaluation/Returns Max               692.649\n",
      "evaluation/Returns Min               472.914\n",
      "evaluation/Estimation Bias Mean      123.44\n",
      "evaluation/Estimation Bias Std       144.107\n",
      "evaluation/EB/Q_True Mean             29.1218\n",
      "evaluation/EB/Q_True Std              86.7679\n",
      "evaluation/EB/Q_Pred Mean            152.562\n",
      "evaluation/EB/Q_Pred Std             109.655\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           579.848\n",
      "evaluation/Actions Mean                0.0460923\n",
      "evaluation/Actions Std                 0.510974\n",
      "evaluation/Actions Max                 0.993673\n",
      "evaluation/Actions Min                -0.996344\n",
      "time/backward_policy (s)               8.37803\n",
      "time/backward_zf1 (s)                  9.61645\n",
      "time/backward_zf2 (s)                  9.38105\n",
      "time/data sampling (s)                 1.56509\n",
      "time/data storing (s)                  0.0860541\n",
      "time/evaluation sampling (s)           0.6014\n",
      "time/exploration sampling (s)          2.41425\n",
      "time/logging (s)                       0.00360781\n",
      "time/preback_alpha (s)                 0.00515189\n",
      "time/preback_policy (s)               15.3245\n",
      "time/preback_start (s)                 0.898816\n",
      "time/preback_zf (s)                   36.1969\n",
      "time/saving (s)                        4.465e-06\n",
      "time/training (s)                     10.9673\n",
      "time/epoch (s)                        95.4387\n",
      "time/total (s)                      1635.45\n",
      "Epoch                                 16\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:46:43.882935 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 17 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 100000\n",
      "trainer/ZF1 Loss                       13.4385\n",
      "trainer/ZF2 Loss                       10.7205\n",
      "trainer/ZF Expert Reward               20.6389\n",
      "trainer/ZF Policy Reward                2.65048\n",
      "trainer/ZF CHI2 Term                   29.2924\n",
      "trainer/Policy Loss                   -82.1486\n",
      "trainer/expert_lambda Loss             38.294\n",
      "trainer/expert_lambda Value            17.6056\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               22.7372\n",
      "trainer/Policy Param Norm              26.6977\n",
      "trainer/Zf1 Grad Norm                1414.07\n",
      "trainer/Zf1 Param Norm                 81.2261\n",
      "trainer/Zf2 Grad Norm                1229.1\n",
      "trainer/Zf2 Param Norm                 80.8425\n",
      "trainer/Z Expert Predictions Mean     355.263\n",
      "trainer/Z Expert Predictions Std       78.4889\n",
      "trainer/Z Expert Predictions Max      482.012\n",
      "trainer/Z Expert Predictions Min      168.73\n",
      "trainer/Z Policy Predictions Mean      80.8638\n",
      "trainer/Z Policy Predictions Std       71.531\n",
      "trainer/Z Policy Predictions Max      237.181\n",
      "trainer/Z Policy Predictions Min        1.41272\n",
      "trainer/Z Expert Targets Mean         334.624\n",
      "trainer/Z Expert Targets Std           78.744\n",
      "trainer/Z Expert Targets Max          469.003\n",
      "trainer/Z Expert Targets Min          148.688\n",
      "trainer/Z Policy Targets Mean          78.2133\n",
      "trainer/Z Policy Targets Std           72.0816\n",
      "trainer/Z Policy Targets Max          234.184\n",
      "trainer/Z Policy Targets Min           -1.72867\n",
      "trainer/Log Pis Mean                    5.90888\n",
      "trainer/Log Pis Std                     7.99786\n",
      "trainer/Policy mu Mean                  0.11997\n",
      "trainer/Policy mu Std                   1.08287\n",
      "trainer/Policy log std Mean            -0.739966\n",
      "trainer/Policy log std Std              0.525405\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total         99072\n",
      "exploration/num paths total          1474\n",
      "evaluation/num steps total          17054\n",
      "evaluation/num paths total            181\n",
      "evaluation/path length Mean           134.7\n",
      "evaluation/path length Std             22.8475\n",
      "evaluation/path length Max            170\n",
      "evaluation/path length Min             97\n",
      "evaluation/Rewards Mean                 5.229\n",
      "evaluation/Rewards Std                  0.231321\n",
      "evaluation/Rewards Max                  6.10166\n",
      "evaluation/Rewards Min                  4.88707\n",
      "evaluation/Returns Mean               704.346\n",
      "evaluation/Returns Std                116.092\n",
      "evaluation/Returns Max                888.802\n",
      "evaluation/Returns Min                512.785\n",
      "evaluation/Estimation Bias Mean       107.132\n",
      "evaluation/Estimation Bias Std        132.803\n",
      "evaluation/EB/Q_True Mean              34.9934\n",
      "evaluation/EB/Q_True Std              100.959\n",
      "evaluation/EB/Q_Pred Mean             142.126\n",
      "evaluation/EB/Q_Pred Std               85.154\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns            704.346\n",
      "evaluation/Actions Mean                 0.0734716\n",
      "evaluation/Actions Std                  0.497942\n",
      "evaluation/Actions Max                  0.995162\n",
      "evaluation/Actions Min                 -0.985836\n",
      "time/backward_policy (s)                8.59575\n",
      "time/backward_zf1 (s)                   9.85192\n",
      "time/backward_zf2 (s)                   9.63224\n",
      "time/data sampling (s)                  1.60209\n",
      "time/data storing (s)                   0.0872314\n",
      "time/evaluation sampling (s)            0.821227\n",
      "time/exploration sampling (s)           2.39991\n",
      "time/logging (s)                        0.00505624\n",
      "time/preback_alpha (s)                  0.00517907\n",
      "time/preback_policy (s)                15.0282\n",
      "time/preback_start (s)                  0.910362\n",
      "time/preback_zf (s)                    36.2469\n",
      "time/saving (s)                         5.801e-06\n",
      "time/training (s)                      10.8481\n",
      "time/epoch (s)                         96.0342\n",
      "time/total (s)                       1731.49\n",
      "Epoch                                  17\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 22:48:20.572162 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 18 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 105000\n",
      "trainer/ZF1 Loss                        2.249\n",
      "trainer/ZF2 Loss                        2.05266\n",
      "trainer/ZF Expert Reward               19.6709\n",
      "trainer/ZF Policy Reward                3.23521\n",
      "trainer/ZF CHI2 Term                   17.8734\n",
      "trainer/Policy Loss                   -80.9224\n",
      "trainer/expert_lambda Loss             14.8765\n",
      "trainer/expert_lambda Value            18.0547\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               24.8733\n",
      "trainer/Policy Param Norm              27.274\n",
      "trainer/Zf1 Grad Norm                 951.419\n",
      "trainer/Zf1 Param Norm                 83.4196\n",
      "trainer/Zf2 Grad Norm                 963.297\n",
      "trainer/Zf2 Param Norm                 82.9558\n",
      "trainer/Z Expert Predictions Mean     405.515\n",
      "trainer/Z Expert Predictions Std       75.8027\n",
      "trainer/Z Expert Predictions Max      529.536\n",
      "trainer/Z Expert Predictions Min      192.279\n",
      "trainer/Z Policy Predictions Mean      78.9606\n",
      "trainer/Z Policy Predictions Std       62.5139\n",
      "trainer/Z Policy Predictions Max      339.184\n",
      "trainer/Z Policy Predictions Min        0.419426\n",
      "trainer/Z Expert Targets Mean         385.844\n",
      "trainer/Z Expert Targets Std           76.6302\n",
      "trainer/Z Expert Targets Max          514.922\n",
      "trainer/Z Expert Targets Min          170.859\n",
      "trainer/Z Policy Targets Mean          75.7254\n",
      "trainer/Z Policy Targets Std           62.3249\n",
      "trainer/Z Policy Targets Max          338.64\n",
      "trainer/Z Policy Targets Min           -1.36825\n",
      "trainer/Log Pis Mean                    4.77338\n",
      "trainer/Log Pis Std                     7.6001\n",
      "trainer/Policy mu Mean                  0.0428568\n",
      "trainer/Policy mu Std                   1.02893\n",
      "trainer/Policy log std Mean            -0.75308\n",
      "trainer/Policy log std Std              0.480808\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        103203\n",
      "exploration/num paths total          1501\n",
      "evaluation/num steps total          18552\n",
      "evaluation/num paths total            192\n",
      "evaluation/path length Mean           136.182\n",
      "evaluation/path length Std             32.6798\n",
      "evaluation/path length Max            217\n",
      "evaluation/path length Min            112\n",
      "evaluation/Rewards Mean                 5.2053\n",
      "evaluation/Rewards Std                  0.361656\n",
      "evaluation/Rewards Max                  6.17274\n",
      "evaluation/Rewards Min                  3.80806\n",
      "evaluation/Returns Mean               708.867\n",
      "evaluation/Returns Std                143.931\n",
      "evaluation/Returns Max               1067.71\n",
      "evaluation/Returns Min                599.049\n",
      "evaluation/Estimation Bias Mean       166.934\n",
      "evaluation/Estimation Bias Std        153.801\n",
      "evaluation/EB/Q_True Mean              29.0659\n",
      "evaluation/EB/Q_True Std               80.3892\n",
      "evaluation/EB/Q_Pred Mean             196\n",
      "evaluation/EB/Q_Pred Std              145.963\n",
      "evaluation/Num Paths                   11\n",
      "evaluation/Average Returns            708.867\n",
      "evaluation/Actions Mean                 0.0936838\n",
      "evaluation/Actions Std                  0.551385\n",
      "evaluation/Actions Max                  0.999739\n",
      "evaluation/Actions Min                 -0.997908\n",
      "time/backward_policy (s)                8.70406\n",
      "time/backward_zf1 (s)                   9.95311\n",
      "time/backward_zf2 (s)                   9.76139\n",
      "time/data sampling (s)                  1.60602\n",
      "time/data storing (s)                   0.0858668\n",
      "time/evaluation sampling (s)            1.1269\n",
      "time/exploration sampling (s)           2.4959\n",
      "time/logging (s)                        0.00396333\n",
      "time/preback_alpha (s)                  0.00520248\n",
      "time/preback_policy (s)                14.8404\n",
      "time/preback_start (s)                  0.906178\n",
      "time/preback_zf (s)                    36.241\n",
      "time/saving (s)                         4.265e-06\n",
      "time/training (s)                      10.7538\n",
      "time/epoch (s)                         96.4838\n",
      "time/total (s)                       1827.98\n",
      "Epoch                                  18\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 22:49:56.410628 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 19 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 110000\n",
      "trainer/ZF1 Loss                       15.6497\n",
      "trainer/ZF2 Loss                       11.9975\n",
      "trainer/ZF Expert Reward               20.0859\n",
      "trainer/ZF Policy Reward                3.42791\n",
      "trainer/ZF CHI2 Term                   29.0849\n",
      "trainer/Policy Loss                   -95.0687\n",
      "trainer/expert_lambda Loss              8.15816\n",
      "trainer/expert_lambda Value            18.5122\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               62.3881\n",
      "trainer/Policy Param Norm              27.9307\n",
      "trainer/Zf1 Grad Norm                2845.94\n",
      "trainer/Zf1 Param Norm                 86.3474\n",
      "trainer/Zf2 Grad Norm                2226.7\n",
      "trainer/Zf2 Param Norm                 85.6958\n",
      "trainer/Z Expert Predictions Mean     494.907\n",
      "trainer/Z Expert Predictions Std       47.555\n",
      "trainer/Z Expert Predictions Max      591.345\n",
      "trainer/Z Expert Predictions Min      356.97\n",
      "trainer/Z Policy Predictions Mean      90.7349\n",
      "trainer/Z Policy Predictions Std       87.0122\n",
      "trainer/Z Policy Predictions Max      349.804\n",
      "trainer/Z Policy Predictions Min        0.607163\n",
      "trainer/Z Expert Targets Mean         474.821\n",
      "trainer/Z Expert Targets Std           47.1141\n",
      "trainer/Z Expert Targets Max          575.744\n",
      "trainer/Z Expert Targets Min          333.19\n",
      "trainer/Z Policy Targets Mean          87.307\n",
      "trainer/Z Policy Targets Std           87.2077\n",
      "trainer/Z Policy Targets Max          366.875\n",
      "trainer/Z Policy Targets Min           -1.52857\n",
      "trainer/Log Pis Mean                    9.03326\n",
      "trainer/Log Pis Std                     9.83795\n",
      "trainer/Policy mu Mean                  0.056258\n",
      "trainer/Policy mu Std                   1.06059\n",
      "trainer/Policy log std Mean            -0.955064\n",
      "trainer/Policy log std Std              0.754231\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        107863\n",
      "exploration/num paths total          1521\n",
      "evaluation/num steps total          20069\n",
      "evaluation/num paths total            202\n",
      "evaluation/path length Mean           151.7\n",
      "evaluation/path length Std             46.9575\n",
      "evaluation/path length Max            244\n",
      "evaluation/path length Min             98\n",
      "evaluation/Rewards Mean                 5.2544\n",
      "evaluation/Rewards Std                  0.278557\n",
      "evaluation/Rewards Max                  6.39922\n",
      "evaluation/Rewards Min                  4.79541\n",
      "evaluation/Returns Mean               797.093\n",
      "evaluation/Returns Std                242.793\n",
      "evaluation/Returns Max               1276.93\n",
      "evaluation/Returns Min                524.525\n",
      "evaluation/Estimation Bias Mean       218.587\n",
      "evaluation/Estimation Bias Std        191.369\n",
      "evaluation/EB/Q_True Mean              37.0173\n",
      "evaluation/EB/Q_True Std               94.0594\n",
      "evaluation/EB/Q_Pred Mean             255.605\n",
      "evaluation/EB/Q_Pred Std              163.622\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns            797.093\n",
      "evaluation/Actions Mean                 0.12507\n",
      "evaluation/Actions Std                  0.539716\n",
      "evaluation/Actions Max                  0.994958\n",
      "evaluation/Actions Min                 -0.996623\n",
      "time/backward_policy (s)                7.57656\n",
      "time/backward_zf1 (s)                   8.86276\n",
      "time/backward_zf2 (s)                   8.51444\n",
      "time/data sampling (s)                  1.5922\n",
      "time/data storing (s)                   0.0854987\n",
      "time/evaluation sampling (s)            1.0642\n",
      "time/exploration sampling (s)           2.57051\n",
      "time/logging (s)                        0.00590588\n",
      "time/preback_alpha (s)                  0.00511475\n",
      "time/preback_policy (s)                16.5577\n",
      "time/preback_start (s)                  0.890834\n",
      "time/preback_zf (s)                    36.2025\n",
      "time/saving (s)                         6.069e-06\n",
      "time/training (s)                      11.7123\n",
      "time/epoch (s)                         95.6405\n",
      "time/total (s)                       1923.62\n",
      "Epoch                                  19\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 22:51:34.892613 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 20 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 115000\n",
      "trainer/ZF1 Loss                       49.9516\n",
      "trainer/ZF2 Loss                       44.4286\n",
      "trainer/ZF Expert Reward               21.0303\n",
      "trainer/ZF Policy Reward                3.01506\n",
      "trainer/ZF CHI2 Term                   63.4671\n",
      "trainer/Policy Loss                  -127.546\n",
      "trainer/expert_lambda Loss             12.4842\n",
      "trainer/expert_lambda Value            18.9539\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               70.0384\n",
      "trainer/Policy Param Norm              28.4914\n",
      "trainer/Zf1 Grad Norm                5828.03\n",
      "trainer/Zf1 Param Norm                 88.9804\n",
      "trainer/Zf2 Grad Norm                5163.7\n",
      "trainer/Zf2 Param Norm                 88.2368\n",
      "trainer/Z Expert Predictions Mean     489.173\n",
      "trainer/Z Expert Predictions Std       44.301\n",
      "trainer/Z Expert Predictions Max      582.987\n",
      "trainer/Z Expert Predictions Min      388.459\n",
      "trainer/Z Policy Predictions Mean     121.307\n",
      "trainer/Z Policy Predictions Std      137.188\n",
      "trainer/Z Policy Predictions Max      512.047\n",
      "trainer/Z Policy Predictions Min        0.278907\n",
      "trainer/Z Expert Targets Mean         468.143\n",
      "trainer/Z Expert Targets Std           44.6763\n",
      "trainer/Z Expert Targets Max          561.507\n",
      "trainer/Z Expert Targets Min          364.421\n",
      "trainer/Z Policy Targets Mean         118.292\n",
      "trainer/Z Policy Targets Std          136.696\n",
      "trainer/Z Policy Targets Max          471.538\n",
      "trainer/Z Policy Targets Min           -2.4778\n",
      "trainer/Log Pis Mean                   12.596\n",
      "trainer/Log Pis Std                    10.3736\n",
      "trainer/Policy mu Mean                  0.149876\n",
      "trainer/Policy mu Std                   1.15387\n",
      "trainer/Policy log std Mean            -1.05621\n",
      "trainer/Policy log std Std              0.851725\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        112563\n",
      "exploration/num paths total          1538\n",
      "evaluation/num steps total          24413\n",
      "evaluation/num paths total            212\n",
      "evaluation/path length Mean           434.4\n",
      "evaluation/path length Std            180.734\n",
      "evaluation/path length Max            717\n",
      "evaluation/path length Min            154\n",
      "evaluation/Rewards Mean                 4.9778\n",
      "evaluation/Rewards Std                  0.312376\n",
      "evaluation/Rewards Max                  6.59812\n",
      "evaluation/Rewards Min                  3.12229\n",
      "evaluation/Returns Mean              2162.36\n",
      "evaluation/Returns Std                934.969\n",
      "evaluation/Returns Max               3584.1\n",
      "evaluation/Returns Min                721.026\n",
      "evaluation/Estimation Bias Mean       120.757\n",
      "evaluation/Estimation Bias Std        217.995\n",
      "evaluation/EB/Q_True Mean              71.04\n",
      "evaluation/EB/Q_True Std              167.222\n",
      "evaluation/EB/Q_Pred Mean             191.797\n",
      "evaluation/EB/Q_Pred Std              140.101\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           2162.36\n",
      "evaluation/Actions Mean                 0.170166\n",
      "evaluation/Actions Std                  0.517439\n",
      "evaluation/Actions Max                  0.999899\n",
      "evaluation/Actions Min                 -0.999793\n",
      "time/backward_policy (s)                8.12589\n",
      "time/backward_zf1 (s)                   9.41765\n",
      "time/backward_zf2 (s)                   9.12589\n",
      "time/data sampling (s)                  1.64239\n",
      "time/data storing (s)                   0.0864961\n",
      "time/evaluation sampling (s)            3.16838\n",
      "time/exploration sampling (s)           2.52415\n",
      "time/logging (s)                        0.00955336\n",
      "time/preback_alpha (s)                  0.00513643\n",
      "time/preback_policy (s)                15.7592\n",
      "time/preback_start (s)                  0.907486\n",
      "time/preback_zf (s)                    36.247\n",
      "time/saving (s)                         4.815e-06\n",
      "time/training (s)                      11.2636\n",
      "time/epoch (s)                         98.2828\n",
      "time/total (s)                       2021.91\n",
      "Epoch                                  20\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 22:53:15.726718 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 21 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 120000\n",
      "trainer/ZF1 Loss                       33.0514\n",
      "trainer/ZF2 Loss                       39.3729\n",
      "trainer/ZF Expert Reward               22.4756\n",
      "trainer/ZF Policy Reward                4.82662\n",
      "trainer/ZF CHI2 Term                   54.4389\n",
      "trainer/Policy Loss                  -142.655\n",
      "trainer/expert_lambda Loss             15.8468\n",
      "trainer/expert_lambda Value            19.3723\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               73.2464\n",
      "trainer/Policy Param Norm              29.1083\n",
      "trainer/Zf1 Grad Norm                4835.87\n",
      "trainer/Zf1 Param Norm                 91.5798\n",
      "trainer/Zf2 Grad Norm                6498.3\n",
      "trainer/Zf2 Param Norm                 90.7773\n",
      "trainer/Z Expert Predictions Mean     441.532\n",
      "trainer/Z Expert Predictions Std       53.2344\n",
      "trainer/Z Expert Predictions Max      652.568\n",
      "trainer/Z Expert Predictions Min      339.322\n",
      "trainer/Z Policy Predictions Mean     135.737\n",
      "trainer/Z Policy Predictions Std      142.799\n",
      "trainer/Z Policy Predictions Max      498.736\n",
      "trainer/Z Policy Predictions Min       -1.6316\n",
      "trainer/Z Expert Targets Mean         419.056\n",
      "trainer/Z Expert Targets Std           52.883\n",
      "trainer/Z Expert Targets Max          638.163\n",
      "trainer/Z Expert Targets Min          307.526\n",
      "trainer/Z Policy Targets Mean         130.911\n",
      "trainer/Z Policy Targets Std          141.493\n",
      "trainer/Z Policy Targets Max          498.886\n",
      "trainer/Z Policy Targets Min           -3.59417\n",
      "trainer/Log Pis Mean                   13.8492\n",
      "trainer/Log Pis Std                     9.26917\n",
      "trainer/Policy mu Mean                  0.13082\n",
      "trainer/Policy mu Std                   1.17608\n",
      "trainer/Policy log std Mean            -1.12509\n",
      "trainer/Policy log std Std              0.805668\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        117511\n",
      "exploration/num paths total          1550\n",
      "evaluation/num steps total          33927\n",
      "evaluation/num paths total            223\n",
      "evaluation/path length Mean           864.909\n",
      "evaluation/path length Std            227.525\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            382\n",
      "evaluation/Rewards Mean                 5.15785\n",
      "evaluation/Rewards Std                  0.128499\n",
      "evaluation/Rewards Max                  5.44166\n",
      "evaluation/Rewards Min                  3.42298\n",
      "evaluation/Returns Mean              4461.07\n",
      "evaluation/Returns Std               1198.01\n",
      "evaluation/Returns Max               5184.94\n",
      "evaluation/Returns Min               1917.85\n",
      "evaluation/Estimation Bias Mean       307.936\n",
      "evaluation/Estimation Bias Std        163.605\n",
      "evaluation/EB/Q_True Mean              49.1231\n",
      "evaluation/EB/Q_True Std              147.174\n",
      "evaluation/EB/Q_Pred Mean             357.059\n",
      "evaluation/EB/Q_Pred Std               58.0066\n",
      "evaluation/Num Paths                   11\n",
      "evaluation/Average Returns           4461.07\n",
      "evaluation/Actions Mean                 0.112435\n",
      "evaluation/Actions Std                  0.500898\n",
      "evaluation/Actions Max                  0.999953\n",
      "evaluation/Actions Min                 -0.99906\n",
      "time/backward_policy (s)                8.76728\n",
      "time/backward_zf1 (s)                  10.0368\n",
      "time/backward_zf2 (s)                   9.84318\n",
      "time/data sampling (s)                  1.71386\n",
      "time/data storing (s)                   0.0869249\n",
      "time/evaluation sampling (s)            4.89274\n",
      "time/exploration sampling (s)           2.51457\n",
      "time/logging (s)                        0.0129244\n",
      "time/preback_alpha (s)                  0.00522324\n",
      "time/preback_policy (s)                14.7572\n",
      "time/preback_start (s)                  0.915211\n",
      "time/preback_zf (s)                    36.3699\n",
      "time/saving (s)                         3.033e-06\n",
      "time/training (s)                      10.717\n",
      "time/epoch (s)                        100.633\n",
      "time/total (s)                       2122.54\n",
      "Epoch                                  21\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 22:54:52.459151 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 22 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 125000\n",
      "trainer/ZF1 Loss                       15.13\n",
      "trainer/ZF2 Loss                       22.9578\n",
      "trainer/ZF Expert Reward               24.0803\n",
      "trainer/ZF Policy Reward                3.34194\n",
      "trainer/ZF CHI2 Term                   39.2293\n",
      "trainer/Policy Loss                  -127.711\n",
      "trainer/expert_lambda Loss             18.7832\n",
      "trainer/expert_lambda Value            19.8006\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               54.8616\n",
      "trainer/Policy Param Norm              29.697\n",
      "trainer/Zf1 Grad Norm                3294.84\n",
      "trainer/Zf1 Param Norm                 93.7118\n",
      "trainer/Zf2 Grad Norm                4228.82\n",
      "trainer/Zf2 Param Norm                 92.8609\n",
      "trainer/Z Expert Predictions Mean     403.114\n",
      "trainer/Z Expert Predictions Std       32.2725\n",
      "trainer/Z Expert Predictions Max      476.803\n",
      "trainer/Z Expert Predictions Min      333.278\n",
      "trainer/Z Policy Predictions Mean     124.457\n",
      "trainer/Z Policy Predictions Std      132.092\n",
      "trainer/Z Policy Predictions Max      403.513\n",
      "trainer/Z Policy Predictions Min       -6.66899\n",
      "trainer/Z Expert Targets Mean         379.034\n",
      "trainer/Z Expert Targets Std           32.7039\n",
      "trainer/Z Expert Targets Max          456.453\n",
      "trainer/Z Expert Targets Min          304.517\n",
      "trainer/Z Policy Targets Mean         121.115\n",
      "trainer/Z Policy Targets Std          131.55\n",
      "trainer/Z Policy Targets Max          387.706\n",
      "trainer/Z Policy Targets Min          -12.0142\n",
      "trainer/Log Pis Mean                   11.8251\n",
      "trainer/Log Pis Std                     9.52898\n",
      "trainer/Policy mu Mean                  0.120951\n",
      "trainer/Policy mu Std                   1.17095\n",
      "trainer/Policy log std Mean            -1.0468\n",
      "trainer/Policy log std Std              0.762003\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        123499\n",
      "exploration/num paths total          1564\n",
      "evaluation/num steps total          35424\n",
      "evaluation/num paths total            233\n",
      "evaluation/path length Mean           149.7\n",
      "evaluation/path length Std             38.9077\n",
      "evaluation/path length Max            230\n",
      "evaluation/path length Min            108\n",
      "evaluation/Rewards Mean                 5.12315\n",
      "evaluation/Rewards Std                  0.240712\n",
      "evaluation/Rewards Max                  6.14193\n",
      "evaluation/Rewards Min                  3.87701\n",
      "evaluation/Returns Mean               766.936\n",
      "evaluation/Returns Std                197.463\n",
      "evaluation/Returns Max               1160.97\n",
      "evaluation/Returns Min                547.206\n",
      "evaluation/Estimation Bias Mean       141.041\n",
      "evaluation/Estimation Bias Std        168.496\n",
      "evaluation/EB/Q_True Mean              47.3546\n",
      "evaluation/EB/Q_True Std              122.659\n",
      "evaluation/EB/Q_Pred Mean             188.396\n",
      "evaluation/EB/Q_Pred Std              120.633\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns            766.936\n",
      "evaluation/Actions Mean                 0.113364\n",
      "evaluation/Actions Std                  0.531128\n",
      "evaluation/Actions Max                  0.999961\n",
      "evaluation/Actions Min                 -0.998937\n",
      "time/backward_policy (s)                8.67155\n",
      "time/backward_zf1 (s)                   9.937\n",
      "time/backward_zf2 (s)                   9.71732\n",
      "time/data sampling (s)                  1.71481\n",
      "time/data storing (s)                   0.08673\n",
      "time/evaluation sampling (s)            0.973483\n",
      "time/exploration sampling (s)           2.51588\n",
      "time/logging (s)                        0.0045208\n",
      "time/preback_alpha (s)                  0.0051581\n",
      "time/preback_policy (s)                14.9257\n",
      "time/preback_start (s)                  0.920367\n",
      "time/preback_zf (s)                    36.2357\n",
      "time/saving (s)                         4.097e-06\n",
      "time/training (s)                      10.8118\n",
      "time/epoch (s)                         96.5201\n",
      "time/total (s)                       2219.06\n",
      "Epoch                                  22\n",
      "---------------------------------  --------------\n",
      "2024-11-18 22:56:31.430744 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 23 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 130000\n",
      "trainer/ZF1 Loss                       13.7861\n",
      "trainer/ZF2 Loss                       16.0737\n",
      "trainer/ZF Expert Reward               21.2771\n",
      "trainer/ZF Policy Reward                2.36067\n",
      "trainer/ZF CHI2 Term                   31.8895\n",
      "trainer/Policy Loss                  -127.091\n",
      "trainer/expert_lambda Loss              8.3147\n",
      "trainer/expert_lambda Value            20.2284\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               73.2076\n",
      "trainer/Policy Param Norm              30.1788\n",
      "trainer/Zf1 Grad Norm                2346.38\n",
      "trainer/Zf1 Param Norm                 95.9724\n",
      "trainer/Zf2 Grad Norm                3885.9\n",
      "trainer/Zf2 Param Norm                 94.9771\n",
      "trainer/Z Expert Predictions Mean     444.49\n",
      "trainer/Z Expert Predictions Std       30.1467\n",
      "trainer/Z Expert Predictions Max      504.65\n",
      "trainer/Z Expert Predictions Min      387.327\n",
      "trainer/Z Policy Predictions Mean     122.887\n",
      "trainer/Z Policy Predictions Std      125.085\n",
      "trainer/Z Policy Predictions Max      385.842\n",
      "trainer/Z Policy Predictions Min      -16.8013\n",
      "trainer/Z Expert Targets Mean         423.213\n",
      "trainer/Z Expert Targets Std           30.2753\n",
      "trainer/Z Expert Targets Max          491.687\n",
      "trainer/Z Expert Targets Min          362.312\n",
      "trainer/Z Policy Targets Mean         120.527\n",
      "trainer/Z Policy Targets Std          125.992\n",
      "trainer/Z Policy Targets Max          376.46\n",
      "trainer/Z Policy Targets Min          -21.2651\n",
      "trainer/Log Pis Mean                   12.9284\n",
      "trainer/Log Pis Std                    10.1163\n",
      "trainer/Policy mu Mean                  0.0949085\n",
      "trainer/Policy mu Std                   1.15048\n",
      "trainer/Policy log std Mean            -1.1209\n",
      "trainer/Policy log std Std              0.854822\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        125430\n",
      "exploration/num paths total          1574\n",
      "evaluation/num steps total          40417\n",
      "evaluation/num paths total            243\n",
      "evaluation/path length Mean           499.3\n",
      "evaluation/path length Std            207.695\n",
      "evaluation/path length Max            843\n",
      "evaluation/path length Min            215\n",
      "evaluation/Rewards Mean                 5.26058\n",
      "evaluation/Rewards Std                  0.184945\n",
      "evaluation/Rewards Max                  6.3451\n",
      "evaluation/Rewards Min                  3.30352\n",
      "evaluation/Returns Mean              2626.61\n",
      "evaluation/Returns Std               1094.24\n",
      "evaluation/Returns Max               4434.1\n",
      "evaluation/Returns Min               1138.63\n",
      "evaluation/Estimation Bias Mean       261.238\n",
      "evaluation/Estimation Bias Std        233.935\n",
      "evaluation/EB/Q_True Mean              69.9885\n",
      "evaluation/EB/Q_True Std              165.273\n",
      "evaluation/EB/Q_Pred Mean             331.227\n",
      "evaluation/EB/Q_Pred Std              133.661\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           2626.61\n",
      "evaluation/Actions Mean                 0.102221\n",
      "evaluation/Actions Std                  0.488613\n",
      "evaluation/Actions Max                  0.999822\n",
      "evaluation/Actions Min                 -0.994406\n",
      "time/backward_policy (s)                7.98901\n",
      "time/backward_zf1 (s)                   9.28146\n",
      "time/backward_zf2 (s)                   8.95859\n",
      "time/data sampling (s)                  1.60645\n",
      "time/data storing (s)                   0.0857825\n",
      "time/evaluation sampling (s)            3.83813\n",
      "time/exploration sampling (s)           2.4834\n",
      "time/logging (s)                        0.0103705\n",
      "time/preback_alpha (s)                  0.00512169\n",
      "time/preback_policy (s)                15.9598\n",
      "time/preback_start (s)                  0.904134\n",
      "time/preback_zf (s)                    36.2452\n",
      "time/saving (s)                         6.458e-06\n",
      "time/training (s)                      11.4069\n",
      "time/epoch (s)                         98.7744\n",
      "time/total (s)                       2317.84\n",
      "Epoch                                  23\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 22:58:11.090839 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 24 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 135000\n",
      "trainer/ZF1 Loss                       20.3922\n",
      "trainer/ZF2 Loss                       11.187\n",
      "trainer/ZF Expert Reward               23.0821\n",
      "trainer/ZF Policy Reward                3.88663\n",
      "trainer/ZF CHI2 Term                   34.1311\n",
      "trainer/Policy Loss                  -152.197\n",
      "trainer/expert_lambda Loss              9.67561\n",
      "trainer/expert_lambda Value            20.6602\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               66.1524\n",
      "trainer/Policy Param Norm              30.6803\n",
      "trainer/Zf1 Grad Norm                3431.3\n",
      "trainer/Zf1 Param Norm                 97.9639\n",
      "trainer/Zf2 Grad Norm                2156.78\n",
      "trainer/Zf2 Param Norm                 96.886\n",
      "trainer/Z Expert Predictions Mean     488.609\n",
      "trainer/Z Expert Predictions Std       39.9779\n",
      "trainer/Z Expert Predictions Max      596.359\n",
      "trainer/Z Expert Predictions Min      412.433\n",
      "trainer/Z Policy Predictions Mean     145.94\n",
      "trainer/Z Policy Predictions Std      145.372\n",
      "trainer/Z Policy Predictions Max      492.86\n",
      "trainer/Z Policy Predictions Min      -22.1922\n",
      "trainer/Z Expert Targets Mean         465.527\n",
      "trainer/Z Expert Targets Std           39.453\n",
      "trainer/Z Expert Targets Max          574.35\n",
      "trainer/Z Expert Targets Min          387.156\n",
      "trainer/Z Policy Targets Mean         142.053\n",
      "trainer/Z Policy Targets Std          144.861\n",
      "trainer/Z Policy Targets Max          456.415\n",
      "trainer/Z Policy Targets Min          -24.435\n",
      "trainer/Log Pis Mean                   13.2713\n",
      "trainer/Log Pis Std                    10.7332\n",
      "trainer/Policy mu Mean                  0.139687\n",
      "trainer/Policy mu Std                   1.12349\n",
      "trainer/Policy log std Mean            -1.18251\n",
      "trainer/Policy log std Std              0.873889\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        133395\n",
      "exploration/num paths total          1586\n",
      "evaluation/num steps total          44019\n",
      "evaluation/num paths total            253\n",
      "evaluation/path length Mean           360.2\n",
      "evaluation/path length Std            118.575\n",
      "evaluation/path length Max            606\n",
      "evaluation/path length Min            172\n",
      "evaluation/Rewards Mean                 5.1817\n",
      "evaluation/Rewards Std                  0.178286\n",
      "evaluation/Rewards Max                  5.70072\n",
      "evaluation/Rewards Min                  3.51033\n",
      "evaluation/Returns Mean              1866.45\n",
      "evaluation/Returns Std                634.502\n",
      "evaluation/Returns Max               3172.75\n",
      "evaluation/Returns Min                813.046\n",
      "evaluation/Estimation Bias Mean       221.015\n",
      "evaluation/Estimation Bias Std        220.359\n",
      "evaluation/EB/Q_True Mean              61.1888\n",
      "evaluation/EB/Q_True Std              147.68\n",
      "evaluation/EB/Q_Pred Mean             282.204\n",
      "evaluation/EB/Q_Pred Std              175.206\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           1866.45\n",
      "evaluation/Actions Mean                 0.0904673\n",
      "evaluation/Actions Std                  0.486751\n",
      "evaluation/Actions Max                  0.999932\n",
      "evaluation/Actions Min                 -0.998184\n",
      "time/backward_policy (s)                8.96831\n",
      "time/backward_zf1 (s)                  10.2616\n",
      "time/backward_zf2 (s)                  10.0678\n",
      "time/data sampling (s)                  1.74654\n",
      "time/data storing (s)                   0.0872856\n",
      "time/evaluation sampling (s)            3.17681\n",
      "time/exploration sampling (s)           2.44619\n",
      "time/logging (s)                        0.0122136\n",
      "time/preback_alpha (s)                  0.00524303\n",
      "time/preback_policy (s)                14.6114\n",
      "time/preback_start (s)                  0.926885\n",
      "time/preback_zf (s)                    36.4156\n",
      "time/saving (s)                         6.422e-06\n",
      "time/training (s)                      10.7308\n",
      "time/epoch (s)                         99.4567\n",
      "time/total (s)                       2417.3\n",
      "Epoch                                  24\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 22:59:51.053825 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 25 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 140000\n",
      "trainer/ZF1 Loss                       24.1861\n",
      "trainer/ZF2 Loss                       31.5067\n",
      "trainer/ZF Expert Reward               22.5936\n",
      "trainer/ZF Policy Reward                3.95796\n",
      "trainer/ZF CHI2 Term                   45.1154\n",
      "trainer/Policy Loss                  -180.106\n",
      "trainer/expert_lambda Loss              9.91968\n",
      "trainer/expert_lambda Value            21.0867\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               80.7458\n",
      "trainer/Policy Param Norm              31.2265\n",
      "trainer/Zf1 Grad Norm                3809.95\n",
      "trainer/Zf1 Param Norm                 99.7768\n",
      "trainer/Zf2 Grad Norm                3788.63\n",
      "trainer/Zf2 Param Norm                 98.649\n",
      "trainer/Z Expert Predictions Mean     543.417\n",
      "trainer/Z Expert Predictions Std       52.483\n",
      "trainer/Z Expert Predictions Max      644.345\n",
      "trainer/Z Expert Predictions Min      453.284\n",
      "trainer/Z Policy Predictions Mean     175.161\n",
      "trainer/Z Policy Predictions Std      163.284\n",
      "trainer/Z Policy Predictions Max      475.937\n",
      "trainer/Z Policy Predictions Min      -25.0347\n",
      "trainer/Z Expert Targets Mean         520.823\n",
      "trainer/Z Expert Targets Std           52.6129\n",
      "trainer/Z Expert Targets Max          625.404\n",
      "trainer/Z Expert Targets Min          426.659\n",
      "trainer/Z Policy Targets Mean         171.203\n",
      "trainer/Z Policy Targets Std          164.497\n",
      "trainer/Z Policy Targets Max          481.135\n",
      "trainer/Z Policy Targets Min          -27.2727\n",
      "trainer/Log Pis Mean                   15.1548\n",
      "trainer/Log Pis Std                    10.8075\n",
      "trainer/Policy mu Mean                  0.119498\n",
      "trainer/Policy mu Std                   1.14097\n",
      "trainer/Policy log std Mean            -1.25244\n",
      "trainer/Policy log std Std              0.880173\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        135043\n",
      "exploration/num paths total          1592\n",
      "evaluation/num steps total          54019\n",
      "evaluation/num paths total            263\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.29829\n",
      "evaluation/Rewards Std                  0.0726433\n",
      "evaluation/Rewards Max                  5.44024\n",
      "evaluation/Rewards Min                  4.83923\n",
      "evaluation/Returns Mean              5298.29\n",
      "evaluation/Returns Std                  3.25518\n",
      "evaluation/Returns Max               5303.73\n",
      "evaluation/Returns Min               5292.51\n",
      "evaluation/Estimation Bias Mean       493.422\n",
      "evaluation/Estimation Bias Std        153.913\n",
      "evaluation/EB/Q_True Mean              47.7974\n",
      "evaluation/EB/Q_True Std              147.199\n",
      "evaluation/EB/Q_Pred Mean             541.219\n",
      "evaluation/EB/Q_Pred Std               50.1763\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5298.29\n",
      "evaluation/Actions Mean                 0.0855792\n",
      "evaluation/Actions Std                  0.512488\n",
      "evaluation/Actions Max                  0.99922\n",
      "evaluation/Actions Min                 -0.998431\n",
      "time/backward_policy (s)                8.2945\n",
      "time/backward_zf1 (s)                   9.57312\n",
      "time/backward_zf2 (s)                   9.29011\n",
      "time/data sampling (s)                  1.64131\n",
      "time/data storing (s)                   0.0854238\n",
      "time/evaluation sampling (s)            4.71899\n",
      "time/exploration sampling (s)           2.42856\n",
      "time/logging (s)                        0.0117665\n",
      "time/preback_alpha (s)                  0.00515247\n",
      "time/preback_policy (s)                15.4673\n",
      "time/preback_start (s)                  0.910775\n",
      "time/preback_zf (s)                    36.221\n",
      "time/saving (s)                         3.097e-06\n",
      "time/training (s)                      11.107\n",
      "time/epoch (s)                         99.755\n",
      "time/total (s)                       2517.06\n",
      "Epoch                                  25\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:01:28.765531 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 26 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 145000\n",
      "trainer/ZF1 Loss                       26.5017\n",
      "trainer/ZF2 Loss                       26.1725\n",
      "trainer/ZF Expert Reward               23.8272\n",
      "trainer/ZF Policy Reward                2.61439\n",
      "trainer/ZF CHI2 Term                   44.2686\n",
      "trainer/Policy Loss                  -220.42\n",
      "trainer/expert_lambda Loss             10.4805\n",
      "trainer/expert_lambda Value            21.5042\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              104.171\n",
      "trainer/Policy Param Norm              31.8058\n",
      "trainer/Zf1 Grad Norm                5395.89\n",
      "trainer/Zf1 Param Norm                101.909\n",
      "trainer/Zf2 Grad Norm                6053.09\n",
      "trainer/Zf2 Param Norm                100.601\n",
      "trainer/Z Expert Predictions Mean     602.559\n",
      "trainer/Z Expert Predictions Std       76.1275\n",
      "trainer/Z Expert Predictions Max      731.489\n",
      "trainer/Z Expert Predictions Min      474.275\n",
      "trainer/Z Policy Predictions Mean     214.355\n",
      "trainer/Z Policy Predictions Std      183.248\n",
      "trainer/Z Policy Predictions Max      599.24\n",
      "trainer/Z Policy Predictions Min      -24.3915\n",
      "trainer/Z Expert Targets Mean         578.731\n",
      "trainer/Z Expert Targets Std           75.9244\n",
      "trainer/Z Expert Targets Max          708.013\n",
      "trainer/Z Expert Targets Min          454.044\n",
      "trainer/Z Policy Targets Mean         211.741\n",
      "trainer/Z Policy Targets Std          183.534\n",
      "trainer/Z Policy Targets Max          583.805\n",
      "trainer/Z Policy Targets Min          -30.0059\n",
      "trainer/Log Pis Mean                   15.8068\n",
      "trainer/Log Pis Std                     9.77829\n",
      "trainer/Policy mu Mean                  0.170752\n",
      "trainer/Policy mu Std                   1.18855\n",
      "trainer/Policy log std Mean            -1.2692\n",
      "trainer/Policy log std Std              0.824333\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        143400\n",
      "exploration/num paths total          1604\n",
      "evaluation/num steps total          56692\n",
      "evaluation/num paths total            273\n",
      "evaluation/path length Mean           267.3\n",
      "evaluation/path length Std             56.7116\n",
      "evaluation/path length Max            374\n",
      "evaluation/path length Min            183\n",
      "evaluation/Rewards Mean                 5.30774\n",
      "evaluation/Rewards Std                  0.189132\n",
      "evaluation/Rewards Max                  6.38211\n",
      "evaluation/Rewards Min                  4.84639\n",
      "evaluation/Returns Mean              1418.76\n",
      "evaluation/Returns Std                304.036\n",
      "evaluation/Returns Max               1997.9\n",
      "evaluation/Returns Min                970.047\n",
      "evaluation/Estimation Bias Mean       360.991\n",
      "evaluation/Estimation Bias Std        218.802\n",
      "evaluation/EB/Q_True Mean              42.1633\n",
      "evaluation/EB/Q_True Std              114.5\n",
      "evaluation/EB/Q_Pred Mean             403.154\n",
      "evaluation/EB/Q_Pred Std              201.752\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           1418.76\n",
      "evaluation/Actions Mean                 0.0619779\n",
      "evaluation/Actions Std                  0.534225\n",
      "evaluation/Actions Max                  0.99955\n",
      "evaluation/Actions Min                 -0.999501\n",
      "time/backward_policy (s)                8.30105\n",
      "time/backward_zf1 (s)                   9.57914\n",
      "time/backward_zf2 (s)                   9.31223\n",
      "time/data sampling (s)                  1.74189\n",
      "time/data storing (s)                   0.0869226\n",
      "time/evaluation sampling (s)            2.1811\n",
      "time/exploration sampling (s)           2.55088\n",
      "time/logging (s)                        0.00733351\n",
      "time/preback_alpha (s)                  0.00515475\n",
      "time/preback_policy (s)                15.4382\n",
      "time/preback_start (s)                  0.914288\n",
      "time/preback_zf (s)                    36.2881\n",
      "time/saving (s)                         6.018e-06\n",
      "time/training (s)                      11.0967\n",
      "time/epoch (s)                         97.5029\n",
      "time/total (s)                       2614.57\n",
      "Epoch                                  26\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:03:08.913683 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 27 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 150000\n",
      "trainer/ZF1 Loss                       37.315\n",
      "trainer/ZF2 Loss                       37.2537\n",
      "trainer/ZF Expert Reward               24.888\n",
      "trainer/ZF Policy Reward                5.12991\n",
      "trainer/ZF CHI2 Term                   55.9864\n",
      "trainer/Policy Loss                  -236.554\n",
      "trainer/expert_lambda Loss             11.3472\n",
      "trainer/expert_lambda Value            21.9121\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              106.898\n",
      "trainer/Policy Param Norm              32.3311\n",
      "trainer/Zf1 Grad Norm                4987.26\n",
      "trainer/Zf1 Param Norm                103.761\n",
      "trainer/Zf2 Grad Norm                5410.21\n",
      "trainer/Zf2 Param Norm                102.433\n",
      "trainer/Z Expert Predictions Mean     630.32\n",
      "trainer/Z Expert Predictions Std       46.0963\n",
      "trainer/Z Expert Predictions Max      754.628\n",
      "trainer/Z Expert Predictions Min      541.628\n",
      "trainer/Z Policy Predictions Mean     230.032\n",
      "trainer/Z Policy Predictions Std      195.864\n",
      "trainer/Z Policy Predictions Max      595.034\n",
      "trainer/Z Policy Predictions Min      -22.1353\n",
      "trainer/Z Expert Targets Mean         605.432\n",
      "trainer/Z Expert Targets Std           46.4395\n",
      "trainer/Z Expert Targets Max          726.005\n",
      "trainer/Z Expert Targets Min          512.939\n",
      "trainer/Z Policy Targets Mean         224.902\n",
      "trainer/Z Policy Targets Std          194.797\n",
      "trainer/Z Policy Targets Max          572.752\n",
      "trainer/Z Policy Targets Min          -20.1709\n",
      "trainer/Log Pis Mean                   17.233\n",
      "trainer/Log Pis Std                    10.0354\n",
      "trainer/Policy mu Mean                  0.165652\n",
      "trainer/Policy mu Std                   1.24009\n",
      "trainer/Policy log std Mean            -1.31765\n",
      "trainer/Policy log std Std              0.848126\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        145100\n",
      "exploration/num paths total          1609\n",
      "evaluation/num steps total          66692\n",
      "evaluation/num paths total            283\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.31697\n",
      "evaluation/Rewards Std                  0.0846999\n",
      "evaluation/Rewards Max                  5.4657\n",
      "evaluation/Rewards Min                  4.77732\n",
      "evaluation/Returns Mean              5316.97\n",
      "evaluation/Returns Std                  6.7271\n",
      "evaluation/Returns Max               5325.74\n",
      "evaluation/Returns Min               5303.21\n",
      "evaluation/Estimation Bias Mean       481.032\n",
      "evaluation/Estimation Bias Std        150.5\n",
      "evaluation/EB/Q_True Mean              48.0764\n",
      "evaluation/EB/Q_True Std              148.075\n",
      "evaluation/EB/Q_Pred Mean             529.109\n",
      "evaluation/EB/Q_Pred Std               36.2166\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5316.97\n",
      "evaluation/Actions Mean                 0.0701478\n",
      "evaluation/Actions Std                  0.507554\n",
      "evaluation/Actions Max                  0.999151\n",
      "evaluation/Actions Min                 -0.999648\n",
      "time/backward_policy (s)                8.67781\n",
      "time/backward_zf1 (s)                   9.93919\n",
      "time/backward_zf2 (s)                   9.72375\n",
      "time/data sampling (s)                  1.64716\n",
      "time/data storing (s)                   0.0859948\n",
      "time/evaluation sampling (s)            4.33926\n",
      "time/exploration sampling (s)           2.50348\n",
      "time/logging (s)                        0.0124075\n",
      "time/preback_alpha (s)                  0.0052117\n",
      "time/preback_policy (s)                14.911\n",
      "time/preback_start (s)                  0.91148\n",
      "time/preback_zf (s)                    36.3945\n",
      "time/saving (s)                         2.751e-06\n",
      "time/training (s)                      10.796\n",
      "time/epoch (s)                         99.9473\n",
      "time/total (s)                       2714.52\n",
      "Epoch                                  27\n",
      "---------------------------------  --------------\n",
      "2024-11-18 23:04:48.981320 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 28 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 155000\n",
      "trainer/ZF1 Loss                       40.2258\n",
      "trainer/ZF2 Loss                       45.5632\n",
      "trainer/ZF Expert Reward               23.0762\n",
      "trainer/ZF Policy Reward                1.56851\n",
      "trainer/ZF CHI2 Term                   59.3986\n",
      "trainer/Policy Loss                  -279.828\n",
      "trainer/expert_lambda Loss              8.05514\n",
      "trainer/expert_lambda Value            22.3146\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              107.774\n",
      "trainer/Policy Param Norm              32.7481\n",
      "trainer/Zf1 Grad Norm                6278.24\n",
      "trainer/Zf1 Param Norm                105.608\n",
      "trainer/Zf2 Grad Norm                5971.3\n",
      "trainer/Zf2 Param Norm                104.42\n",
      "trainer/Z Expert Predictions Mean     672.602\n",
      "trainer/Z Expert Predictions Std       56.2303\n",
      "trainer/Z Expert Predictions Max      792.384\n",
      "trainer/Z Expert Predictions Min      554.551\n",
      "trainer/Z Policy Predictions Mean     273.153\n",
      "trainer/Z Policy Predictions Std      207.733\n",
      "trainer/Z Policy Predictions Max      670.011\n",
      "trainer/Z Policy Predictions Min      -12.9\n",
      "trainer/Z Expert Targets Mean         649.526\n",
      "trainer/Z Expert Targets Std           56.3004\n",
      "trainer/Z Expert Targets Max          771.655\n",
      "trainer/Z Expert Targets Min          530.227\n",
      "trainer/Z Policy Targets Mean         271.584\n",
      "trainer/Z Policy Targets Std          208.795\n",
      "trainer/Z Policy Targets Max          659.031\n",
      "trainer/Z Policy Targets Min          -18.7569\n",
      "trainer/Log Pis Mean                   17.3295\n",
      "trainer/Log Pis Std                    10.3898\n",
      "trainer/Policy mu Mean                  0.0928189\n",
      "trainer/Policy mu Std                   1.17963\n",
      "trainer/Policy log std Mean            -1.40692\n",
      "trainer/Policy log std Std              0.873175\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        153231\n",
      "exploration/num paths total          1618\n",
      "evaluation/num steps total          76692\n",
      "evaluation/num paths total            293\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.3426\n",
      "evaluation/Rewards Std                  0.0958259\n",
      "evaluation/Rewards Max                  5.6431\n",
      "evaluation/Rewards Min                  4.79516\n",
      "evaluation/Returns Mean              5342.6\n",
      "evaluation/Returns Std                  7.62346\n",
      "evaluation/Returns Max               5359.23\n",
      "evaluation/Returns Min               5329.16\n",
      "evaluation/Estimation Bias Mean       410.986\n",
      "evaluation/Estimation Bias Std        165.458\n",
      "evaluation/EB/Q_True Mean              48.2789\n",
      "evaluation/EB/Q_True Std              148.663\n",
      "evaluation/EB/Q_Pred Mean             459.265\n",
      "evaluation/EB/Q_Pred Std               79.0732\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5342.6\n",
      "evaluation/Actions Mean                 0.0991505\n",
      "evaluation/Actions Std                  0.566148\n",
      "evaluation/Actions Max                  0.99979\n",
      "evaluation/Actions Min                 -0.999908\n",
      "time/backward_policy (s)                8.3057\n",
      "time/backward_zf1 (s)                   9.58455\n",
      "time/backward_zf2 (s)                   9.29979\n",
      "time/data sampling (s)                  1.6936\n",
      "time/data storing (s)                   0.0863526\n",
      "time/evaluation sampling (s)            4.63062\n",
      "time/exploration sampling (s)           2.50765\n",
      "time/logging (s)                        0.0116767\n",
      "time/preback_alpha (s)                  0.00514711\n",
      "time/preback_policy (s)                15.4814\n",
      "time/preback_start (s)                  0.906651\n",
      "time/preback_zf (s)                    36.2406\n",
      "time/saving (s)                         2.888e-06\n",
      "time/training (s)                      11.1096\n",
      "time/epoch (s)                         99.8633\n",
      "time/total (s)                       2814.38\n",
      "Epoch                                  28\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:06:29.174508 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 29 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 160000\n",
      "trainer/ZF1 Loss                       40.1038\n",
      "trainer/ZF2 Loss                       22.8117\n",
      "trainer/ZF Expert Reward               24.7415\n",
      "trainer/ZF Policy Reward                3.53665\n",
      "trainer/ZF CHI2 Term                   49.804\n",
      "trainer/Policy Loss                  -279.709\n",
      "trainer/expert_lambda Loss              9.33011\n",
      "trainer/expert_lambda Value            22.7294\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              109.479\n",
      "trainer/Policy Param Norm              33.2024\n",
      "trainer/Zf1 Grad Norm                8031.3\n",
      "trainer/Zf1 Param Norm                107.359\n",
      "trainer/Zf2 Grad Norm                4681.57\n",
      "trainer/Zf2 Param Norm                106.152\n",
      "trainer/Z Expert Predictions Mean     653.659\n",
      "trainer/Z Expert Predictions Std       49.3562\n",
      "trainer/Z Expert Predictions Max      774.232\n",
      "trainer/Z Expert Predictions Min      550.649\n",
      "trainer/Z Policy Predictions Mean     274.216\n",
      "trainer/Z Policy Predictions Std      219.729\n",
      "trainer/Z Policy Predictions Max      637.155\n",
      "trainer/Z Policy Predictions Min      -17.236\n",
      "trainer/Z Expert Targets Mean         628.918\n",
      "trainer/Z Expert Targets Std           49.8367\n",
      "trainer/Z Expert Targets Max          757.186\n",
      "trainer/Z Expert Targets Min          526.134\n",
      "trainer/Z Policy Targets Mean         270.68\n",
      "trainer/Z Policy Targets Std          219.827\n",
      "trainer/Z Policy Targets Max          617.684\n",
      "trainer/Z Policy Targets Min          -20.7833\n",
      "trainer/Log Pis Mean                   18.3688\n",
      "trainer/Log Pis Std                     9.11428\n",
      "trainer/Policy mu Mean                  0.198614\n",
      "trainer/Policy mu Std                   1.27928\n",
      "trainer/Policy log std Mean            -1.35209\n",
      "trainer/Policy log std Std              0.872028\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        154459\n",
      "exploration/num paths total          1620\n",
      "evaluation/num steps total          86692\n",
      "evaluation/num paths total            303\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.33685\n",
      "evaluation/Rewards Std                  0.0933582\n",
      "evaluation/Rewards Max                  5.83099\n",
      "evaluation/Rewards Min                  4.81957\n",
      "evaluation/Returns Mean              5336.85\n",
      "evaluation/Returns Std                  3.81052\n",
      "evaluation/Returns Max               5343.05\n",
      "evaluation/Returns Min               5331.23\n",
      "evaluation/Estimation Bias Mean       421.992\n",
      "evaluation/Estimation Bias Std        154.138\n",
      "evaluation/EB/Q_True Mean              48.2812\n",
      "evaluation/EB/Q_True Std              148.634\n",
      "evaluation/EB/Q_Pred Mean             470.273\n",
      "evaluation/EB/Q_Pred Std               47.0903\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5336.85\n",
      "evaluation/Actions Mean                 0.0747243\n",
      "evaluation/Actions Std                  0.545344\n",
      "evaluation/Actions Max                  0.999479\n",
      "evaluation/Actions Min                 -0.999929\n",
      "time/backward_policy (s)                8.51385\n",
      "time/backward_zf1 (s)                   9.77657\n",
      "time/backward_zf2 (s)                   9.54547\n",
      "time/data sampling (s)                  1.6779\n",
      "time/data storing (s)                   0.0874623\n",
      "time/evaluation sampling (s)            4.79769\n",
      "time/exploration sampling (s)           2.46558\n",
      "time/logging (s)                        0.0114886\n",
      "time/preback_alpha (s)                  0.00516213\n",
      "time/preback_policy (s)                15.0945\n",
      "time/preback_start (s)                  0.907538\n",
      "time/preback_zf (s)                    36.2509\n",
      "time/saving (s)                         3.06e-06\n",
      "time/training (s)                      10.8543\n",
      "time/epoch (s)                         99.9885\n",
      "time/total (s)                       2914.38\n",
      "Epoch                                  29\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:08:09.592452 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 30 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 165000\n",
      "trainer/ZF1 Loss                       39.2853\n",
      "trainer/ZF2 Loss                       27.4519\n",
      "trainer/ZF Expert Reward               26.7195\n",
      "trainer/ZF Policy Reward                2.48669\n",
      "trainer/ZF CHI2 Term                   53.6604\n",
      "trainer/Policy Loss                  -282.334\n",
      "trainer/expert_lambda Loss             15.6129\n",
      "trainer/expert_lambda Value            23.1233\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               94.5889\n",
      "trainer/Policy Param Norm              33.6568\n",
      "trainer/Zf1 Grad Norm                5888.69\n",
      "trainer/Zf1 Param Norm                108.973\n",
      "trainer/Zf2 Grad Norm                5105.23\n",
      "trainer/Zf2 Param Norm                108.011\n",
      "trainer/Z Expert Predictions Mean     658.44\n",
      "trainer/Z Expert Predictions Std       61.9355\n",
      "trainer/Z Expert Predictions Max      812.412\n",
      "trainer/Z Expert Predictions Min      505.59\n",
      "trainer/Z Policy Predictions Mean     276.316\n",
      "trainer/Z Policy Predictions Std      206.396\n",
      "trainer/Z Policy Predictions Max      599.668\n",
      "trainer/Z Policy Predictions Min       -7.84605\n",
      "trainer/Z Expert Targets Mean         631.721\n",
      "trainer/Z Expert Targets Std           61.9826\n",
      "trainer/Z Expert Targets Max          787.238\n",
      "trainer/Z Expert Targets Min          476.721\n",
      "trainer/Z Policy Targets Mean         273.829\n",
      "trainer/Z Policy Targets Std          206.516\n",
      "trainer/Z Policy Targets Max          618.201\n",
      "trainer/Z Policy Targets Min           -3.07421\n",
      "trainer/Log Pis Mean                   18.5912\n",
      "trainer/Log Pis Std                    10.2331\n",
      "trainer/Policy mu Mean                  0.189157\n",
      "trainer/Policy mu Std                   1.29356\n",
      "trainer/Policy log std Mean            -1.35722\n",
      "trainer/Policy log std Std              0.85659\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        162459\n",
      "exploration/num paths total          1628\n",
      "evaluation/num steps total          94511\n",
      "evaluation/num paths total            318\n",
      "evaluation/path length Mean           521.267\n",
      "evaluation/path length Std            405.404\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            108\n",
      "evaluation/Rewards Mean                 5.29208\n",
      "evaluation/Rewards Std                  0.144838\n",
      "evaluation/Rewards Max                  6.27502\n",
      "evaluation/Rewards Min                  3.62767\n",
      "evaluation/Returns Mean              2758.58\n",
      "evaluation/Returns Std               2152.16\n",
      "evaluation/Returns Max               5307.75\n",
      "evaluation/Returns Min                572.462\n",
      "evaluation/Estimation Bias Mean       488.302\n",
      "evaluation/Estimation Bias Std        249.787\n",
      "evaluation/EB/Q_True Mean              61.3203\n",
      "evaluation/EB/Q_True Std              164.504\n",
      "evaluation/EB/Q_Pred Mean             549.623\n",
      "evaluation/EB/Q_Pred Std              134.072\n",
      "evaluation/Num Paths                   15\n",
      "evaluation/Average Returns           2758.58\n",
      "evaluation/Actions Mean                 0.0787694\n",
      "evaluation/Actions Std                  0.520161\n",
      "evaluation/Actions Max                  0.999947\n",
      "evaluation/Actions Min                 -0.999996\n",
      "time/backward_policy (s)                8.51963\n",
      "time/backward_zf1 (s)                   9.78936\n",
      "time/backward_zf2 (s)                   9.55346\n",
      "time/data sampling (s)                  1.66787\n",
      "time/data storing (s)                   0.0855327\n",
      "time/evaluation sampling (s)            4.9072\n",
      "time/exploration sampling (s)           2.54266\n",
      "time/logging (s)                        0.011902\n",
      "time/preback_alpha (s)                  0.00516793\n",
      "time/preback_policy (s)                15.0446\n",
      "time/preback_start (s)                  0.907176\n",
      "time/preback_zf (s)                    36.3426\n",
      "time/saving (s)                         4.842e-06\n",
      "time/training (s)                      10.8371\n",
      "time/epoch (s)                        100.214\n",
      "time/total (s)                       3014.59\n",
      "Epoch                                  30\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:09:50.556495 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 31 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 170000\n",
      "trainer/ZF1 Loss                       29.5044\n",
      "trainer/ZF2 Loss                       28.9675\n",
      "trainer/ZF Expert Reward               25.1788\n",
      "trainer/ZF Policy Reward                2.00713\n",
      "trainer/ZF CHI2 Term                   48.1522\n",
      "trainer/Policy Loss                  -285.158\n",
      "trainer/expert_lambda Loss             10.9914\n",
      "trainer/expert_lambda Value            23.5165\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               78.141\n",
      "trainer/Policy Param Norm              34.2103\n",
      "trainer/Zf1 Grad Norm                4710.58\n",
      "trainer/Zf1 Param Norm                110.61\n",
      "trainer/Zf2 Grad Norm                5030.88\n",
      "trainer/Zf2 Param Norm                109.741\n",
      "trainer/Z Expert Predictions Mean     641.368\n",
      "trainer/Z Expert Predictions Std       80.7202\n",
      "trainer/Z Expert Predictions Max      878.648\n",
      "trainer/Z Expert Predictions Min      456.815\n",
      "trainer/Z Policy Predictions Mean     278.333\n",
      "trainer/Z Policy Predictions Std      203.462\n",
      "trainer/Z Policy Predictions Max      700.812\n",
      "trainer/Z Policy Predictions Min      -19.0649\n",
      "trainer/Z Expert Targets Mean         616.189\n",
      "trainer/Z Expert Targets Std           81.0324\n",
      "trainer/Z Expert Targets Max          862.839\n",
      "trainer/Z Expert Targets Min          429.938\n",
      "trainer/Z Policy Targets Mean         276.326\n",
      "trainer/Z Policy Targets Std          203.67\n",
      "trainer/Z Policy Targets Max          669.821\n",
      "trainer/Z Policy Targets Min          -22.9972\n",
      "trainer/Log Pis Mean                   21.1783\n",
      "trainer/Log Pis Std                    10.1151\n",
      "trainer/Policy mu Mean                  0.2183\n",
      "trainer/Policy mu Std                   1.40997\n",
      "trainer/Policy log std Mean            -1.32008\n",
      "trainer/Policy log std Std              0.85939\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        164579\n",
      "exploration/num paths total          1631\n",
      "evaluation/num steps total         104511\n",
      "evaluation/num paths total            328\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.30879\n",
      "evaluation/Rewards Std                  0.0717513\n",
      "evaluation/Rewards Max                  5.49232\n",
      "evaluation/Rewards Min                  4.8525\n",
      "evaluation/Returns Mean              5308.79\n",
      "evaluation/Returns Std                  1.60971\n",
      "evaluation/Returns Max               5311.61\n",
      "evaluation/Returns Min               5306.78\n",
      "evaluation/Estimation Bias Mean       431.079\n",
      "evaluation/Estimation Bias Std        148.281\n",
      "evaluation/EB/Q_True Mean              47.948\n",
      "evaluation/EB/Q_True Std              147.667\n",
      "evaluation/EB/Q_Pred Mean             479.027\n",
      "evaluation/EB/Q_Pred Std               23.9235\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5308.79\n",
      "evaluation/Actions Mean                 0.0856247\n",
      "evaluation/Actions Std                  0.490769\n",
      "evaluation/Actions Max                  0.998834\n",
      "evaluation/Actions Min                 -0.998599\n",
      "time/backward_policy (s)                8.71584\n",
      "time/backward_zf1 (s)                  10.0314\n",
      "time/backward_zf2 (s)                   9.7958\n",
      "time/data sampling (s)                  1.78945\n",
      "time/data storing (s)                   0.0874556\n",
      "time/evaluation sampling (s)            4.58855\n",
      "time/exploration sampling (s)           2.5238\n",
      "time/logging (s)                        0.0125325\n",
      "time/preback_alpha (s)                  0.00524943\n",
      "time/preback_policy (s)                14.9264\n",
      "time/preback_start (s)                  0.921329\n",
      "time/preback_zf (s)                    36.4275\n",
      "time/saving (s)                         2.959e-06\n",
      "time/training (s)                      10.9333\n",
      "time/epoch (s)                        100.759\n",
      "time/total (s)                       3115.36\n",
      "Epoch                                  31\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:11:31.141215 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 32 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 175000\n",
      "trainer/ZF1 Loss                       24.6678\n",
      "trainer/ZF2 Loss                       17.6989\n",
      "trainer/ZF Expert Reward               26.4499\n",
      "trainer/ZF Policy Reward                4.60027\n",
      "trainer/ZF CHI2 Term                   42.2049\n",
      "trainer/Policy Loss                  -263.85\n",
      "trainer/expert_lambda Loss             12.9325\n",
      "trainer/expert_lambda Value            23.913\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               99.3021\n",
      "trainer/Policy Param Norm              34.7303\n",
      "trainer/Zf1 Grad Norm                5899.83\n",
      "trainer/Zf1 Param Norm                112.068\n",
      "trainer/Zf2 Grad Norm                2751.18\n",
      "trainer/Zf2 Param Norm                111.309\n",
      "trainer/Z Expert Predictions Mean     559.083\n",
      "trainer/Z Expert Predictions Std       78.226\n",
      "trainer/Z Expert Predictions Max      686.187\n",
      "trainer/Z Expert Predictions Min      385.391\n",
      "trainer/Z Policy Predictions Mean     258.076\n",
      "trainer/Z Policy Predictions Std      179.683\n",
      "trainer/Z Policy Predictions Max      532.973\n",
      "trainer/Z Policy Predictions Min      -25.8818\n",
      "trainer/Z Expert Targets Mean         532.633\n",
      "trainer/Z Expert Targets Std           77.6646\n",
      "trainer/Z Expert Targets Max          654.829\n",
      "trainer/Z Expert Targets Min          363.547\n",
      "trainer/Z Policy Targets Mean         253.476\n",
      "trainer/Z Policy Targets Std          180.318\n",
      "trainer/Z Policy Targets Max          527.273\n",
      "trainer/Z Policy Targets Min          -43.9322\n",
      "trainer/Log Pis Mean                   19.2722\n",
      "trainer/Log Pis Std                    10.6461\n",
      "trainer/Policy mu Mean                  0.180926\n",
      "trainer/Policy mu Std                   1.38834\n",
      "trainer/Policy log std Mean            -1.27046\n",
      "trainer/Policy log std Std              0.811838\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        171421\n",
      "exploration/num paths total          1638\n",
      "evaluation/num steps total         114511\n",
      "evaluation/num paths total            338\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.30673\n",
      "evaluation/Rewards Std                  0.079003\n",
      "evaluation/Rewards Max                  5.47579\n",
      "evaluation/Rewards Min                  4.84106\n",
      "evaluation/Returns Mean              5306.73\n",
      "evaluation/Returns Std                  5.59749\n",
      "evaluation/Returns Max               5314.24\n",
      "evaluation/Returns Min               5292.89\n",
      "evaluation/Estimation Bias Mean       345.622\n",
      "evaluation/Estimation Bias Std        148.726\n",
      "evaluation/EB/Q_True Mean              47.8977\n",
      "evaluation/EB/Q_True Std              147.534\n",
      "evaluation/EB/Q_Pred Mean             393.52\n",
      "evaluation/EB/Q_Pred Std               24.8065\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5306.73\n",
      "evaluation/Actions Mean                 0.0812506\n",
      "evaluation/Actions Std                  0.509259\n",
      "evaluation/Actions Max                  0.99828\n",
      "evaluation/Actions Min                 -0.999356\n",
      "time/backward_policy (s)                8.7673\n",
      "time/backward_zf1 (s)                  10.0268\n",
      "time/backward_zf2 (s)                   9.84568\n",
      "time/data sampling (s)                  1.7782\n",
      "time/data storing (s)                   0.0877286\n",
      "time/evaluation sampling (s)            4.43978\n",
      "time/exploration sampling (s)           2.5289\n",
      "time/logging (s)                        0.0144564\n",
      "time/preback_alpha (s)                  0.00522639\n",
      "time/preback_policy (s)                14.8236\n",
      "time/preback_start (s)                  0.921618\n",
      "time/preback_zf (s)                    36.3789\n",
      "time/saving (s)                         2.963e-06\n",
      "time/training (s)                      10.7625\n",
      "time/epoch (s)                        100.381\n",
      "time/total (s)                       3215.74\n",
      "Epoch                                  32\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:13:13.800525 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 33 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 180000\n",
      "trainer/ZF1 Loss                       16.2652\n",
      "trainer/ZF2 Loss                        5.96373\n",
      "trainer/ZF Expert Reward               28.3548\n",
      "trainer/ZF Policy Reward                2.69442\n",
      "trainer/ZF CHI2 Term                   34.7111\n",
      "trainer/Policy Loss                  -241.335\n",
      "trainer/expert_lambda Loss             18.9276\n",
      "trainer/expert_lambda Value            24.3127\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               68.2965\n",
      "trainer/Policy Param Norm              35.2369\n",
      "trainer/Zf1 Grad Norm                2976.2\n",
      "trainer/Zf1 Param Norm                113.575\n",
      "trainer/Zf2 Grad Norm                2772.64\n",
      "trainer/Zf2 Param Norm                112.875\n",
      "trainer/Z Expert Predictions Mean     491.956\n",
      "trainer/Z Expert Predictions Std       69.2077\n",
      "trainer/Z Expert Predictions Max      643.553\n",
      "trainer/Z Expert Predictions Min      329.028\n",
      "trainer/Z Policy Predictions Mean     236.553\n",
      "trainer/Z Policy Predictions Std      157.59\n",
      "trainer/Z Policy Predictions Max      454.975\n",
      "trainer/Z Policy Predictions Min      -56.4092\n",
      "trainer/Z Expert Targets Mean         463.601\n",
      "trainer/Z Expert Targets Std           68.8987\n",
      "trainer/Z Expert Targets Max          611.704\n",
      "trainer/Z Expert Targets Min          296.591\n",
      "trainer/Z Policy Targets Mean         233.858\n",
      "trainer/Z Policy Targets Std          157.721\n",
      "trainer/Z Policy Targets Max          456.365\n",
      "trainer/Z Policy Targets Min          -62.2107\n",
      "trainer/Log Pis Mean                   17.3727\n",
      "trainer/Log Pis Std                    10.2769\n",
      "trainer/Policy mu Mean                  0.118679\n",
      "trainer/Policy mu Std                   1.30128\n",
      "trainer/Policy log std Mean            -1.32826\n",
      "trainer/Policy log std Std              0.753989\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        174421\n",
      "exploration/num paths total          1641\n",
      "evaluation/num steps total         122333\n",
      "evaluation/num paths total            349\n",
      "evaluation/path length Mean           711.091\n",
      "evaluation/path length Std            263.299\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            296\n",
      "evaluation/Rewards Mean                 5.06714\n",
      "evaluation/Rewards Std                  0.217555\n",
      "evaluation/Rewards Max                  5.35375\n",
      "evaluation/Rewards Min                  2.85186\n",
      "evaluation/Returns Mean              3603.2\n",
      "evaluation/Returns Std               1368.64\n",
      "evaluation/Returns Max               5133.11\n",
      "evaluation/Returns Min               1460.43\n",
      "evaluation/Estimation Bias Mean       275.692\n",
      "evaluation/Estimation Bias Std        180.745\n",
      "evaluation/EB/Q_True Mean              58.9386\n",
      "evaluation/EB/Q_True Std              158.179\n",
      "evaluation/EB/Q_Pred Mean             334.63\n",
      "evaluation/EB/Q_Pred Std               74.2073\n",
      "evaluation/Num Paths                   11\n",
      "evaluation/Average Returns           3603.2\n",
      "evaluation/Actions Mean                 0.150504\n",
      "evaluation/Actions Std                  0.471368\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -1\n",
      "time/backward_policy (s)                8.44616\n",
      "time/backward_zf1 (s)                   9.91089\n",
      "time/backward_zf2 (s)                   9.5636\n",
      "time/data sampling (s)                  1.78974\n",
      "time/data storing (s)                   0.092082\n",
      "time/evaluation sampling (s)            4.95062\n",
      "time/exploration sampling (s)           2.63274\n",
      "time/logging (s)                        0.0146365\n",
      "time/preback_alpha (s)                  0.00531311\n",
      "time/preback_policy (s)                15.7389\n",
      "time/preback_start (s)                  0.951045\n",
      "time/preback_zf (s)                    36.9285\n",
      "time/saving (s)                         5.306e-06\n",
      "time/training (s)                      11.42\n",
      "time/epoch (s)                        102.444\n",
      "time/total (s)                       3318.19\n",
      "Epoch                                  33\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:14:54.726542 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 34 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 185000\n",
      "trainer/ZF1 Loss                        9.74703\n",
      "trainer/ZF2 Loss                        8.02447\n",
      "trainer/ZF Expert Reward               27.5925\n",
      "trainer/ZF Policy Reward                3.51287\n",
      "trainer/ZF CHI2 Term                   32.6512\n",
      "trainer/Policy Loss                  -209.408\n",
      "trainer/expert_lambda Loss             12.3886\n",
      "trainer/expert_lambda Value            24.723\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               56.4348\n",
      "trainer/Policy Param Norm              35.6192\n",
      "trainer/Zf1 Grad Norm                2061.51\n",
      "trainer/Zf1 Param Norm                114.984\n",
      "trainer/Zf2 Grad Norm                2036.55\n",
      "trainer/Zf2 Param Norm                114.248\n",
      "trainer/Z Expert Predictions Mean     402.045\n",
      "trainer/Z Expert Predictions Std       50.1828\n",
      "trainer/Z Expert Predictions Max      507.773\n",
      "trainer/Z Expert Predictions Min      301.81\n",
      "trainer/Z Policy Predictions Mean     205.291\n",
      "trainer/Z Policy Predictions Std      134.871\n",
      "trainer/Z Policy Predictions Max      393.818\n",
      "trainer/Z Policy Predictions Min      -36.2708\n",
      "trainer/Z Expert Targets Mean         374.453\n",
      "trainer/Z Expert Targets Std           50.101\n",
      "trainer/Z Expert Targets Max          482.321\n",
      "trainer/Z Expert Targets Min          275.6\n",
      "trainer/Z Policy Targets Mean         201.779\n",
      "trainer/Z Policy Targets Std          134.051\n",
      "trainer/Z Policy Targets Max          383.658\n",
      "trainer/Z Policy Targets Min          -47.3375\n",
      "trainer/Log Pis Mean                   15.7444\n",
      "trainer/Log Pis Std                     9.75932\n",
      "trainer/Policy mu Mean                  0.138055\n",
      "trainer/Policy mu Std                   1.31035\n",
      "trainer/Policy log std Mean            -1.14613\n",
      "trainer/Policy log std Std              0.677308\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        180920\n",
      "exploration/num paths total          1649\n",
      "evaluation/num steps total         131048\n",
      "evaluation/num paths total            360\n",
      "evaluation/path length Mean           792.273\n",
      "evaluation/path length Std            283.086\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            268\n",
      "evaluation/Rewards Mean                 5.26007\n",
      "evaluation/Rewards Std                  0.109272\n",
      "evaluation/Rewards Max                  6.02393\n",
      "evaluation/Rewards Min                  4.80692\n",
      "evaluation/Returns Mean              4167.41\n",
      "evaluation/Returns Std               1495.97\n",
      "evaluation/Returns Max               5274.28\n",
      "evaluation/Returns Min               1411.05\n",
      "evaluation/Estimation Bias Mean       241.238\n",
      "evaluation/Estimation Bias Std        170.585\n",
      "evaluation/EB/Q_True Mean              54.6161\n",
      "evaluation/EB/Q_True Std              155.823\n",
      "evaluation/EB/Q_Pred Mean             295.854\n",
      "evaluation/EB/Q_Pred Std               44.0292\n",
      "evaluation/Num Paths                   11\n",
      "evaluation/Average Returns           4167.41\n",
      "evaluation/Actions Mean                 0.076282\n",
      "evaluation/Actions Std                  0.511001\n",
      "evaluation/Actions Max                  0.999213\n",
      "evaluation/Actions Min                 -0.999615\n",
      "time/backward_policy (s)                8.45427\n",
      "time/backward_zf1 (s)                   9.80496\n",
      "time/backward_zf2 (s)                   9.49356\n",
      "time/data sampling (s)                  1.83606\n",
      "time/data storing (s)                   0.0906785\n",
      "time/evaluation sampling (s)            4.48773\n",
      "time/exploration sampling (s)           2.67024\n",
      "time/logging (s)                        0.0111871\n",
      "time/preback_alpha (s)                  0.00523577\n",
      "time/preback_policy (s)                15.3667\n",
      "time/preback_start (s)                  0.932618\n",
      "time/preback_zf (s)                    36.472\n",
      "time/saving (s)                         2.972e-06\n",
      "time/training (s)                      11.09\n",
      "time/epoch (s)                        100.715\n",
      "time/total (s)                       3418.9\n",
      "Epoch                                  34\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:16:35.346181 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 35 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 190000\n",
      "trainer/ZF1 Loss                       -1.66022\n",
      "trainer/ZF2 Loss                       -0.336817\n",
      "trainer/ZF Expert Reward               27.4155\n",
      "trainer/ZF Policy Reward                1.8593\n",
      "trainer/ZF CHI2 Term                   22.3745\n",
      "trainer/Policy Loss                  -199.052\n",
      "trainer/expert_lambda Loss             12.6703\n",
      "trainer/expert_lambda Value            25.1409\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               92.8033\n",
      "trainer/Policy Param Norm              35.9367\n",
      "trainer/Zf1 Grad Norm                1909.04\n",
      "trainer/Zf1 Param Norm                116.534\n",
      "trainer/Zf2 Grad Norm                1452.21\n",
      "trainer/Zf2 Param Norm                115.941\n",
      "trainer/Z Expert Predictions Mean     420.371\n",
      "trainer/Z Expert Predictions Std       53.2741\n",
      "trainer/Z Expert Predictions Max      555.15\n",
      "trainer/Z Expert Predictions Min      301.142\n",
      "trainer/Z Policy Predictions Mean     195.744\n",
      "trainer/Z Policy Predictions Std      118.347\n",
      "trainer/Z Policy Predictions Max      390.731\n",
      "trainer/Z Policy Predictions Min      -16.496\n",
      "trainer/Z Expert Targets Mean         392.955\n",
      "trainer/Z Expert Targets Std           52.8902\n",
      "trainer/Z Expert Targets Max          525.892\n",
      "trainer/Z Expert Targets Min          272.283\n",
      "trainer/Z Policy Targets Mean         193.885\n",
      "trainer/Z Policy Targets Std          118.533\n",
      "trainer/Z Policy Targets Max          370.971\n",
      "trainer/Z Policy Targets Min          -19.6361\n",
      "trainer/Log Pis Mean                   15.9353\n",
      "trainer/Log Pis Std                     9.67416\n",
      "trainer/Policy mu Mean                  0.100375\n",
      "trainer/Policy mu Std                   1.21093\n",
      "trainer/Policy log std Mean            -1.27307\n",
      "trainer/Policy log std Std              0.751885\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        183851\n",
      "exploration/num paths total          1653\n",
      "evaluation/num steps total         141048\n",
      "evaluation/num paths total            370\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.2843\n",
      "evaluation/Rewards Std                  0.0765234\n",
      "evaluation/Rewards Max                  5.44014\n",
      "evaluation/Rewards Min                  4.87198\n",
      "evaluation/Returns Mean              5284.3\n",
      "evaluation/Returns Std                  2.83028\n",
      "evaluation/Returns Max               5289.99\n",
      "evaluation/Returns Min               5279.64\n",
      "evaluation/Estimation Bias Mean       284.493\n",
      "evaluation/Estimation Bias Std        150.465\n",
      "evaluation/EB/Q_True Mean              47.7506\n",
      "evaluation/EB/Q_True Std              147.055\n",
      "evaluation/EB/Q_Pred Mean             332.244\n",
      "evaluation/EB/Q_Pred Std               26.2322\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5284.3\n",
      "evaluation/Actions Mean                 0.077152\n",
      "evaluation/Actions Std                  0.520266\n",
      "evaluation/Actions Max                  0.997506\n",
      "evaluation/Actions Min                 -0.998483\n",
      "time/backward_policy (s)                8.78232\n",
      "time/backward_zf1 (s)                  10.0578\n",
      "time/backward_zf2 (s)                   9.86316\n",
      "time/data sampling (s)                  1.82106\n",
      "time/data storing (s)                   0.0889415\n",
      "time/evaluation sampling (s)            4.41681\n",
      "time/exploration sampling (s)           2.50354\n",
      "time/logging (s)                        0.0124567\n",
      "time/preback_alpha (s)                  0.00523992\n",
      "time/preback_policy (s)                14.7905\n",
      "time/preback_start (s)                  0.925661\n",
      "time/preback_zf (s)                    36.4136\n",
      "time/saving (s)                         2.892e-06\n",
      "time/training (s)                      10.7331\n",
      "time/epoch (s)                        100.414\n",
      "time/total (s)                       3519.32\n",
      "Epoch                                  35\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:18:15.626497 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 36 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 195000\n",
      "trainer/ZF1 Loss                        2.29295\n",
      "trainer/ZF2 Loss                        3.91501\n",
      "trainer/ZF Expert Reward               28.4064\n",
      "trainer/ZF Policy Reward                2.68421\n",
      "trainer/ZF CHI2 Term                   27.156\n",
      "trainer/Policy Loss                  -205.124\n",
      "trainer/expert_lambda Loss             15.4507\n",
      "trainer/expert_lambda Value            25.5628\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              169.831\n",
      "trainer/Policy Param Norm              36.3034\n",
      "trainer/Zf1 Grad Norm                2325.56\n",
      "trainer/Zf1 Param Norm                118.358\n",
      "trainer/Zf2 Grad Norm                3057.4\n",
      "trainer/Zf2 Param Norm                117.931\n",
      "trainer/Z Expert Predictions Mean     451.862\n",
      "trainer/Z Expert Predictions Std       68.0313\n",
      "trainer/Z Expert Predictions Max      589.784\n",
      "trainer/Z Expert Predictions Min      299.77\n",
      "trainer/Z Policy Predictions Mean     200.773\n",
      "trainer/Z Policy Predictions Std      113.817\n",
      "trainer/Z Policy Predictions Max      452.153\n",
      "trainer/Z Policy Predictions Min       -5.56172\n",
      "trainer/Z Expert Targets Mean         423.456\n",
      "trainer/Z Expert Targets Std           68.0693\n",
      "trainer/Z Expert Targets Max          557.97\n",
      "trainer/Z Expert Targets Min          272.197\n",
      "trainer/Z Policy Targets Mean         198.089\n",
      "trainer/Z Policy Targets Std          113.005\n",
      "trainer/Z Policy Targets Max          439.551\n",
      "trainer/Z Policy Targets Min           -6.60863\n",
      "trainer/Log Pis Mean                   17.0279\n",
      "trainer/Log Pis Std                     8.97471\n",
      "trainer/Policy mu Mean                  0.151524\n",
      "trainer/Policy mu Std                   1.1779\n",
      "trainer/Policy log std Mean            -1.40191\n",
      "trainer/Policy log std Std              0.800513\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        189851\n",
      "exploration/num paths total          1659\n",
      "evaluation/num steps total         149956\n",
      "evaluation/num paths total            380\n",
      "evaluation/path length Mean           890.8\n",
      "evaluation/path length Std            219.764\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            267\n",
      "evaluation/Rewards Mean                 5.29968\n",
      "evaluation/Rewards Std                  0.135836\n",
      "evaluation/Rewards Max                  6.59978\n",
      "evaluation/Rewards Min                  3.4087\n",
      "evaluation/Returns Mean              4720.96\n",
      "evaluation/Returns Std               1165.04\n",
      "evaluation/Returns Max               5313.72\n",
      "evaluation/Returns Min               1421.39\n",
      "evaluation/Estimation Bias Mean       273.851\n",
      "evaluation/Estimation Bias Std        171.931\n",
      "evaluation/EB/Q_True Mean              53.8714\n",
      "evaluation/EB/Q_True Std              155.544\n",
      "evaluation/EB/Q_Pred Mean             327.723\n",
      "evaluation/EB/Q_Pred Std               75.4029\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4720.96\n",
      "evaluation/Actions Mean                 0.093891\n",
      "evaluation/Actions Std                  0.545649\n",
      "evaluation/Actions Max                  0.999399\n",
      "evaluation/Actions Min                 -0.999235\n",
      "time/backward_policy (s)                8.81399\n",
      "time/backward_zf1 (s)                  10.0801\n",
      "time/backward_zf2 (s)                   9.89289\n",
      "time/data sampling (s)                  1.78705\n",
      "time/data storing (s)                   0.0875347\n",
      "time/evaluation sampling (s)            4.59423\n",
      "time/exploration sampling (s)           2.45732\n",
      "time/logging (s)                        0.0106804\n",
      "time/preback_alpha (s)                  0.00519761\n",
      "time/preback_policy (s)                14.5826\n",
      "time/preback_start (s)                  0.936237\n",
      "time/preback_zf (s)                    36.2539\n",
      "time/saving (s)                         2.897e-06\n",
      "time/training (s)                      10.573\n",
      "time/epoch (s)                        100.075\n",
      "time/total (s)                       3619.4\n",
      "Epoch                                  36\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:19:55.946813 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 37 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 200000\n",
      "trainer/ZF1 Loss                        3.30631\n",
      "trainer/ZF2 Loss                        8.81511\n",
      "trainer/ZF Expert Reward               26.6025\n",
      "trainer/ZF Policy Reward                2.88092\n",
      "trainer/ZF CHI2 Term                   28.1638\n",
      "trainer/Policy Loss                  -200.34\n",
      "trainer/expert_lambda Loss             16.4649\n",
      "trainer/expert_lambda Value            25.979\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              108.955\n",
      "trainer/Policy Param Norm              36.6742\n",
      "trainer/Zf1 Grad Norm                1582.07\n",
      "trainer/Zf1 Param Norm                120.05\n",
      "trainer/Zf2 Grad Norm                3634.51\n",
      "trainer/Zf2 Param Norm                119.645\n",
      "trainer/Z Expert Predictions Mean     466.033\n",
      "trainer/Z Expert Predictions Std       50.9092\n",
      "trainer/Z Expert Predictions Max      573.667\n",
      "trainer/Z Expert Predictions Min      330.457\n",
      "trainer/Z Policy Predictions Mean     196.791\n",
      "trainer/Z Policy Predictions Std      116.416\n",
      "trainer/Z Policy Predictions Max      412.129\n",
      "trainer/Z Policy Predictions Min        0.318282\n",
      "trainer/Z Expert Targets Mean         439.431\n",
      "trainer/Z Expert Targets Std           51.1317\n",
      "trainer/Z Expert Targets Max          547.228\n",
      "trainer/Z Expert Targets Min          301.39\n",
      "trainer/Z Policy Targets Mean         193.91\n",
      "trainer/Z Policy Targets Std          116.671\n",
      "trainer/Z Policy Targets Max          399.805\n",
      "trainer/Z Policy Targets Min           -0.470141\n",
      "trainer/Log Pis Mean                   15.9689\n",
      "trainer/Log Pis Std                     8.86043\n",
      "trainer/Policy mu Mean                  0.267135\n",
      "trainer/Policy mu Std                   1.16484\n",
      "trainer/Policy log std Mean            -1.32368\n",
      "trainer/Policy log std Std              0.767605\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        195184\n",
      "exploration/num paths total          1665\n",
      "evaluation/num steps total         159956\n",
      "evaluation/num paths total            390\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.31394\n",
      "evaluation/Rewards Std                  0.0701265\n",
      "evaluation/Rewards Max                  5.46871\n",
      "evaluation/Rewards Min                  4.90844\n",
      "evaluation/Returns Mean              5313.94\n",
      "evaluation/Returns Std                  3.11985\n",
      "evaluation/Returns Max               5319.27\n",
      "evaluation/Returns Min               5309.11\n",
      "evaluation/Estimation Bias Mean       312.594\n",
      "evaluation/Estimation Bias Std        153.831\n",
      "evaluation/EB/Q_True Mean              48.0238\n",
      "evaluation/EB/Q_True Std              147.9\n",
      "evaluation/EB/Q_Pred Mean             360.618\n",
      "evaluation/EB/Q_Pred Std               42.1185\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5313.94\n",
      "evaluation/Actions Mean                 0.0665884\n",
      "evaluation/Actions Std                  0.512968\n",
      "evaluation/Actions Max                  0.998991\n",
      "evaluation/Actions Min                 -0.999296\n",
      "time/backward_policy (s)                8.70503\n",
      "time/backward_zf1 (s)                   9.95798\n",
      "time/backward_zf2 (s)                   9.76733\n",
      "time/data sampling (s)                  1.7991\n",
      "time/data storing (s)                   0.0873837\n",
      "time/evaluation sampling (s)            4.67157\n",
      "time/exploration sampling (s)           2.46335\n",
      "time/logging (s)                        0.0122289\n",
      "time/preback_alpha (s)                  0.00516224\n",
      "time/preback_policy (s)                14.7986\n",
      "time/preback_start (s)                  0.916301\n",
      "time/preback_zf (s)                    36.2334\n",
      "time/saving (s)                         2.773e-06\n",
      "time/training (s)                      10.6999\n",
      "time/epoch (s)                        100.117\n",
      "time/total (s)                       3719.52\n",
      "Epoch                                  37\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:21:36.420098 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 38 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 205000\n",
      "trainer/ZF1 Loss                       10.3732\n",
      "trainer/ZF2 Loss                        6.75867\n",
      "trainer/ZF Expert Reward               29.7795\n",
      "trainer/ZF Policy Reward                4.95631\n",
      "trainer/ZF CHI2 Term                   33.938\n",
      "trainer/Policy Loss                  -215.726\n",
      "trainer/expert_lambda Loss             22.8153\n",
      "trainer/expert_lambda Value            26.3964\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              126.283\n",
      "trainer/Policy Param Norm              36.9993\n",
      "trainer/Zf1 Grad Norm                4428.7\n",
      "trainer/Zf1 Param Norm                121.493\n",
      "trainer/Zf2 Grad Norm                1959.61\n",
      "trainer/Zf2 Param Norm                121.204\n",
      "trainer/Z Expert Predictions Mean     465.121\n",
      "trainer/Z Expert Predictions Std       90.0034\n",
      "trainer/Z Expert Predictions Max      627.993\n",
      "trainer/Z Expert Predictions Min      302.658\n",
      "trainer/Z Policy Predictions Mean     212.244\n",
      "trainer/Z Policy Predictions Std      119.452\n",
      "trainer/Z Policy Predictions Max      480.741\n",
      "trainer/Z Policy Predictions Min       -7.01161\n",
      "trainer/Z Expert Targets Mean         435.342\n",
      "trainer/Z Expert Targets Std           89.7221\n",
      "trainer/Z Expert Targets Max          599.494\n",
      "trainer/Z Expert Targets Min          274.194\n",
      "trainer/Z Policy Targets Mean         207.288\n",
      "trainer/Z Policy Targets Std          118.583\n",
      "trainer/Z Policy Targets Max          453.699\n",
      "trainer/Z Policy Targets Min           -9.21384\n",
      "trainer/Log Pis Mean                   15.762\n",
      "trainer/Log Pis Std                     8.70768\n",
      "trainer/Policy mu Mean                  0.191072\n",
      "trainer/Policy mu Std                   1.13559\n",
      "trainer/Policy log std Mean            -1.3665\n",
      "trainer/Policy log std Std              0.77732\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        199472\n",
      "exploration/num paths total          1670\n",
      "evaluation/num steps total         169956\n",
      "evaluation/num paths total            400\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.22428\n",
      "evaluation/Rewards Std                  0.0554428\n",
      "evaluation/Rewards Max                  5.40918\n",
      "evaluation/Rewards Min                  4.87371\n",
      "evaluation/Returns Mean              5224.28\n",
      "evaluation/Returns Std                 11.3835\n",
      "evaluation/Returns Max               5243.69\n",
      "evaluation/Returns Min               5208.3\n",
      "evaluation/Estimation Bias Mean       326.439\n",
      "evaluation/Estimation Bias Std        146.672\n",
      "evaluation/EB/Q_True Mean              46.9949\n",
      "evaluation/EB/Q_True Std              144.749\n",
      "evaluation/EB/Q_Pred Mean             373.434\n",
      "evaluation/EB/Q_Pred Std               28.1563\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5224.28\n",
      "evaluation/Actions Mean                 0.0864459\n",
      "evaluation/Actions Std                  0.53061\n",
      "evaluation/Actions Max                  0.999827\n",
      "evaluation/Actions Min                 -0.999583\n",
      "time/backward_policy (s)                8.69567\n",
      "time/backward_zf1 (s)                   9.96526\n",
      "time/backward_zf2 (s)                   9.74352\n",
      "time/data sampling (s)                  1.78855\n",
      "time/data storing (s)                   0.0876179\n",
      "time/evaluation sampling (s)            4.44205\n",
      "time/exploration sampling (s)           2.56489\n",
      "time/logging (s)                        0.0140989\n",
      "time/preback_alpha (s)                  0.00526832\n",
      "time/preback_policy (s)                14.8989\n",
      "time/preback_start (s)                  0.949283\n",
      "time/preback_zf (s)                    36.3301\n",
      "time/saving (s)                         2.873e-06\n",
      "time/training (s)                      10.784\n",
      "time/epoch (s)                        100.269\n",
      "time/total (s)                       3819.79\n",
      "Epoch                                  38\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:23:17.104783 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 39 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 210000\n",
      "trainer/ZF1 Loss                        9.23636\n",
      "trainer/ZF2 Loss                        6.74754\n",
      "trainer/ZF Expert Reward               27.7863\n",
      "trainer/ZF Policy Reward                0.473086\n",
      "trainer/ZF CHI2 Term                   31.8096\n",
      "trainer/Policy Loss                  -190.695\n",
      "trainer/expert_lambda Loss             16.0919\n",
      "trainer/expert_lambda Value            26.8183\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               45.5866\n",
      "trainer/Policy Param Norm              37.295\n",
      "trainer/Zf1 Grad Norm                4430.67\n",
      "trainer/Zf1 Param Norm                123.026\n",
      "trainer/Zf2 Grad Norm                3748.45\n",
      "trainer/Zf2 Param Norm                122.592\n",
      "trainer/Z Expert Predictions Mean     420.899\n",
      "trainer/Z Expert Predictions Std       81.8381\n",
      "trainer/Z Expert Predictions Max      611.631\n",
      "trainer/Z Expert Predictions Min      290.107\n",
      "trainer/Z Policy Predictions Mean     186.754\n",
      "trainer/Z Policy Predictions Std      125.12\n",
      "trainer/Z Policy Predictions Max      444.922\n",
      "trainer/Z Policy Predictions Min       -7.12713\n",
      "trainer/Z Expert Targets Mean         393.113\n",
      "trainer/Z Expert Targets Std           80.7918\n",
      "trainer/Z Expert Targets Max          577.321\n",
      "trainer/Z Expert Targets Min          262.809\n",
      "trainer/Z Policy Targets Mean         186.281\n",
      "trainer/Z Policy Targets Std          125.955\n",
      "trainer/Z Policy Targets Max          456.584\n",
      "trainer/Z Policy Targets Min           -9.10862\n",
      "trainer/Log Pis Mean                   13.4617\n",
      "trainer/Log Pis Std                     8.50437\n",
      "trainer/Policy mu Mean                  0.239278\n",
      "trainer/Policy mu Std                   1.21402\n",
      "trainer/Policy log std Mean            -1.1121\n",
      "trainer/Policy log std Std              0.701592\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        204472\n",
      "exploration/num paths total          1675\n",
      "evaluation/num steps total         179956\n",
      "evaluation/num paths total            410\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.24446\n",
      "evaluation/Rewards Std                  0.0562089\n",
      "evaluation/Rewards Max                  5.43166\n",
      "evaluation/Rewards Min                  4.85173\n",
      "evaluation/Returns Mean              5244.46\n",
      "evaluation/Returns Std                  2.70221\n",
      "evaluation/Returns Max               5250.08\n",
      "evaluation/Returns Min               5240.71\n",
      "evaluation/Estimation Bias Mean       324.962\n",
      "evaluation/Estimation Bias Std        146.933\n",
      "evaluation/EB/Q_True Mean              47.379\n",
      "evaluation/EB/Q_True Std              145.916\n",
      "evaluation/EB/Q_Pred Mean             372.341\n",
      "evaluation/EB/Q_Pred Std               19.0656\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5244.46\n",
      "evaluation/Actions Mean                 0.105075\n",
      "evaluation/Actions Std                  0.48726\n",
      "evaluation/Actions Max                  0.998744\n",
      "evaluation/Actions Min                 -0.992951\n",
      "time/backward_policy (s)                8.53485\n",
      "time/backward_zf1 (s)                   9.82211\n",
      "time/backward_zf2 (s)                   9.58463\n",
      "time/data sampling (s)                  1.76164\n",
      "time/data storing (s)                   0.0879744\n",
      "time/evaluation sampling (s)            4.78114\n",
      "time/exploration sampling (s)           2.57219\n",
      "time/logging (s)                        0.0145666\n",
      "time/preback_alpha (s)                  0.00517445\n",
      "time/preback_policy (s)                15.1147\n",
      "time/preback_start (s)                  0.937406\n",
      "time/preback_zf (s)                    36.2922\n",
      "time/saving (s)                         5.167e-06\n",
      "time/training (s)                      10.9716\n",
      "time/epoch (s)                        100.48\n",
      "time/total (s)                       3920.28\n",
      "Epoch                                  39\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:24:57.469922 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 40 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 215000\n",
      "trainer/ZF1 Loss                       -2.76957\n",
      "trainer/ZF2 Loss                       -3.66843\n",
      "trainer/ZF Expert Reward               27.6394\n",
      "trainer/ZF Policy Reward                1.40054\n",
      "trainer/ZF CHI2 Term                   20.5066\n",
      "trainer/Policy Loss                  -208.285\n",
      "trainer/expert_lambda Loss             11.1861\n",
      "trainer/expert_lambda Value            27.2324\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               58.5481\n",
      "trainer/Policy Param Norm              37.5573\n",
      "trainer/Zf1 Grad Norm                1777.28\n",
      "trainer/Zf1 Param Norm                124.509\n",
      "trainer/Zf2 Grad Norm                1608.45\n",
      "trainer/Zf2 Param Norm                123.923\n",
      "trainer/Z Expert Predictions Mean     413\n",
      "trainer/Z Expert Predictions Std       65.4101\n",
      "trainer/Z Expert Predictions Max      527.765\n",
      "trainer/Z Expert Predictions Min      295.883\n",
      "trainer/Z Policy Predictions Mean     205.74\n",
      "trainer/Z Policy Predictions Std      125.964\n",
      "trainer/Z Policy Predictions Max      402.185\n",
      "trainer/Z Policy Predictions Min       -2.09912\n",
      "trainer/Z Expert Targets Mean         385.36\n",
      "trainer/Z Expert Targets Std           65.8879\n",
      "trainer/Z Expert Targets Max          500.93\n",
      "trainer/Z Expert Targets Min          267.367\n",
      "trainer/Z Policy Targets Mean         204.339\n",
      "trainer/Z Policy Targets Std          126.736\n",
      "trainer/Z Policy Targets Max          405.574\n",
      "trainer/Z Policy Targets Min           -1.40461\n",
      "trainer/Log Pis Mean                   13.5441\n",
      "trainer/Log Pis Std                     7.91673\n",
      "trainer/Policy mu Mean                  0.252974\n",
      "trainer/Policy mu Std                   1.11041\n",
      "trainer/Policy log std Mean            -1.22006\n",
      "trainer/Policy log std Std              0.638101\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        210252\n",
      "exploration/num paths total          1682\n",
      "evaluation/num steps total         189956\n",
      "evaluation/num paths total            420\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.21069\n",
      "evaluation/Rewards Std                  0.0577954\n",
      "evaluation/Rewards Max                  5.40485\n",
      "evaluation/Rewards Min                  4.81792\n",
      "evaluation/Returns Mean              5210.69\n",
      "evaluation/Returns Std                  2.28582\n",
      "evaluation/Returns Max               5214.04\n",
      "evaluation/Returns Min               5207.23\n",
      "evaluation/Estimation Bias Mean       287.119\n",
      "evaluation/Estimation Bias Std        146.291\n",
      "evaluation/EB/Q_True Mean              47.0162\n",
      "evaluation/EB/Q_True Std              144.788\n",
      "evaluation/EB/Q_Pred Mean             334.136\n",
      "evaluation/EB/Q_Pred Std               22.0589\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5210.69\n",
      "evaluation/Actions Mean                 0.0904909\n",
      "evaluation/Actions Std                  0.446583\n",
      "evaluation/Actions Max                  0.999448\n",
      "evaluation/Actions Min                 -0.999185\n",
      "time/backward_policy (s)                8.70006\n",
      "time/backward_zf1 (s)                   9.98573\n",
      "time/backward_zf2 (s)                   9.77994\n",
      "time/data sampling (s)                  1.75728\n",
      "time/data storing (s)                   0.0881898\n",
      "time/evaluation sampling (s)            4.41616\n",
      "time/exploration sampling (s)           2.60632\n",
      "time/logging (s)                        0.0121176\n",
      "time/preback_alpha (s)                  0.00519942\n",
      "time/preback_policy (s)                14.8074\n",
      "time/preback_start (s)                  0.92131\n",
      "time/preback_zf (s)                    36.3435\n",
      "time/saving (s)                         2.757e-06\n",
      "time/training (s)                      10.7332\n",
      "time/epoch (s)                        100.156\n",
      "time/total (s)                       4020.44\n",
      "Epoch                                  40\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:26:38.637659 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 41 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 220000\n",
      "trainer/ZF1 Loss                        6.31418\n",
      "trainer/ZF2 Loss                        0.883064\n",
      "trainer/ZF Expert Reward               30.5561\n",
      "trainer/ZF Policy Reward                1.8354\n",
      "trainer/ZF CHI2 Term                   30.0084\n",
      "trainer/Policy Loss                  -208.085\n",
      "trainer/expert_lambda Loss             19.8774\n",
      "trainer/expert_lambda Value            27.6582\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               71.0583\n",
      "trainer/Policy Param Norm              37.7986\n",
      "trainer/Zf1 Grad Norm                2888.97\n",
      "trainer/Zf1 Param Norm                126.111\n",
      "trainer/Zf2 Grad Norm                2555.58\n",
      "trainer/Zf2 Param Norm                125.447\n",
      "trainer/Z Expert Predictions Mean     433.048\n",
      "trainer/Z Expert Predictions Std       86.3446\n",
      "trainer/Z Expert Predictions Max      676.132\n",
      "trainer/Z Expert Predictions Min      309.431\n",
      "trainer/Z Policy Predictions Mean     203.405\n",
      "trainer/Z Policy Predictions Std      123.994\n",
      "trainer/Z Policy Predictions Max      375.96\n",
      "trainer/Z Policy Predictions Min       -6.70329\n",
      "trainer/Z Expert Targets Mean         402.491\n",
      "trainer/Z Expert Targets Std           86.0917\n",
      "trainer/Z Expert Targets Max          647.934\n",
      "trainer/Z Expert Targets Min          277.913\n",
      "trainer/Z Policy Targets Mean         201.57\n",
      "trainer/Z Policy Targets Std          124.876\n",
      "trainer/Z Policy Targets Max          365.227\n",
      "trainer/Z Policy Targets Min           -9.58657\n",
      "trainer/Log Pis Mean                   14.4654\n",
      "trainer/Log Pis Std                     7.78025\n",
      "trainer/Policy mu Mean                  0.23119\n",
      "trainer/Policy mu Std                   1.10836\n",
      "trainer/Policy log std Mean            -1.28492\n",
      "trainer/Policy log std Std              0.70065\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        214366\n",
      "exploration/num paths total          1687\n",
      "evaluation/num steps total         199956\n",
      "evaluation/num paths total            430\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.14557\n",
      "evaluation/Rewards Std                  0.0468825\n",
      "evaluation/Rewards Max                  5.30958\n",
      "evaluation/Rewards Min                  4.80384\n",
      "evaluation/Returns Mean              5145.57\n",
      "evaluation/Returns Std                  2.67702\n",
      "evaluation/Returns Max               5149.75\n",
      "evaluation/Returns Min               5141\n",
      "evaluation/Estimation Bias Mean       227.442\n",
      "evaluation/Estimation Bias Std        145.049\n",
      "evaluation/EB/Q_True Mean              46.3953\n",
      "evaluation/EB/Q_True Std              142.913\n",
      "evaluation/EB/Q_Pred Mean             273.838\n",
      "evaluation/EB/Q_Pred Std               27.6842\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5145.57\n",
      "evaluation/Actions Mean                 0.0488768\n",
      "evaluation/Actions Std                  0.448249\n",
      "evaluation/Actions Max                  0.997038\n",
      "evaluation/Actions Min                 -0.997997\n",
      "time/backward_policy (s)                8.85542\n",
      "time/backward_zf1 (s)                  10.159\n",
      "time/backward_zf2 (s)                   9.92234\n",
      "time/data sampling (s)                  1.76056\n",
      "time/data storing (s)                   0.0884581\n",
      "time/evaluation sampling (s)            4.61431\n",
      "time/exploration sampling (s)           2.56557\n",
      "time/logging (s)                        0.0135316\n",
      "time/preback_alpha (s)                  0.00524199\n",
      "time/preback_policy (s)                14.7884\n",
      "time/preback_start (s)                  0.936477\n",
      "time/preback_zf (s)                    36.4459\n",
      "time/saving (s)                         4.44e-06\n",
      "time/training (s)                      10.8066\n",
      "time/epoch (s)                        100.962\n",
      "time/total (s)                       4121.4\n",
      "Epoch                                  41\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:28:19.255238 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 42 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 225000\n",
      "trainer/ZF1 Loss                        4.45378\n",
      "trainer/ZF2 Loss                       -0.583178\n",
      "trainer/ZF Expert Reward               27.9135\n",
      "trainer/ZF Policy Reward                3.3016\n",
      "trainer/ZF CHI2 Term                   26.1938\n",
      "trainer/Policy Loss                  -201.004\n",
      "trainer/expert_lambda Loss             18.112\n",
      "trainer/expert_lambda Value            28.0908\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               81.6968\n",
      "trainer/Policy Param Norm              38.057\n",
      "trainer/Zf1 Grad Norm                1776.63\n",
      "trainer/Zf1 Param Norm                127.471\n",
      "trainer/Zf2 Grad Norm                1493.98\n",
      "trainer/Zf2 Param Norm                126.725\n",
      "trainer/Z Expert Predictions Mean     387.526\n",
      "trainer/Z Expert Predictions Std       71.7149\n",
      "trainer/Z Expert Predictions Max      627.271\n",
      "trainer/Z Expert Predictions Min      293.529\n",
      "trainer/Z Policy Predictions Mean     197.896\n",
      "trainer/Z Policy Predictions Std      115.271\n",
      "trainer/Z Policy Predictions Max      452.994\n",
      "trainer/Z Policy Predictions Min       -6.11513\n",
      "trainer/Z Expert Targets Mean         359.613\n",
      "trainer/Z Expert Targets Std           73.3918\n",
      "trainer/Z Expert Targets Max          600.999\n",
      "trainer/Z Expert Targets Min          262.82\n",
      "trainer/Z Policy Targets Mean         194.595\n",
      "trainer/Z Policy Targets Std          114.927\n",
      "trainer/Z Policy Targets Max          447.341\n",
      "trainer/Z Policy Targets Min           -6.21323\n",
      "trainer/Log Pis Mean                   12.7509\n",
      "trainer/Log Pis Std                     8.49813\n",
      "trainer/Policy mu Mean                  0.152129\n",
      "trainer/Policy mu Std                   1.08518\n",
      "trainer/Policy log std Mean            -1.26924\n",
      "trainer/Policy log std Std              0.701516\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        220366\n",
      "exploration/num paths total          1693\n",
      "evaluation/num steps total         209232\n",
      "evaluation/num paths total            440\n",
      "evaluation/path length Mean           927.6\n",
      "evaluation/path length Std            217.2\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            276\n",
      "evaluation/Rewards Mean                 5.26392\n",
      "evaluation/Rewards Std                  0.0745415\n",
      "evaluation/Rewards Max                  5.45869\n",
      "evaluation/Rewards Min                  4.87154\n",
      "evaluation/Returns Mean              4882.81\n",
      "evaluation/Returns Std               1145.27\n",
      "evaluation/Returns Max               5279.32\n",
      "evaluation/Returns Min               1447.03\n",
      "evaluation/Estimation Bias Mean       224.214\n",
      "evaluation/Estimation Bias Std        153.912\n",
      "evaluation/EB/Q_True Mean              51.2248\n",
      "evaluation/EB/Q_True Std              151.339\n",
      "evaluation/EB/Q_Pred Mean             275.439\n",
      "evaluation/EB/Q_Pred Std               19.5927\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4882.81\n",
      "evaluation/Actions Mean                 0.0773609\n",
      "evaluation/Actions Std                  0.460916\n",
      "evaluation/Actions Max                  0.999\n",
      "evaluation/Actions Min                 -0.995912\n",
      "time/backward_policy (s)                8.69504\n",
      "time/backward_zf1 (s)                   9.97871\n",
      "time/backward_zf2 (s)                   9.7552\n",
      "time/data sampling (s)                  1.76469\n",
      "time/data storing (s)                   0.0869974\n",
      "time/evaluation sampling (s)            4.66269\n",
      "time/exploration sampling (s)           2.5773\n",
      "time/logging (s)                        0.0117257\n",
      "time/preback_alpha (s)                  0.00519274\n",
      "time/preback_policy (s)                14.8575\n",
      "time/preback_start (s)                  0.919386\n",
      "time/preback_zf (s)                    36.314\n",
      "time/saving (s)                         6.27e-06\n",
      "time/training (s)                      10.7817\n",
      "time/epoch (s)                        100.41\n",
      "time/total (s)                       4221.82\n",
      "Epoch                                  42\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:29:59.421290 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 43 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 230000\n",
      "trainer/ZF1 Loss                       -2.71282\n",
      "trainer/ZF2 Loss                       -5.2459\n",
      "trainer/ZF Expert Reward               31.9449\n",
      "trainer/ZF Policy Reward                2.2765\n",
      "trainer/ZF CHI2 Term                   24.795\n",
      "trainer/Policy Loss                  -180.553\n",
      "trainer/expert_lambda Loss             28.7215\n",
      "trainer/expert_lambda Value            28.5179\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               48.2622\n",
      "trainer/Policy Param Norm              38.267\n",
      "trainer/Zf1 Grad Norm                1191.29\n",
      "trainer/Zf1 Param Norm                128.902\n",
      "trainer/Zf2 Grad Norm                1119.12\n",
      "trainer/Zf2 Param Norm                128.079\n",
      "trainer/Z Expert Predictions Mean     340.908\n",
      "trainer/Z Expert Predictions Std       62.9034\n",
      "trainer/Z Expert Predictions Max      589.673\n",
      "trainer/Z Expert Predictions Min      272.987\n",
      "trainer/Z Policy Predictions Mean     178.308\n",
      "trainer/Z Policy Predictions Std      101.093\n",
      "trainer/Z Policy Predictions Max      327.525\n",
      "trainer/Z Policy Predictions Min       -2.63133\n",
      "trainer/Z Expert Targets Mean         308.963\n",
      "trainer/Z Expert Targets Std           63.3462\n",
      "trainer/Z Expert Targets Max          555.599\n",
      "trainer/Z Expert Targets Min          240.782\n",
      "trainer/Z Policy Targets Mean         176.031\n",
      "trainer/Z Policy Targets Std          100.317\n",
      "trainer/Z Policy Targets Max          324.461\n",
      "trainer/Z Policy Targets Min           -4.48806\n",
      "trainer/Log Pis Mean                   11.0002\n",
      "trainer/Log Pis Std                     7.67507\n",
      "trainer/Policy mu Mean                  0.154277\n",
      "trainer/Policy mu Std                   1.05806\n",
      "trainer/Policy log std Mean            -1.18795\n",
      "trainer/Policy log std Std              0.617054\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        224984\n",
      "exploration/num paths total          1698\n",
      "evaluation/num steps total         218546\n",
      "evaluation/num paths total            450\n",
      "evaluation/path length Mean           931.4\n",
      "evaluation/path length Std            110.505\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            682\n",
      "evaluation/Rewards Mean                 5.28917\n",
      "evaluation/Rewards Std                  0.0991811\n",
      "evaluation/Rewards Max                  5.53905\n",
      "evaluation/Rewards Min                  4.3982\n",
      "evaluation/Returns Mean              4926.33\n",
      "evaluation/Returns Std                590.7\n",
      "evaluation/Returns Max               5303.78\n",
      "evaluation/Returns Min               3621.72\n",
      "evaluation/Estimation Bias Mean       186.941\n",
      "evaluation/Estimation Bias Std        155.339\n",
      "evaluation/EB/Q_True Mean              51.3798\n",
      "evaluation/EB/Q_True Std              152.114\n",
      "evaluation/EB/Q_Pred Mean             238.321\n",
      "evaluation/EB/Q_Pred Std               29.4942\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4926.33\n",
      "evaluation/Actions Mean                 0.0664459\n",
      "evaluation/Actions Std                  0.448801\n",
      "evaluation/Actions Max                  0.999634\n",
      "evaluation/Actions Min                 -0.998791\n",
      "time/backward_policy (s)                8.63007\n",
      "time/backward_zf1 (s)                   9.94581\n",
      "time/backward_zf2 (s)                   9.70322\n",
      "time/data sampling (s)                  1.83782\n",
      "time/data storing (s)                   0.0869201\n",
      "time/evaluation sampling (s)            4.33941\n",
      "time/exploration sampling (s)           2.50182\n",
      "time/logging (s)                        0.0160195\n",
      "time/preback_alpha (s)                  0.00518883\n",
      "time/preback_policy (s)                14.8662\n",
      "time/preback_start (s)                  0.921951\n",
      "time/preback_zf (s)                    36.3137\n",
      "time/saving (s)                         4.961e-06\n",
      "time/training (s)                      10.7978\n",
      "time/epoch (s)                         99.966\n",
      "time/total (s)                       4321.79\n",
      "Epoch                                  43\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:31:39.899739 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 44 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 235000\n",
      "trainer/ZF1 Loss                        1.74446\n",
      "trainer/ZF2 Loss                       19.5211\n",
      "trainer/ZF Expert Reward               32.3795\n",
      "trainer/ZF Policy Reward                3.60505\n",
      "trainer/ZF CHI2 Term                   39.607\n",
      "trainer/Policy Loss                  -172.914\n",
      "trainer/expert_lambda Loss             32.0157\n",
      "trainer/expert_lambda Value            28.947\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              124.391\n",
      "trainer/Policy Param Norm              38.4516\n",
      "trainer/Zf1 Grad Norm                2205.59\n",
      "trainer/Zf1 Param Norm                130.29\n",
      "trainer/Zf2 Grad Norm                5403.76\n",
      "trainer/Zf2 Param Norm                129.518\n",
      "trainer/Z Expert Predictions Mean     356.599\n",
      "trainer/Z Expert Predictions Std       61.7644\n",
      "trainer/Z Expert Predictions Max      600.017\n",
      "trainer/Z Expert Predictions Min      253.213\n",
      "trainer/Z Policy Predictions Mean     171.274\n",
      "trainer/Z Policy Predictions Std       95.4017\n",
      "trainer/Z Policy Predictions Max      401.026\n",
      "trainer/Z Policy Predictions Min       -2.8531\n",
      "trainer/Z Expert Targets Mean         324.219\n",
      "trainer/Z Expert Targets Std           61.9868\n",
      "trainer/Z Expert Targets Max          566.277\n",
      "trainer/Z Expert Targets Min          221.244\n",
      "trainer/Z Policy Targets Mean         167.669\n",
      "trainer/Z Policy Targets Std           93.7373\n",
      "trainer/Z Policy Targets Max          352.325\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   12.8091\n",
      "trainer/Log Pis Std                     7.9209\n",
      "trainer/Policy mu Mean                  0.175274\n",
      "trainer/Policy mu Std                   0.989294\n",
      "trainer/Policy log std Mean            -1.34819\n",
      "trainer/Policy log std Std              0.746406\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        233659\n",
      "exploration/num paths total          1715\n",
      "evaluation/num steps total         226174\n",
      "evaluation/num paths total            464\n",
      "evaluation/path length Mean           544.857\n",
      "evaluation/path length Std            455.164\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             77\n",
      "evaluation/Rewards Mean                 5.28933\n",
      "evaluation/Rewards Std                  0.236886\n",
      "evaluation/Rewards Max                  5.49876\n",
      "evaluation/Rewards Min                  4.06982\n",
      "evaluation/Returns Mean              2881.93\n",
      "evaluation/Returns Std               2467.38\n",
      "evaluation/Returns Max               5358.4\n",
      "evaluation/Returns Min                349.599\n",
      "evaluation/Estimation Bias Mean       255.24\n",
      "evaluation/Estimation Bias Std        224.403\n",
      "evaluation/EB/Q_True Mean              63.5055\n",
      "evaluation/EB/Q_True Std              167.976\n",
      "evaluation/EB/Q_Pred Mean             318.745\n",
      "evaluation/EB/Q_Pred Std               77.2446\n",
      "evaluation/Num Paths                   14\n",
      "evaluation/Average Returns           2881.93\n",
      "evaluation/Actions Mean                 0.0862107\n",
      "evaluation/Actions Std                  0.499559\n",
      "evaluation/Actions Max                  0.999742\n",
      "evaluation/Actions Min                 -0.998564\n",
      "time/backward_policy (s)                8.42888\n",
      "time/backward_zf1 (s)                   9.72288\n",
      "time/backward_zf2 (s)                   9.45765\n",
      "time/data sampling (s)                  1.82995\n",
      "time/data storing (s)                   0.0877701\n",
      "time/evaluation sampling (s)            4.57017\n",
      "time/exploration sampling (s)           2.57984\n",
      "time/logging (s)                        0.0162303\n",
      "time/preback_alpha (s)                  0.00516634\n",
      "time/preback_policy (s)                15.2712\n",
      "time/preback_start (s)                  0.947422\n",
      "time/preback_zf (s)                    36.3461\n",
      "time/saving (s)                         5.148e-06\n",
      "time/training (s)                      11.0103\n",
      "time/epoch (s)                        100.274\n",
      "time/total (s)                       4422.07\n",
      "Epoch                                  44\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:33:18.128243 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 45 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 240000\n",
      "trainer/ZF1 Loss                        2.70983\n",
      "trainer/ZF2 Loss                       14.5297\n",
      "trainer/ZF Expert Reward               32.0397\n",
      "trainer/ZF Policy Reward                4.09115\n",
      "trainer/ZF CHI2 Term                   36.9642\n",
      "trainer/Policy Loss                  -196.686\n",
      "trainer/expert_lambda Loss             19.1717\n",
      "trainer/expert_lambda Value            29.3627\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               96.1445\n",
      "trainer/Policy Param Norm              38.7082\n",
      "trainer/Zf1 Grad Norm                1901.75\n",
      "trainer/Zf1 Param Norm                131.667\n",
      "trainer/Zf2 Grad Norm                5745.27\n",
      "trainer/Zf2 Param Norm                131.042\n",
      "trainer/Z Expert Predictions Mean     390.562\n",
      "trainer/Z Expert Predictions Std      124.739\n",
      "trainer/Z Expert Predictions Max      630.525\n",
      "trainer/Z Expert Predictions Min      225.758\n",
      "trainer/Z Policy Predictions Mean     194.922\n",
      "trainer/Z Policy Predictions Std      117.743\n",
      "trainer/Z Policy Predictions Max      467.498\n",
      "trainer/Z Policy Predictions Min       -2.18658\n",
      "trainer/Z Expert Targets Mean         358.523\n",
      "trainer/Z Expert Targets Std          125.135\n",
      "trainer/Z Expert Targets Max          597.997\n",
      "trainer/Z Expert Targets Min          193.037\n",
      "trainer/Z Policy Targets Mean         190.831\n",
      "trainer/Z Policy Targets Std          115.786\n",
      "trainer/Z Policy Targets Max          478.627\n",
      "trainer/Z Policy Targets Min           -5.63723\n",
      "trainer/Log Pis Mean                   13.8667\n",
      "trainer/Log Pis Std                     7.9265\n",
      "trainer/Policy mu Mean                  0.251719\n",
      "trainer/Policy mu Std                   1.04919\n",
      "trainer/Policy log std Mean            -1.34726\n",
      "trainer/Policy log std Std              0.777287\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        233918\n",
      "exploration/num paths total          1716\n",
      "evaluation/num steps total         227479\n",
      "evaluation/num paths total            474\n",
      "evaluation/path length Mean           130.5\n",
      "evaluation/path length Std            108.048\n",
      "evaluation/path length Max            451\n",
      "evaluation/path length Min             81\n",
      "evaluation/Rewards Mean                 4.88311\n",
      "evaluation/Rewards Std                  0.42755\n",
      "evaluation/Rewards Max                  5.89869\n",
      "evaluation/Rewards Min                  3.51953\n",
      "evaluation/Returns Mean               637.246\n",
      "evaluation/Returns Std                588.258\n",
      "evaluation/Returns Max               2386.37\n",
      "evaluation/Returns Min                375.533\n",
      "evaluation/Estimation Bias Mean        53.5089\n",
      "evaluation/Estimation Bias Std        275.035\n",
      "evaluation/EB/Q_True Mean             144.017\n",
      "evaluation/EB/Q_True Std              211.863\n",
      "evaluation/EB/Q_Pred Mean             197.526\n",
      "evaluation/EB/Q_Pred Std              137.642\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns            637.246\n",
      "evaluation/Actions Mean                 0.0985023\n",
      "evaluation/Actions Std                  0.524787\n",
      "evaluation/Actions Max                  0.999736\n",
      "evaluation/Actions Min                 -0.999524\n",
      "time/backward_policy (s)                8.78535\n",
      "time/backward_zf1 (s)                  10.0746\n",
      "time/backward_zf2 (s)                   9.86056\n",
      "time/data sampling (s)                  1.7791\n",
      "time/data storing (s)                   0.0862609\n",
      "time/evaluation sampling (s)            2.0749\n",
      "time/exploration sampling (s)           2.56359\n",
      "time/logging (s)                        0.00268108\n",
      "time/preback_alpha (s)                  0.00525432\n",
      "time/preback_policy (s)                14.7111\n",
      "time/preback_start (s)                  0.916325\n",
      "time/preback_zf (s)                    36.4304\n",
      "time/saving (s)                         7.759e-06\n",
      "time/training (s)                      10.7182\n",
      "time/epoch (s)                         98.0083\n",
      "time/total (s)                       4520.08\n",
      "Epoch                                  45\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:34:59.806099 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 46 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 245000\n",
      "trainer/ZF1 Loss                       -6.14574\n",
      "trainer/ZF2 Loss                       -3.48478\n",
      "trainer/ZF Expert Reward               31.513\n",
      "trainer/ZF Policy Reward                2.97809\n",
      "trainer/ZF CHI2 Term                   22.7994\n",
      "trainer/Policy Loss                  -225.774\n",
      "trainer/expert_lambda Loss             11.5743\n",
      "trainer/expert_lambda Value            29.7873\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               86.4556\n",
      "trainer/Policy Param Norm              38.9914\n",
      "trainer/Zf1 Grad Norm                2178.16\n",
      "trainer/Zf1 Param Norm                133.196\n",
      "trainer/Zf2 Grad Norm                3136.42\n",
      "trainer/Zf2 Param Norm                132.559\n",
      "trainer/Z Expert Predictions Mean     411.212\n",
      "trainer/Z Expert Predictions Std       91.9479\n",
      "trainer/Z Expert Predictions Max      636.526\n",
      "trainer/Z Expert Predictions Min      266.069\n",
      "trainer/Z Policy Predictions Mean     222.644\n",
      "trainer/Z Policy Predictions Std      134.592\n",
      "trainer/Z Policy Predictions Max      405.878\n",
      "trainer/Z Policy Predictions Min       -7.16358\n",
      "trainer/Z Expert Targets Mean         379.699\n",
      "trainer/Z Expert Targets Std           91.3629\n",
      "trainer/Z Expert Targets Max          610.314\n",
      "trainer/Z Expert Targets Min          236.282\n",
      "trainer/Z Policy Targets Mean         219.666\n",
      "trainer/Z Policy Targets Std          134.65\n",
      "trainer/Z Policy Targets Max          409.407\n",
      "trainer/Z Policy Targets Min          -11.51\n",
      "trainer/Log Pis Mean                   14.1116\n",
      "trainer/Log Pis Std                     8.43094\n",
      "trainer/Policy mu Mean                  0.172664\n",
      "trainer/Policy mu Std                   1.08904\n",
      "trainer/Policy log std Mean            -1.31222\n",
      "trainer/Policy log std Std              0.702581\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        243129\n",
      "exploration/num paths total          1727\n",
      "evaluation/num steps total         237479\n",
      "evaluation/num paths total            484\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.26645\n",
      "evaluation/Rewards Std                  0.0629242\n",
      "evaluation/Rewards Max                  5.39424\n",
      "evaluation/Rewards Min                  4.83113\n",
      "evaluation/Returns Mean              5266.45\n",
      "evaluation/Returns Std                  1.29078\n",
      "evaluation/Returns Max               5268.09\n",
      "evaluation/Returns Min               5264.46\n",
      "evaluation/Estimation Bias Mean       324.175\n",
      "evaluation/Estimation Bias Std        147.253\n",
      "evaluation/EB/Q_True Mean              47.5184\n",
      "evaluation/EB/Q_True Std              146.348\n",
      "evaluation/EB/Q_Pred Mean             371.693\n",
      "evaluation/EB/Q_Pred Std               16.5399\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5266.45\n",
      "evaluation/Actions Mean                 0.0815536\n",
      "evaluation/Actions Std                  0.461743\n",
      "evaluation/Actions Max                  0.998078\n",
      "evaluation/Actions Min                 -0.994055\n",
      "time/backward_policy (s)                9.05463\n",
      "time/backward_zf1 (s)                  10.4014\n",
      "time/backward_zf2 (s)                  10.1942\n",
      "time/data sampling (s)                  1.78408\n",
      "time/data storing (s)                   0.0869782\n",
      "time/evaluation sampling (s)            4.67312\n",
      "time/exploration sampling (s)           2.60349\n",
      "time/logging (s)                        0.0127079\n",
      "time/preback_alpha (s)                  0.00526937\n",
      "time/preback_policy (s)                14.5724\n",
      "time/preback_start (s)                  0.920993\n",
      "time/preback_zf (s)                    36.4494\n",
      "time/saving (s)                         2.854e-06\n",
      "time/training (s)                      10.7251\n",
      "time/epoch (s)                        101.484\n",
      "time/total (s)                       4621.56\n",
      "Epoch                                  46\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:36:39.649855 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 47 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 250000\n",
      "trainer/ZF1 Loss                        0.700417\n",
      "trainer/ZF2 Loss                        3.32903\n",
      "trainer/ZF Expert Reward               32.9535\n",
      "trainer/ZF Policy Reward                3.28961\n",
      "trainer/ZF CHI2 Term                   30.6126\n",
      "trainer/Policy Loss                  -228.331\n",
      "trainer/expert_lambda Loss             24.9627\n",
      "trainer/expert_lambda Value            30.2226\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               83.3111\n",
      "trainer/Policy Param Norm              39.2132\n",
      "trainer/Zf1 Grad Norm                3059.89\n",
      "trainer/Zf1 Param Norm                134.676\n",
      "trainer/Zf2 Grad Norm                2185.13\n",
      "trainer/Zf2 Param Norm                133.963\n",
      "trainer/Z Expert Predictions Mean     459.376\n",
      "trainer/Z Expert Predictions Std       93.755\n",
      "trainer/Z Expert Predictions Max      682.547\n",
      "trainer/Z Expert Predictions Min      329.553\n",
      "trainer/Z Policy Predictions Mean     225.179\n",
      "trainer/Z Policy Predictions Std      135.607\n",
      "trainer/Z Policy Predictions Max      412.082\n",
      "trainer/Z Policy Predictions Min       -0.101269\n",
      "trainer/Z Expert Targets Mean         426.423\n",
      "trainer/Z Expert Targets Std           92.2471\n",
      "trainer/Z Expert Targets Max          649.901\n",
      "trainer/Z Expert Targets Min          299.196\n",
      "trainer/Z Policy Targets Mean         221.889\n",
      "trainer/Z Policy Targets Std          135.437\n",
      "trainer/Z Policy Targets Max          413.624\n",
      "trainer/Z Policy Targets Min           -0.210697\n",
      "trainer/Log Pis Mean                   13.4184\n",
      "trainer/Log Pis Std                     8.32593\n",
      "trainer/Policy mu Mean                  0.121518\n",
      "trainer/Policy mu Std                   1.07131\n",
      "trainer/Policy log std Mean            -1.30422\n",
      "trainer/Policy log std Std              0.70782\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        244129\n",
      "exploration/num paths total          1728\n",
      "evaluation/num steps total         247479\n",
      "evaluation/num paths total            494\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.23792\n",
      "evaluation/Rewards Std                  0.062266\n",
      "evaluation/Rewards Max                  5.41453\n",
      "evaluation/Rewards Min                  4.82168\n",
      "evaluation/Returns Mean              5237.92\n",
      "evaluation/Returns Std                  2.86021\n",
      "evaluation/Returns Max               5241.29\n",
      "evaluation/Returns Min               5232.06\n",
      "evaluation/Estimation Bias Mean       317.57\n",
      "evaluation/Estimation Bias Std        146.831\n",
      "evaluation/EB/Q_True Mean              47.2993\n",
      "evaluation/EB/Q_True Std              145.673\n",
      "evaluation/EB/Q_Pred Mean             364.869\n",
      "evaluation/EB/Q_Pred Std               14.7456\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5237.92\n",
      "evaluation/Actions Mean                 0.0949274\n",
      "evaluation/Actions Std                  0.482408\n",
      "evaluation/Actions Max                  0.998308\n",
      "evaluation/Actions Min                 -0.993141\n",
      "time/backward_policy (s)                8.06092\n",
      "time/backward_zf1 (s)                   9.32931\n",
      "time/backward_zf2 (s)                   9.02936\n",
      "time/data sampling (s)                  1.70772\n",
      "time/data storing (s)                   0.0868013\n",
      "time/evaluation sampling (s)            4.50172\n",
      "time/exploration sampling (s)           2.53571\n",
      "time/logging (s)                        0.0118108\n",
      "time/preback_alpha (s)                  0.00518258\n",
      "time/preback_policy (s)                15.8455\n",
      "time/preback_start (s)                  0.919381\n",
      "time/preback_zf (s)                    36.3345\n",
      "time/saving (s)                         2.97e-06\n",
      "time/training (s)                      11.2699\n",
      "time/epoch (s)                         99.6379\n",
      "time/total (s)                       4721.21\n",
      "Epoch                                  47\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:38:20.068826 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 48 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 255000\n",
      "trainer/ZF1 Loss                       -0.602571\n",
      "trainer/ZF2 Loss                        2.4184\n",
      "trainer/ZF Expert Reward               32.0705\n",
      "trainer/ZF Policy Reward                4.31811\n",
      "trainer/ZF CHI2 Term                   29.0732\n",
      "trainer/Policy Loss                  -250.347\n",
      "trainer/expert_lambda Loss             13.5615\n",
      "trainer/expert_lambda Value            30.6538\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              103.796\n",
      "trainer/Policy Param Norm              39.448\n",
      "trainer/Zf1 Grad Norm                2056.65\n",
      "trainer/Zf1 Param Norm                136.012\n",
      "trainer/Zf2 Grad Norm                2079.61\n",
      "trainer/Zf2 Param Norm                135.373\n",
      "trainer/Z Expert Predictions Mean     416.64\n",
      "trainer/Z Expert Predictions Std       79.0252\n",
      "trainer/Z Expert Predictions Max      741.299\n",
      "trainer/Z Expert Predictions Min      332.543\n",
      "trainer/Z Policy Predictions Mean     246.146\n",
      "trainer/Z Policy Predictions Std      133.779\n",
      "trainer/Z Policy Predictions Max      547.893\n",
      "trainer/Z Policy Predictions Min       -1.00484\n",
      "trainer/Z Expert Targets Mean         384.57\n",
      "trainer/Z Expert Targets Std           79.9191\n",
      "trainer/Z Expert Targets Max          712.921\n",
      "trainer/Z Expert Targets Min          298.16\n",
      "trainer/Z Policy Targets Mean         241.828\n",
      "trainer/Z Policy Targets Std          134.113\n",
      "trainer/Z Policy Targets Max          573.423\n",
      "trainer/Z Policy Targets Min           -4.00575\n",
      "trainer/Log Pis Mean                   14.2783\n",
      "trainer/Log Pis Std                     8.72362\n",
      "trainer/Policy mu Mean                  0.133324\n",
      "trainer/Policy mu Std                   1.10128\n",
      "trainer/Policy log std Mean            -1.30995\n",
      "trainer/Policy log std Std              0.692668\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        253129\n",
      "exploration/num paths total          1737\n",
      "evaluation/num steps total         257479\n",
      "evaluation/num paths total            504\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.25057\n",
      "evaluation/Rewards Std                  0.0697477\n",
      "evaluation/Rewards Max                  5.39823\n",
      "evaluation/Rewards Min                  4.8237\n",
      "evaluation/Returns Mean              5250.57\n",
      "evaluation/Returns Std                  3.02491\n",
      "evaluation/Returns Max               5255.05\n",
      "evaluation/Returns Min               5245.68\n",
      "evaluation/Estimation Bias Mean       299.708\n",
      "evaluation/Estimation Bias Std        146.372\n",
      "evaluation/EB/Q_True Mean              47.3525\n",
      "evaluation/EB/Q_True Std              145.846\n",
      "evaluation/EB/Q_Pred Mean             347.061\n",
      "evaluation/EB/Q_Pred Std               13.4979\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5250.57\n",
      "evaluation/Actions Mean                 0.0928976\n",
      "evaluation/Actions Std                  0.460692\n",
      "evaluation/Actions Max                  0.998446\n",
      "evaluation/Actions Min                 -0.990188\n",
      "time/backward_policy (s)                8.65256\n",
      "time/backward_zf1 (s)                   9.93365\n",
      "time/backward_zf2 (s)                   9.72126\n",
      "time/data sampling (s)                  1.72461\n",
      "time/data storing (s)                   0.0858812\n",
      "time/evaluation sampling (s)            4.46432\n",
      "time/exploration sampling (s)           2.50747\n",
      "time/logging (s)                        0.0117781\n",
      "time/preback_alpha (s)                  0.00523282\n",
      "time/preback_policy (s)                14.9433\n",
      "time/preback_start (s)                  0.917445\n",
      "time/preback_zf (s)                    36.4572\n",
      "time/saving (s)                         2.961e-06\n",
      "time/training (s)                      10.7894\n",
      "time/epoch (s)                        100.214\n",
      "time/total (s)                       4821.42\n",
      "Epoch                                  48\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:39:59.654624 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 49 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 260000\n",
      "trainer/ZF1 Loss                       -6.10084\n",
      "trainer/ZF2 Loss                       -8.48508\n",
      "trainer/ZF Expert Reward               33.2091\n",
      "trainer/ZF Policy Reward                2.09846\n",
      "trainer/ZF CHI2 Term                   21.9323\n",
      "trainer/Policy Loss                  -238.334\n",
      "trainer/expert_lambda Loss             22.767\n",
      "trainer/expert_lambda Value            31.0884\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               72.4857\n",
      "trainer/Policy Param Norm              39.6609\n",
      "trainer/Zf1 Grad Norm                1121.71\n",
      "trainer/Zf1 Param Norm                137.398\n",
      "trainer/Zf2 Grad Norm                 691.016\n",
      "trainer/Zf2 Param Norm                136.795\n",
      "trainer/Z Expert Predictions Mean     419.963\n",
      "trainer/Z Expert Predictions Std       76.386\n",
      "trainer/Z Expert Predictions Max      763.059\n",
      "trainer/Z Expert Predictions Min      334.518\n",
      "trainer/Z Policy Predictions Mean     234.885\n",
      "trainer/Z Policy Predictions Std      121.678\n",
      "trainer/Z Policy Predictions Max      375.844\n",
      "trainer/Z Policy Predictions Min       -0.662373\n",
      "trainer/Z Expert Targets Mean         386.754\n",
      "trainer/Z Expert Targets Std           76.0196\n",
      "trainer/Z Expert Targets Max          731.106\n",
      "trainer/Z Expert Targets Min          300.467\n",
      "trainer/Z Policy Targets Mean         232.787\n",
      "trainer/Z Policy Targets Std          121.908\n",
      "trainer/Z Policy Targets Max          376.143\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   12.1064\n",
      "trainer/Log Pis Std                     8.19493\n",
      "trainer/Policy mu Mean                  0.118634\n",
      "trainer/Policy mu Std                   1.08838\n",
      "trainer/Policy log std Mean            -1.21136\n",
      "trainer/Policy log std Std              0.624541\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        254592\n",
      "exploration/num paths total          1739\n",
      "evaluation/num steps total         267479\n",
      "evaluation/num paths total            514\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.20795\n",
      "evaluation/Rewards Std                  0.0707121\n",
      "evaluation/Rewards Max                  5.47402\n",
      "evaluation/Rewards Min                  4.83187\n",
      "evaluation/Returns Mean              5207.95\n",
      "evaluation/Returns Std                 24.4148\n",
      "evaluation/Returns Max               5252.75\n",
      "evaluation/Returns Min               5168.44\n",
      "evaluation/Estimation Bias Mean       253.442\n",
      "evaluation/Estimation Bias Std        149.269\n",
      "evaluation/EB/Q_True Mean              47.0756\n",
      "evaluation/EB/Q_True Std              144.99\n",
      "evaluation/EB/Q_Pred Mean             300.518\n",
      "evaluation/EB/Q_Pred Std               32.8208\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5207.95\n",
      "evaluation/Actions Mean                 0.068256\n",
      "evaluation/Actions Std                  0.496732\n",
      "evaluation/Actions Max                  0.999275\n",
      "evaluation/Actions Min                 -0.997366\n",
      "time/backward_policy (s)                7.9015\n",
      "time/backward_zf1 (s)                   9.1649\n",
      "time/backward_zf2 (s)                   8.84828\n",
      "time/data sampling (s)                  1.68666\n",
      "time/data storing (s)                   0.0867507\n",
      "time/evaluation sampling (s)            4.51991\n",
      "time/exploration sampling (s)           2.43773\n",
      "time/logging (s)                        0.0115361\n",
      "time/preback_alpha (s)                  0.00511365\n",
      "time/preback_policy (s)                16.095\n",
      "time/preback_start (s)                  0.912145\n",
      "time/preback_zf (s)                    36.2631\n",
      "time/saving (s)                         2.848e-06\n",
      "time/training (s)                      11.4486\n",
      "time/epoch (s)                         99.3813\n",
      "time/total (s)                       4920.81\n",
      "Epoch                                  49\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:41:40.035274 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 50 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 265000\n",
      "trainer/ZF1 Loss                       -4.1778\n",
      "trainer/ZF2 Loss                        1.30716\n",
      "trainer/ZF Expert Reward               32.2455\n",
      "trainer/ZF Policy Reward                1.8399\n",
      "trainer/ZF CHI2 Term                   26.9926\n",
      "trainer/Policy Loss                  -244.267\n",
      "trainer/expert_lambda Loss             21.4447\n",
      "trainer/expert_lambda Value            31.517\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              149.12\n",
      "trainer/Policy Param Norm              39.8352\n",
      "trainer/Zf1 Grad Norm                1453.79\n",
      "trainer/Zf1 Param Norm                138.908\n",
      "trainer/Zf2 Grad Norm                1595.19\n",
      "trainer/Zf2 Param Norm                138.298\n",
      "trainer/Z Expert Predictions Mean     409.429\n",
      "trainer/Z Expert Predictions Std       84.6073\n",
      "trainer/Z Expert Predictions Max      724.626\n",
      "trainer/Z Expert Predictions Min      313.167\n",
      "trainer/Z Policy Predictions Mean     241.176\n",
      "trainer/Z Policy Predictions Std      127.996\n",
      "trainer/Z Policy Predictions Max      561.327\n",
      "trainer/Z Policy Predictions Min       -8.18678\n",
      "trainer/Z Expert Targets Mean         377.183\n",
      "trainer/Z Expert Targets Std           85.2001\n",
      "trainer/Z Expert Targets Max          694.617\n",
      "trainer/Z Expert Targets Min          281.202\n",
      "trainer/Z Policy Targets Mean         239.337\n",
      "trainer/Z Policy Targets Std          128.399\n",
      "trainer/Z Policy Targets Max          556.196\n",
      "trainer/Z Policy Targets Min           -9.99206\n",
      "trainer/Log Pis Mean                   15.3133\n",
      "trainer/Log Pis Std                     7.76443\n",
      "trainer/Policy mu Mean                  0.152491\n",
      "trainer/Policy mu Std                   1.17589\n",
      "trainer/Policy log std Mean            -1.30078\n",
      "trainer/Policy log std Std              0.707321\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        262912\n",
      "exploration/num paths total          1748\n",
      "evaluation/num steps total         277479\n",
      "evaluation/num paths total            524\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.27957\n",
      "evaluation/Rewards Std                  0.0614463\n",
      "evaluation/Rewards Max                  5.40995\n",
      "evaluation/Rewards Min                  4.82762\n",
      "evaluation/Returns Mean              5279.57\n",
      "evaluation/Returns Std                  3.98068\n",
      "evaluation/Returns Max               5283.86\n",
      "evaluation/Returns Min               5270.97\n",
      "evaluation/Estimation Bias Mean       296.551\n",
      "evaluation/Estimation Bias Std        147.144\n",
      "evaluation/EB/Q_True Mean              47.6581\n",
      "evaluation/EB/Q_True Std              146.783\n",
      "evaluation/EB/Q_Pred Mean             344.21\n",
      "evaluation/EB/Q_Pred Std                9.59965\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5279.57\n",
      "evaluation/Actions Mean                 0.0907679\n",
      "evaluation/Actions Std                  0.460223\n",
      "evaluation/Actions Max                  0.996844\n",
      "evaluation/Actions Min                 -0.996251\n",
      "time/backward_policy (s)                8.33128\n",
      "time/backward_zf1 (s)                   9.62134\n",
      "time/backward_zf2 (s)                   9.34359\n",
      "time/data sampling (s)                  1.72477\n",
      "time/data storing (s)                   0.0868177\n",
      "time/evaluation sampling (s)            4.68086\n",
      "time/exploration sampling (s)           2.51982\n",
      "time/logging (s)                        0.0120922\n",
      "time/preback_alpha (s)                  0.00518605\n",
      "time/preback_policy (s)                15.4638\n",
      "time/preback_start (s)                  0.913015\n",
      "time/preback_zf (s)                    36.3473\n",
      "time/saving (s)                         3.03e-06\n",
      "time/training (s)                      11.1281\n",
      "time/epoch (s)                        100.178\n",
      "time/total (s)                       5020.99\n",
      "Epoch                                  50\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:43:26.787425 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 51 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 270000\n",
      "trainer/ZF1 Loss                       -7.83708\n",
      "trainer/ZF2 Loss                       -2.68373\n",
      "trainer/ZF Expert Reward               36.3026\n",
      "trainer/ZF Policy Reward                3.75062\n",
      "trainer/ZF CHI2 Term                   27.5615\n",
      "trainer/Policy Loss                  -234.031\n",
      "trainer/expert_lambda Loss             27.7988\n",
      "trainer/expert_lambda Value            31.9558\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               51.1927\n",
      "trainer/Policy Param Norm              40.079\n",
      "trainer/Zf1 Grad Norm                 933.233\n",
      "trainer/Zf1 Param Norm                140.042\n",
      "trainer/Zf2 Grad Norm                1548.55\n",
      "trainer/Zf2 Param Norm                139.347\n",
      "trainer/Z Expert Predictions Mean     374.173\n",
      "trainer/Z Expert Predictions Std       43.4131\n",
      "trainer/Z Expert Predictions Max      486.151\n",
      "trainer/Z Expert Predictions Min      294.269\n",
      "trainer/Z Policy Predictions Mean     231.139\n",
      "trainer/Z Policy Predictions Std      110.796\n",
      "trainer/Z Policy Predictions Max      357.51\n",
      "trainer/Z Policy Predictions Min       -2.454\n",
      "trainer/Z Expert Targets Mean         337.87\n",
      "trainer/Z Expert Targets Std           44.2005\n",
      "trainer/Z Expert Targets Max          455.583\n",
      "trainer/Z Expert Targets Min          257.697\n",
      "trainer/Z Policy Targets Mean         227.389\n",
      "trainer/Z Policy Targets Std          110.533\n",
      "trainer/Z Policy Targets Max          353.314\n",
      "trainer/Z Policy Targets Min           -5.97728\n",
      "trainer/Log Pis Mean                   12.965\n",
      "trainer/Log Pis Std                     6.86002\n",
      "trainer/Policy mu Mean                  0.108084\n",
      "trainer/Policy mu Std                   1.12315\n",
      "trainer/Policy log std Mean            -1.22385\n",
      "trainer/Policy log std Std              0.574677\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        264130\n",
      "exploration/num paths total          1751\n",
      "evaluation/num steps total         287479\n",
      "evaluation/num paths total            534\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.28612\n",
      "evaluation/Rewards Std                  0.0634709\n",
      "evaluation/Rewards Max                  5.46641\n",
      "evaluation/Rewards Min                  4.86529\n",
      "evaluation/Returns Mean              5286.12\n",
      "evaluation/Returns Std                  3.03217\n",
      "evaluation/Returns Max               5290.42\n",
      "evaluation/Returns Min               5279.7\n",
      "evaluation/Estimation Bias Mean       253.715\n",
      "evaluation/Estimation Bias Std        147.468\n",
      "evaluation/EB/Q_True Mean              47.7159\n",
      "evaluation/EB/Q_True Std              146.956\n",
      "evaluation/EB/Q_Pred Mean             301.431\n",
      "evaluation/EB/Q_Pred Std                9.27674\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5286.12\n",
      "evaluation/Actions Mean                 0.093155\n",
      "evaluation/Actions Std                  0.527029\n",
      "evaluation/Actions Max                  0.998049\n",
      "evaluation/Actions Min                 -0.990919\n",
      "time/backward_policy (s)                9.14662\n",
      "time/backward_zf1 (s)                  10.864\n",
      "time/backward_zf2 (s)                  10.461\n",
      "time/data sampling (s)                  1.94333\n",
      "time/data storing (s)                   0.0993794\n",
      "time/evaluation sampling (s)            4.73361\n",
      "time/exploration sampling (s)           2.75684\n",
      "time/logging (s)                        0.0132309\n",
      "time/preback_alpha (s)                  0.00559637\n",
      "time/preback_policy (s)                15.6609\n",
      "time/preback_start (s)                  1.01701\n",
      "time/preback_zf (s)                    38.2144\n",
      "time/saving (s)                         3.047e-06\n",
      "time/training (s)                      11.6082\n",
      "time/epoch (s)                        106.524\n",
      "time/total (s)                       5127.52\n",
      "Epoch                                  51\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:45:10.299540 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 52 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 275000\n",
      "trainer/ZF1 Loss                       -5.28326\n",
      "trainer/ZF2 Loss                       -3.70701\n",
      "trainer/ZF Expert Reward               35.9868\n",
      "trainer/ZF Policy Reward                3.82409\n",
      "trainer/ZF CHI2 Term                   27.9205\n",
      "trainer/Policy Loss                  -218.142\n",
      "trainer/expert_lambda Loss             26.0107\n",
      "trainer/expert_lambda Value            32.397\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               46.8971\n",
      "trainer/Policy Param Norm              40.3263\n",
      "trainer/Zf1 Grad Norm                 989.677\n",
      "trainer/Zf1 Param Norm                141.331\n",
      "trainer/Zf2 Grad Norm                1472.22\n",
      "trainer/Zf2 Param Norm                140.567\n",
      "trainer/Z Expert Predictions Mean     382.002\n",
      "trainer/Z Expert Predictions Std       70.2509\n",
      "trainer/Z Expert Predictions Max      551.361\n",
      "trainer/Z Expert Predictions Min      293.315\n",
      "trainer/Z Policy Predictions Mean     216.206\n",
      "trainer/Z Policy Predictions Std      112.694\n",
      "trainer/Z Policy Predictions Max      343.283\n",
      "trainer/Z Policy Predictions Min       -4.54521\n",
      "trainer/Z Expert Targets Mean         346.015\n",
      "trainer/Z Expert Targets Std           69.384\n",
      "trainer/Z Expert Targets Max          512.909\n",
      "trainer/Z Expert Targets Min          257.029\n",
      "trainer/Z Policy Targets Mean         212.382\n",
      "trainer/Z Policy Targets Std          112.308\n",
      "trainer/Z Policy Targets Max          343.474\n",
      "trainer/Z Policy Targets Min           -4.71968\n",
      "trainer/Log Pis Mean                   12.5917\n",
      "trainer/Log Pis Std                     7.10968\n",
      "trainer/Policy mu Mean                  0.17613\n",
      "trainer/Policy mu Std                   1.09105\n",
      "trainer/Policy log std Mean            -1.2089\n",
      "trainer/Policy log std Std              0.60924\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        271130\n",
      "exploration/num paths total          1758\n",
      "evaluation/num steps total         296562\n",
      "evaluation/num paths total            544\n",
      "evaluation/path length Mean           908.3\n",
      "evaluation/path length Std            275.1\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             83\n",
      "evaluation/Rewards Mean                 5.15877\n",
      "evaluation/Rewards Std                  0.0778133\n",
      "evaluation/Rewards Max                  5.30639\n",
      "evaluation/Rewards Min                  3.69593\n",
      "evaluation/Returns Mean              4685.71\n",
      "evaluation/Returns Std               1433.49\n",
      "evaluation/Returns Max               5166.6\n",
      "evaluation/Returns Min                385.256\n",
      "evaluation/Estimation Bias Mean       243.356\n",
      "evaluation/Estimation Bias Std        155.443\n",
      "evaluation/EB/Q_True Mean              51.3043\n",
      "evaluation/EB/Q_True Std              149.809\n",
      "evaluation/EB/Q_Pred Mean             294.66\n",
      "evaluation/EB/Q_Pred Std               18.6374\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4685.71\n",
      "evaluation/Actions Mean                 0.108732\n",
      "evaluation/Actions Std                  0.42559\n",
      "evaluation/Actions Max                  0.998737\n",
      "evaluation/Actions Min                 -0.99929\n",
      "time/backward_policy (s)                8.88019\n",
      "time/backward_zf1 (s)                  10.3268\n",
      "time/backward_zf2 (s)                   9.99908\n",
      "time/data sampling (s)                  1.85602\n",
      "time/data storing (s)                   0.0920381\n",
      "time/evaluation sampling (s)            5.16863\n",
      "time/exploration sampling (s)           2.69741\n",
      "time/logging (s)                        0.0122604\n",
      "time/preback_alpha (s)                  0.00533541\n",
      "time/preback_policy (s)                15.1861\n",
      "time/preback_start (s)                  0.955708\n",
      "time/preback_zf (s)                    36.8641\n",
      "time/saving (s)                         3.551e-06\n",
      "time/training (s)                      11.2541\n",
      "time/epoch (s)                        103.298\n",
      "time/total (s)                       5230.82\n",
      "Epoch                                  52\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:46:49.903815 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 53 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 280000\n",
      "trainer/ZF1 Loss                      -10.9155\n",
      "trainer/ZF2 Loss                      -13.8427\n",
      "trainer/ZF Expert Reward               35.8385\n",
      "trainer/ZF Policy Reward                3.54111\n",
      "trainer/ZF CHI2 Term                   20.2394\n",
      "trainer/Policy Loss                  -216.088\n",
      "trainer/expert_lambda Loss             23.0485\n",
      "trainer/expert_lambda Value            32.8409\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               45.9061\n",
      "trainer/Policy Param Norm              40.5197\n",
      "trainer/Zf1 Grad Norm                1119.21\n",
      "trainer/Zf1 Param Norm                142.625\n",
      "trainer/Zf2 Grad Norm                 607.835\n",
      "trainer/Zf2 Param Norm                141.827\n",
      "trainer/Z Expert Predictions Mean     348.518\n",
      "trainer/Z Expert Predictions Std       59.1472\n",
      "trainer/Z Expert Predictions Max      562.455\n",
      "trainer/Z Expert Predictions Min      293.489\n",
      "trainer/Z Policy Predictions Mean     213.781\n",
      "trainer/Z Policy Predictions Std      100.033\n",
      "trainer/Z Policy Predictions Max      331.158\n",
      "trainer/Z Policy Predictions Min        3.4882\n",
      "trainer/Z Expert Targets Mean         312.679\n",
      "trainer/Z Expert Targets Std           58.5097\n",
      "trainer/Z Expert Targets Max          523.379\n",
      "trainer/Z Expert Targets Min          254.819\n",
      "trainer/Z Policy Targets Mean         210.24\n",
      "trainer/Z Policy Targets Std          100.183\n",
      "trainer/Z Policy Targets Max          334.236\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   11.0656\n",
      "trainer/Log Pis Std                     7.50873\n",
      "trainer/Policy mu Mean                  0.0569667\n",
      "trainer/Policy mu Std                   1.08693\n",
      "trainer/Policy log std Mean            -1.15223\n",
      "trainer/Policy log std Std              0.497267\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        276843\n",
      "exploration/num paths total          1767\n",
      "evaluation/num steps total         299496\n",
      "evaluation/num paths total            554\n",
      "evaluation/path length Mean           293.4\n",
      "evaluation/path length Std            118.701\n",
      "evaluation/path length Max            456\n",
      "evaluation/path length Min            153\n",
      "evaluation/Rewards Mean                 5.01362\n",
      "evaluation/Rewards Std                  0.287329\n",
      "evaluation/Rewards Max                  5.59259\n",
      "evaluation/Rewards Min                  3.80782\n",
      "evaluation/Returns Mean              1471\n",
      "evaluation/Returns Std                602.237\n",
      "evaluation/Returns Max               2314.22\n",
      "evaluation/Returns Min                751.684\n",
      "evaluation/Estimation Bias Mean       153.294\n",
      "evaluation/Estimation Bias Std        184.645\n",
      "evaluation/EB/Q_True Mean              61.7653\n",
      "evaluation/EB/Q_True Std              153.463\n",
      "evaluation/EB/Q_Pred Mean             215.06\n",
      "evaluation/EB/Q_Pred Std               93.9638\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           1471\n",
      "evaluation/Actions Mean                 0.0866967\n",
      "evaluation/Actions Std                  0.487326\n",
      "evaluation/Actions Max                  0.999774\n",
      "evaluation/Actions Min                 -0.999101\n",
      "time/backward_policy (s)                8.49168\n",
      "time/backward_zf1 (s)                   9.90122\n",
      "time/backward_zf2 (s)                   9.57967\n",
      "time/data sampling (s)                  1.92226\n",
      "time/data storing (s)                   0.0880688\n",
      "time/evaluation sampling (s)            2.22114\n",
      "time/exploration sampling (s)           2.60691\n",
      "time/logging (s)                        0.006576\n",
      "time/preback_alpha (s)                  0.00529468\n",
      "time/preback_policy (s)                15.5497\n",
      "time/preback_start (s)                  0.93825\n",
      "time/preback_zf (s)                    36.7553\n",
      "time/saving (s)                         5.375e-06\n",
      "time/training (s)                      11.32\n",
      "time/epoch (s)                         99.3861\n",
      "time/total (s)                       5330.21\n",
      "Epoch                                  53\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:48:30.185043 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 54 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 285000\n",
      "trainer/ZF1 Loss                      -11.3991\n",
      "trainer/ZF2 Loss                       -6.11209\n",
      "trainer/ZF Expert Reward               36.652\n",
      "trainer/ZF Policy Reward                3.43315\n",
      "trainer/ZF CHI2 Term                   24.8545\n",
      "trainer/Policy Loss                  -208.118\n",
      "trainer/expert_lambda Loss             28.8234\n",
      "trainer/expert_lambda Value            33.2841\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               35.6968\n",
      "trainer/Policy Param Norm              40.6659\n",
      "trainer/Zf1 Grad Norm                 749.082\n",
      "trainer/Zf1 Param Norm                143.908\n",
      "trainer/Zf2 Grad Norm                 898.678\n",
      "trainer/Zf2 Param Norm                143.16\n",
      "trainer/Z Expert Predictions Mean     330.629\n",
      "trainer/Z Expert Predictions Std       43.3903\n",
      "trainer/Z Expert Predictions Max      441.998\n",
      "trainer/Z Expert Predictions Min      282.154\n",
      "trainer/Z Policy Predictions Mean     207.439\n",
      "trainer/Z Policy Predictions Std       94.1366\n",
      "trainer/Z Policy Predictions Max      310.945\n",
      "trainer/Z Policy Predictions Min        6.43813\n",
      "trainer/Z Expert Targets Mean         293.977\n",
      "trainer/Z Expert Targets Std           42.5724\n",
      "trainer/Z Expert Targets Max          407.194\n",
      "trainer/Z Expert Targets Min          242.823\n",
      "trainer/Z Policy Targets Mean         204.006\n",
      "trainer/Z Policy Targets Std           94.3852\n",
      "trainer/Z Policy Targets Max          294.868\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                    9.75402\n",
      "trainer/Log Pis Std                     7.07402\n",
      "trainer/Policy mu Mean                  0.123462\n",
      "trainer/Policy mu Std                   1.04194\n",
      "trainer/Policy log std Mean            -1.09443\n",
      "trainer/Policy log std Std              0.488374\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        280129\n",
      "exploration/num paths total          1771\n",
      "evaluation/num steps total         309496\n",
      "evaluation/num paths total            564\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.28995\n",
      "evaluation/Rewards Std                  0.0706944\n",
      "evaluation/Rewards Max                  5.48607\n",
      "evaluation/Rewards Min                  4.84489\n",
      "evaluation/Returns Mean              5289.95\n",
      "evaluation/Returns Std                  2.76147\n",
      "evaluation/Returns Max               5293.93\n",
      "evaluation/Returns Min               5285.09\n",
      "evaluation/Estimation Bias Mean       204.124\n",
      "evaluation/Estimation Bias Std        147.201\n",
      "evaluation/EB/Q_True Mean              47.7794\n",
      "evaluation/EB/Q_True Std              147.145\n",
      "evaluation/EB/Q_Pred Mean             251.904\n",
      "evaluation/EB/Q_Pred Std               12.9138\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5289.95\n",
      "evaluation/Actions Mean                 0.0889846\n",
      "evaluation/Actions Std                  0.514311\n",
      "evaluation/Actions Max                  0.994566\n",
      "evaluation/Actions Min                 -0.9931\n",
      "time/backward_policy (s)                8.12504\n",
      "time/backward_zf1 (s)                   9.45457\n",
      "time/backward_zf2 (s)                   9.11708\n",
      "time/data sampling (s)                  1.75293\n",
      "time/data storing (s)                   0.0880085\n",
      "time/evaluation sampling (s)            4.59414\n",
      "time/exploration sampling (s)           2.51315\n",
      "time/logging (s)                        0.0124602\n",
      "time/preback_alpha (s)                  0.00513197\n",
      "time/preback_policy (s)                15.8022\n",
      "time/preback_start (s)                  0.920917\n",
      "time/preback_zf (s)                    36.3568\n",
      "time/saving (s)                         3e-06\n",
      "time/training (s)                      11.3419\n",
      "time/epoch (s)                        100.084\n",
      "time/total (s)                       5430.29\n",
      "Epoch                                  54\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:50:09.981674 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 55 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 290000\n",
      "trainer/ZF1 Loss                       -4.20735\n",
      "trainer/ZF2 Loss                       -2.28736\n",
      "trainer/ZF Expert Reward               33.6595\n",
      "trainer/ZF Policy Reward                1.61065\n",
      "trainer/ZF CHI2 Term                   26.7889\n",
      "trainer/Policy Loss                  -206.897\n",
      "trainer/expert_lambda Loss             22.5312\n",
      "trainer/expert_lambda Value            33.7241\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               96.9842\n",
      "trainer/Policy Param Norm              40.8367\n",
      "trainer/Zf1 Grad Norm                1031.72\n",
      "trainer/Zf1 Param Norm                145.174\n",
      "trainer/Zf2 Grad Norm                2394.92\n",
      "trainer/Zf2 Param Norm                144.507\n",
      "trainer/Z Expert Predictions Mean     383.069\n",
      "trainer/Z Expert Predictions Std       76.3538\n",
      "trainer/Z Expert Predictions Max      560.701\n",
      "trainer/Z Expert Predictions Min      266.226\n",
      "trainer/Z Policy Predictions Mean     205.022\n",
      "trainer/Z Policy Predictions Std       90.8234\n",
      "trainer/Z Policy Predictions Max      317.855\n",
      "trainer/Z Policy Predictions Min       -0.286507\n",
      "trainer/Z Expert Targets Mean         349.41\n",
      "trainer/Z Expert Targets Std           75.8805\n",
      "trainer/Z Expert Targets Max          524.77\n",
      "trainer/Z Expert Targets Min          225.377\n",
      "trainer/Z Policy Targets Mean         203.411\n",
      "trainer/Z Policy Targets Std           91.5056\n",
      "trainer/Z Policy Targets Max          358.643\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   15.6398\n",
      "trainer/Log Pis Std                     6.46743\n",
      "trainer/Policy mu Mean                  0.199003\n",
      "trainer/Policy mu Std                   1.01503\n",
      "trainer/Policy log std Mean            -1.4899\n",
      "trainer/Policy log std Std              0.701206\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        286129\n",
      "exploration/num paths total          1777\n",
      "evaluation/num steps total         317829\n",
      "evaluation/num paths total            574\n",
      "evaluation/path length Mean           833.3\n",
      "evaluation/path length Std            334.164\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            116\n",
      "evaluation/Rewards Mean                 5.39532\n",
      "evaluation/Rewards Std                  0.13055\n",
      "evaluation/Rewards Max                  6.17077\n",
      "evaluation/Rewards Min                  4.85853\n",
      "evaluation/Returns Mean              4495.92\n",
      "evaluation/Returns Std               1815.47\n",
      "evaluation/Returns Max               5420.42\n",
      "evaluation/Returns Min                615.616\n",
      "evaluation/Estimation Bias Mean       268.119\n",
      "evaluation/Estimation Bias Std        208.407\n",
      "evaluation/EB/Q_True Mean              58.7236\n",
      "evaluation/EB/Q_True Std              163.316\n",
      "evaluation/EB/Q_Pred Mean             326.843\n",
      "evaluation/EB/Q_Pred Std               71.688\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4495.92\n",
      "evaluation/Actions Mean                 0.035236\n",
      "evaluation/Actions Std                  0.521897\n",
      "evaluation/Actions Max                  0.999394\n",
      "evaluation/Actions Min                 -0.999394\n",
      "time/backward_policy (s)                8.08258\n",
      "time/backward_zf1 (s)                   9.36209\n",
      "time/backward_zf2 (s)                   9.05348\n",
      "time/data sampling (s)                  1.78616\n",
      "time/data storing (s)                   0.0884904\n",
      "time/evaluation sampling (s)            4.45056\n",
      "time/exploration sampling (s)           2.50683\n",
      "time/logging (s)                        0.0103663\n",
      "time/preback_alpha (s)                  0.00511001\n",
      "time/preback_policy (s)                15.806\n",
      "time/preback_start (s)                  0.909956\n",
      "time/preback_zf (s)                    36.2612\n",
      "time/saving (s)                         2.762e-06\n",
      "time/training (s)                      11.2693\n",
      "time/epoch (s)                         99.5921\n",
      "time/total (s)                       5529.89\n",
      "Epoch                                  55\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:51:51.713869 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 56 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 295000\n",
      "trainer/ZF1 Loss                        9.7434\n",
      "trainer/ZF2 Loss                       15.1135\n",
      "trainer/ZF Expert Reward               36.7229\n",
      "trainer/ZF Policy Reward                2.83436\n",
      "trainer/ZF CHI2 Term                   45.2424\n",
      "trainer/Policy Loss                  -260.026\n",
      "trainer/expert_lambda Loss             19.3767\n",
      "trainer/expert_lambda Value            34.1401\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              131.75\n",
      "trainer/Policy Param Norm              41.0584\n",
      "trainer/Zf1 Grad Norm                3491.07\n",
      "trainer/Zf1 Param Norm                146.679\n",
      "trainer/Zf2 Grad Norm                5576.11\n",
      "trainer/Zf2 Param Norm                146.016\n",
      "trainer/Z Expert Predictions Mean     419.239\n",
      "trainer/Z Expert Predictions Std      135.977\n",
      "trainer/Z Expert Predictions Max      707.706\n",
      "trainer/Z Expert Predictions Min      263.99\n",
      "trainer/Z Policy Predictions Mean     256.048\n",
      "trainer/Z Policy Predictions Std      134.163\n",
      "trainer/Z Policy Predictions Max      582.123\n",
      "trainer/Z Policy Predictions Min        1.67893\n",
      "trainer/Z Expert Targets Mean         382.516\n",
      "trainer/Z Expert Targets Std          136.788\n",
      "trainer/Z Expert Targets Max          676.473\n",
      "trainer/Z Expert Targets Min          227.356\n",
      "trainer/Z Policy Targets Mean         253.214\n",
      "trainer/Z Policy Targets Std          134.164\n",
      "trainer/Z Policy Targets Max          581.827\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   16.9104\n",
      "trainer/Log Pis Std                     7.73911\n",
      "trainer/Policy mu Mean                  0.201469\n",
      "trainer/Policy mu Std                   1.1131\n",
      "trainer/Policy log std Mean            -1.46187\n",
      "trainer/Policy log std Std              0.736289\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        290129\n",
      "exploration/num paths total          1781\n",
      "evaluation/num steps total         327829\n",
      "evaluation/num paths total            584\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.25314\n",
      "evaluation/Rewards Std                  0.0463022\n",
      "evaluation/Rewards Max                  5.36773\n",
      "evaluation/Rewards Min                  4.8863\n",
      "evaluation/Returns Mean              5253.14\n",
      "evaluation/Returns Std                  2.99937\n",
      "evaluation/Returns Max               5258.01\n",
      "evaluation/Returns Min               5248.26\n",
      "evaluation/Estimation Bias Mean       413.378\n",
      "evaluation/Estimation Bias Std        148.747\n",
      "evaluation/EB/Q_True Mean              47.3709\n",
      "evaluation/EB/Q_True Std              145.904\n",
      "evaluation/EB/Q_Pred Mean             460.749\n",
      "evaluation/EB/Q_Pred Std               29.2827\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5253.14\n",
      "evaluation/Actions Mean                 0.0879069\n",
      "evaluation/Actions Std                  0.479699\n",
      "evaluation/Actions Max                  0.997485\n",
      "evaluation/Actions Min                 -0.999159\n",
      "time/backward_policy (s)                8.92792\n",
      "time/backward_zf1 (s)                  10.2186\n",
      "time/backward_zf2 (s)                  10.006\n",
      "time/data sampling (s)                  1.87995\n",
      "time/data storing (s)                   0.0869661\n",
      "time/evaluation sampling (s)            4.8448\n",
      "time/exploration sampling (s)           2.53095\n",
      "time/logging (s)                        0.0127088\n",
      "time/preback_alpha (s)                  0.00528209\n",
      "time/preback_policy (s)                14.7166\n",
      "time/preback_start (s)                  0.926081\n",
      "time/preback_zf (s)                    36.5012\n",
      "time/saving (s)                         3.131e-06\n",
      "time/training (s)                      10.869\n",
      "time/epoch (s)                        101.526\n",
      "time/total (s)                       5631.42\n",
      "Epoch                                  56\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:53:31.030936 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 57 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 300000\n",
      "trainer/ZF1 Loss                      -10.5939\n",
      "trainer/ZF2 Loss                      -14.0785\n",
      "trainer/ZF Expert Reward               36.0098\n",
      "trainer/ZF Policy Reward                3.58395\n",
      "trainer/ZF CHI2 Term                   19.3228\n",
      "trainer/Policy Loss                  -309.923\n",
      "trainer/expert_lambda Loss             13.057\n",
      "trainer/expert_lambda Value            34.5618\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               74.8106\n",
      "trainer/Policy Param Norm              41.2378\n",
      "trainer/Zf1 Grad Norm                1370.94\n",
      "trainer/Zf1 Param Norm                148.367\n",
      "trainer/Zf2 Grad Norm                 875.068\n",
      "trainer/Zf2 Param Norm                147.645\n",
      "trainer/Z Expert Predictions Mean     457.942\n",
      "trainer/Z Expert Predictions Std       98.1411\n",
      "trainer/Z Expert Predictions Max      706.833\n",
      "trainer/Z Expert Predictions Min      296.305\n",
      "trainer/Z Policy Predictions Mean     307.837\n",
      "trainer/Z Policy Predictions Std      154.728\n",
      "trainer/Z Policy Predictions Max      484.708\n",
      "trainer/Z Policy Predictions Min       -9.23375\n",
      "trainer/Z Expert Targets Mean         421.932\n",
      "trainer/Z Expert Targets Std           98.3873\n",
      "trainer/Z Expert Targets Max          675.618\n",
      "trainer/Z Expert Targets Min          257.007\n",
      "trainer/Z Policy Targets Mean         304.253\n",
      "trainer/Z Policy Targets Std          154.293\n",
      "trainer/Z Policy Targets Max          482.858\n",
      "trainer/Z Policy Targets Min          -10.7619\n",
      "trainer/Log Pis Mean                   14.2568\n",
      "trainer/Log Pis Std                     7.37635\n",
      "trainer/Policy mu Mean                  0.2884\n",
      "trainer/Policy mu Std                   1.0596\n",
      "trainer/Policy log std Mean            -1.33867\n",
      "trainer/Policy log std Std              0.623044\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        296129\n",
      "exploration/num paths total          1787\n",
      "evaluation/num steps total         337829\n",
      "evaluation/num paths total            594\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.31376\n",
      "evaluation/Rewards Std                  0.0638262\n",
      "evaluation/Rewards Max                  5.44113\n",
      "evaluation/Rewards Min                  4.85239\n",
      "evaluation/Returns Mean              5313.76\n",
      "evaluation/Returns Std                  2.32248\n",
      "evaluation/Returns Max               5316.83\n",
      "evaluation/Returns Min               5309.63\n",
      "evaluation/Estimation Bias Mean       381.009\n",
      "evaluation/Estimation Bias Std        148.77\n",
      "evaluation/EB/Q_True Mean              47.9739\n",
      "evaluation/EB/Q_True Std              147.747\n",
      "evaluation/EB/Q_Pred Mean             428.982\n",
      "evaluation/EB/Q_Pred Std               17.2783\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5313.76\n",
      "evaluation/Actions Mean                 0.0879486\n",
      "evaluation/Actions Std                  0.494903\n",
      "evaluation/Actions Max                  0.996475\n",
      "evaluation/Actions Min                 -0.999459\n",
      "time/backward_policy (s)                7.91635\n",
      "time/backward_zf1 (s)                   9.20024\n",
      "time/backward_zf2 (s)                   8.88452\n",
      "time/data sampling (s)                  1.78782\n",
      "time/data storing (s)                   0.0877471\n",
      "time/evaluation sampling (s)            4.20315\n",
      "time/exploration sampling (s)           2.46539\n",
      "time/logging (s)                        0.0120778\n",
      "time/preback_alpha (s)                  0.00511111\n",
      "time/preback_policy (s)                16.0113\n",
      "time/preback_start (s)                  0.915924\n",
      "time/preback_zf (s)                    36.2163\n",
      "time/saving (s)                         2.839e-06\n",
      "time/training (s)                      11.4074\n",
      "time/epoch (s)                         99.1133\n",
      "time/total (s)                       5730.54\n",
      "Epoch                                  57\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:55:11.426707 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 58 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 305000\n",
      "trainer/ZF1 Loss                       -8.7604\n",
      "trainer/ZF2 Loss                       -5.69324\n",
      "trainer/ZF Expert Reward               36.9137\n",
      "trainer/ZF Policy Reward                2.98589\n",
      "trainer/ZF CHI2 Term                   25.0879\n",
      "trainer/Policy Loss                  -293.852\n",
      "trainer/expert_lambda Loss             23.4809\n",
      "trainer/expert_lambda Value            35.0004\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               49.4808\n",
      "trainer/Policy Param Norm              41.4704\n",
      "trainer/Zf1 Grad Norm                1174.53\n",
      "trainer/Zf1 Param Norm                149.865\n",
      "trainer/Zf2 Grad Norm                1045.46\n",
      "trainer/Zf2 Param Norm                149.078\n",
      "trainer/Z Expert Predictions Mean     487.088\n",
      "trainer/Z Expert Predictions Std       70.9659\n",
      "trainer/Z Expert Predictions Max      665.503\n",
      "trainer/Z Expert Predictions Min      325.746\n",
      "trainer/Z Policy Predictions Mean     291.892\n",
      "trainer/Z Policy Predictions Std      158.469\n",
      "trainer/Z Policy Predictions Max      461.174\n",
      "trainer/Z Policy Predictions Min       -6.18655\n",
      "trainer/Z Expert Targets Mean         450.174\n",
      "trainer/Z Expert Targets Std           71.4867\n",
      "trainer/Z Expert Targets Max          641.148\n",
      "trainer/Z Expert Targets Min          289.187\n",
      "trainer/Z Policy Targets Mean         288.906\n",
      "trainer/Z Policy Targets Std          158.604\n",
      "trainer/Z Policy Targets Max          450.353\n",
      "trainer/Z Policy Targets Min           -6.52014\n",
      "trainer/Log Pis Mean                   11.3934\n",
      "trainer/Log Pis Std                     6.67559\n",
      "trainer/Policy mu Mean                  0.318276\n",
      "trainer/Policy mu Std                   1.05316\n",
      "trainer/Policy log std Mean            -1.15182\n",
      "trainer/Policy log std Std              0.547497\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        300723\n",
      "exploration/num paths total          1792\n",
      "evaluation/num steps total         347829\n",
      "evaluation/num paths total            604\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.32291\n",
      "evaluation/Rewards Std                  0.0820812\n",
      "evaluation/Rewards Max                  5.49077\n",
      "evaluation/Rewards Min                  4.82443\n",
      "evaluation/Returns Mean              5322.91\n",
      "evaluation/Returns Std                 17.1259\n",
      "evaluation/Returns Max               5337.67\n",
      "evaluation/Returns Min               5289.51\n",
      "evaluation/Estimation Bias Mean       471.728\n",
      "evaluation/Estimation Bias Std        174.87\n",
      "evaluation/EB/Q_True Mean              48.1569\n",
      "evaluation/EB/Q_True Std              148.324\n",
      "evaluation/EB/Q_Pred Mean             519.885\n",
      "evaluation/EB/Q_Pred Std               54.3506\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5322.91\n",
      "evaluation/Actions Mean                 0.0726895\n",
      "evaluation/Actions Std                  0.487775\n",
      "evaluation/Actions Max                  0.997844\n",
      "evaluation/Actions Min                 -0.998299\n",
      "time/backward_policy (s)                8.63287\n",
      "time/backward_zf1 (s)                   9.90612\n",
      "time/backward_zf2 (s)                   9.68553\n",
      "time/data sampling (s)                  1.75962\n",
      "time/data storing (s)                   0.0874475\n",
      "time/evaluation sampling (s)            4.60837\n",
      "time/exploration sampling (s)           2.49922\n",
      "time/logging (s)                        0.0120995\n",
      "time/preback_alpha (s)                  0.00519551\n",
      "time/preback_policy (s)                14.9336\n",
      "time/preback_start (s)                  0.919038\n",
      "time/preback_zf (s)                    36.3305\n",
      "time/saving (s)                         3.077e-06\n",
      "time/training (s)                      10.8107\n",
      "time/epoch (s)                        100.19\n",
      "time/total (s)                       5830.73\n",
      "Epoch                                  58\n",
      "---------------------------------  ---------------\n",
      "2024-11-18 23:56:52.160546 +0330 | [humanoid_2024_11_18_22_17_50_0000--s-4] Epoch 59 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 310000\n",
      "trainer/ZF1 Loss                        1.99891\n",
      "trainer/ZF2 Loss                        7.93409\n",
      "trainer/ZF Expert Reward               36.4365\n",
      "trainer/ZF Policy Reward                2.35007\n",
      "trainer/ZF CHI2 Term                   36.1939\n",
      "trainer/Policy Loss                  -351.594\n",
      "trainer/expert_lambda Loss             18.1213\n",
      "trainer/expert_lambda Value            35.4244\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              133.867\n",
      "trainer/Policy Param Norm              41.6567\n",
      "trainer/Zf1 Grad Norm                2388.32\n",
      "trainer/Zf1 Param Norm                151.297\n",
      "trainer/Zf2 Grad Norm                4921.28\n",
      "trainer/Zf2 Param Norm                150.399\n",
      "trainer/Z Expert Predictions Mean     550.806\n",
      "trainer/Z Expert Predictions Std      127.289\n",
      "trainer/Z Expert Predictions Max      827.361\n",
      "trainer/Z Expert Predictions Min      371.934\n",
      "trainer/Z Policy Predictions Mean     347.206\n",
      "trainer/Z Policy Predictions Std      188.951\n",
      "trainer/Z Policy Predictions Max      652.96\n",
      "trainer/Z Policy Predictions Min        3.2821\n",
      "trainer/Z Expert Targets Mean         514.369\n",
      "trainer/Z Expert Targets Std          128.069\n",
      "trainer/Z Expert Targets Max          797.39\n",
      "trainer/Z Expert Targets Min          333.877\n",
      "trainer/Z Policy Targets Mean         344.856\n",
      "trainer/Z Policy Targets Std          188.75\n",
      "trainer/Z Policy Targets Max          652.542\n",
      "trainer/Z Policy Targets Min            2.14356\n",
      "trainer/Log Pis Mean                   17.3781\n",
      "trainer/Log Pis Std                     7.74935\n",
      "trainer/Policy mu Mean                  0.228776\n",
      "trainer/Policy mu Std                   1.08682\n",
      "trainer/Policy log std Mean            -1.52648\n",
      "trainer/Policy log std Std              0.813358\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        305723\n",
      "exploration/num paths total          1797\n",
      "evaluation/num steps total         357829\n",
      "evaluation/num paths total            614\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.26496\n",
      "evaluation/Rewards Std                  0.056413\n",
      "evaluation/Rewards Max                  5.39084\n",
      "evaluation/Rewards Min                  4.87738\n",
      "evaluation/Returns Mean              5264.96\n",
      "evaluation/Returns Std                  2.27378\n",
      "evaluation/Returns Max               5267.48\n",
      "evaluation/Returns Min               5261.03\n",
      "evaluation/Estimation Bias Mean       476.961\n",
      "evaluation/Estimation Bias Std        148.233\n",
      "evaluation/EB/Q_True Mean              47.5147\n",
      "evaluation/EB/Q_True Std              146.331\n",
      "evaluation/EB/Q_Pred Mean             524.476\n",
      "evaluation/EB/Q_Pred Std               22.069\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5264.96\n",
      "evaluation/Actions Mean                 0.0677782\n",
      "evaluation/Actions Std                  0.485193\n",
      "evaluation/Actions Max                  0.997571\n",
      "evaluation/Actions Min                 -0.997274\n",
      "time/backward_policy (s)                8.5273\n",
      "time/backward_zf1 (s)                   9.81251\n",
      "time/backward_zf2 (s)                   9.56076\n",
      "time/data sampling (s)                  1.78606\n",
      "time/data storing (s)                   0.0884943\n",
      "time/evaluation sampling (s)            4.71693\n",
      "time/exploration sampling (s)           2.56789\n",
      "time/logging (s)                        0.0158329\n",
      "time/preback_alpha (s)                  0.00518496\n",
      "time/preback_policy (s)                15.1735\n",
      "time/preback_start (s)                  0.926506\n",
      "time/preback_zf (s)                    36.3852\n",
      "time/saving (s)                         5.652e-06\n",
      "time/training (s)                      10.9663\n",
      "time/epoch (s)                        100.532\n",
      "time/total (s)                       5931.27\n",
      "Epoch                                  59\n",
      "---------------------------------  ---------------\n"
     ]
    }
   ],
   "source": [
    "!python train.py --env humanoid --loss 'v0' --alpha 0.05 --regularize 'TD_both' --reward_type 'sparse' --noise_std 1.0 --sparse_prob 0.5 --sparse_type 'empty' --seed 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58750be-67de-4e16-a844-e4d92a78adfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
