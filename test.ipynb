{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae3af68-ce31-48f3-8b90-4b5291e443b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No personal conf_private.py found.\n",
      "doodad not detected\n",
      "2024-11-01 23:04:38.304376 +0330 | Variant:\n",
      "2024-11-01 23:04:38.304618 +0330 | {\n",
      "  \"algorithm_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"min_num_steps_before_training\": 10000,\n",
      "    \"num_epochs\": 68,\n",
      "    \"num_eval_paths_per_epoch\": 10,\n",
      "    \"num_expl_steps_per_train_loop\": 5000,\n",
      "    \"num_trains_per_train_loop\": 5000\n",
      "  },\n",
      "  \"iq_kwargs\": {\n",
      "    \"demos\": 1,\n",
      "    \"regularize\": \"TD_both\",\n",
      "    \"loss\": \"v0\",\n",
      "    \"chi\": 0.5,\n",
      "    \"expert_path\": \"experts/Humanoid-v2_25.pkl\",\n",
      "    \"subsample_freq\": 1,\n",
      "    \"reward_type\": \"sparse\",\n",
      "    \"sparse_prob\": 0.5,\n",
      "    \"sparse_type\": \"random\"\n",
      "  },\n",
      "  \"env\": \"Humanoid-v2\",\n",
      "  \"seed\": 0,\n",
      "  \"expectation_z\": true,\n",
      "  \"use_policy_expert_obs\": false,\n",
      "  \"eval_env_num\": 10,\n",
      "  \"expl_env_num\": 10,\n",
      "  \"layer_size\": 256,\n",
      "  \"num_quantiles\": 24,\n",
      "  \"replay_buffer_size\": 1000000,\n",
      "  \"trainer_kwargs\": {\n",
      "    \"alpha\": 0.05,\n",
      "    \"discount\": 0.99,\n",
      "    \"policy_lr\": 1e-05,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"tau_type\": \"iqn\",\n",
      "    \"use_automatic_entropy_tuning\": false,\n",
      "    \"zf_lr\": 0.0003,\n",
      "    \"expert_lambda\": 10,\n",
      "    \"expert_lambda_lr\": 0.0001,\n",
      "    \"tune_expert_lambda\": true,\n",
      "    \"policy_lambda\": 5,\n",
      "    \"policy_lambda_lr\": 0.0001,\n",
      "    \"tune_policy_lambda\": false\n",
      "  },\n",
      "  \"version\": \"normal-iqn-neutral\"\n",
      "}\n",
      "/home/eddie/venvs/LSIQ/lib/python3.10/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2024-11-01 23:06:22.136720 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 0 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 15000\n",
      "trainer/ZF1 Loss                      39.8248\n",
      "trainer/ZF2 Loss                      40.2744\n",
      "trainer/ZF Expert Reward              -0.949272\n",
      "trainer/ZF Policy Reward              -1.00876\n",
      "trainer/ZF CHI2 Term                  39.0965\n",
      "trainer/Policy Loss                   -0.385944\n",
      "trainer/expert_lambda Loss            59.9685\n",
      "trainer/expert_lambda Value            9.9999\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm               0.418591\n",
      "trainer/Policy Param Norm             17.4287\n",
      "trainer/Zf1 Grad Norm                107.781\n",
      "trainer/Zf1 Param Norm                32.0463\n",
      "trainer/Zf2 Grad Norm                107.169\n",
      "trainer/Zf2 Param Norm                32.036\n",
      "trainer/Z Expert Predictions Mean     -0.190365\n",
      "trainer/Z Expert Predictions Std       0.18118\n",
      "trainer/Z Expert Predictions Max       0.13977\n",
      "trainer/Z Expert Predictions Min      -0.804621\n",
      "trainer/Z Policy Predictions Mean     -0.315152\n",
      "trainer/Z Policy Predictions Std       0.181904\n",
      "trainer/Z Policy Predictions Max       0.0547174\n",
      "trainer/Z Policy Predictions Min      -0.79134\n",
      "trainer/Z Expert Targets Mean          0.758907\n",
      "trainer/Z Expert Targets Std           0.145199\n",
      "trainer/Z Expert Targets Max           1.04483\n",
      "trainer/Z Expert Targets Min           0.287778\n",
      "trainer/Z Policy Targets Mean          0.693611\n",
      "trainer/Z Policy Targets Std           0.266941\n",
      "trainer/Z Policy Targets Max           1.34168\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                 -11.3364\n",
      "trainer/Log Pis Std                    0.865301\n",
      "trainer/Policy mu Mean                -0.00112765\n",
      "trainer/Policy mu Std                  0.0060151\n",
      "trainer/Policy log std Mean            0.000127196\n",
      "trainer/Policy log std Std             0.0047607\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        14830\n",
      "exploration/num paths total          490\n",
      "evaluation/num steps total           806\n",
      "evaluation/num paths total            10\n",
      "evaluation/path length Mean           80.6\n",
      "evaluation/path length Std             4.47661\n",
      "evaluation/path length Max            90\n",
      "evaluation/path length Min            73\n",
      "evaluation/Rewards Mean                4.44158\n",
      "evaluation/Rewards Std                 0.379885\n",
      "evaluation/Rewards Max                 4.89656\n",
      "evaluation/Rewards Min                 3.40131\n",
      "evaluation/Returns Mean              357.991\n",
      "evaluation/Returns Std                21.8495\n",
      "evaluation/Returns Max               403.81\n",
      "evaluation/Returns Min               322.317\n",
      "evaluation/Estimation Bias Mean      110.602\n",
      "evaluation/Estimation Bias Std        54.4343\n",
      "evaluation/EB/Q_True Mean             16.7246\n",
      "evaluation/EB/Q_True Std              54.141\n",
      "evaluation/EB/Q_Pred Mean            127.326\n",
      "evaluation/EB/Q_Pred Std               5.23242\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           357.991\n",
      "evaluation/Actions Mean                0.0110781\n",
      "evaluation/Actions Std                 0.690812\n",
      "evaluation/Actions Max                 0.996181\n",
      "evaluation/Actions Min                -0.99968\n",
      "time/backward_policy (s)               9.0788\n",
      "time/backward_zf1 (s)                 10.3378\n",
      "time/backward_zf2 (s)                  9.94515\n",
      "time/data sampling (s)                 1.58892\n",
      "time/data storing (s)                  0.087703\n",
      "time/evaluation sampling (s)           0.288361\n",
      "time/exploration sampling (s)          2.8269\n",
      "time/logging (s)                       0.00154607\n",
      "time/preback_alpha (s)                 0.00637712\n",
      "time/preback_policy (s)               14.8005\n",
      "time/preback_start (s)                 1.01401\n",
      "time/preback_zf (s)                   36.8779\n",
      "time/saving (s)                        3.321e-06\n",
      "time/training (s)                     10.9256\n",
      "time/epoch (s)                        97.7795\n",
      "time/total (s)                       105.217\n",
      "Epoch                                  0\n",
      "---------------------------------  ---------------\n",
      "2024-11-01 23:07:58.081450 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 1 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 20000\n",
      "trainer/ZF1 Loss                       4.78431\n",
      "trainer/ZF2 Loss                       4.23279\n",
      "trainer/ZF Expert Reward              12.0032\n",
      "trainer/ZF Policy Reward               3.18575\n",
      "trainer/ZF CHI2 Term                  15.2514\n",
      "trainer/Policy Loss                 -114.341\n",
      "trainer/expert_lambda Loss            19.6761\n",
      "trainer/expert_lambda Value           10.3478\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              12.9908\n",
      "trainer/Policy Param Norm             18.5491\n",
      "trainer/Zf1 Grad Norm                515.273\n",
      "trainer/Zf1 Param Norm                41.8867\n",
      "trainer/Zf2 Grad Norm                599.714\n",
      "trainer/Zf2 Param Norm                41.6249\n",
      "trainer/Z Expert Predictions Mean    133.488\n",
      "trainer/Z Expert Predictions Std      15.3999\n",
      "trainer/Z Expert Predictions Max     163.542\n",
      "trainer/Z Expert Predictions Min      37.4918\n",
      "trainer/Z Policy Predictions Mean    109.031\n",
      "trainer/Z Policy Predictions Std      38.5414\n",
      "trainer/Z Policy Predictions Max     144.918\n",
      "trainer/Z Policy Predictions Min      -2.82614\n",
      "trainer/Z Expert Targets Mean        121.485\n",
      "trainer/Z Expert Targets Std          17.3669\n",
      "trainer/Z Expert Targets Max         158.244\n",
      "trainer/Z Expert Targets Min          21.3343\n",
      "trainer/Z Policy Targets Mean        105.846\n",
      "trainer/Z Policy Targets Std          38.6815\n",
      "trainer/Z Policy Targets Max         145.402\n",
      "trainer/Z Policy Targets Min          -3.43859\n",
      "trainer/Log Pis Mean                   9.23228\n",
      "trainer/Log Pis Std                   10.5517\n",
      "trainer/Policy mu Mean                 0.302426\n",
      "trainer/Policy mu Std                  1.35114\n",
      "trainer/Policy log std Mean           -0.47218\n",
      "trainer/Policy log std Std             0.278084\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        19577\n",
      "exploration/num paths total          548\n",
      "evaluation/num steps total          1896\n",
      "evaluation/num paths total            20\n",
      "evaluation/path length Mean          109\n",
      "evaluation/path length Std            19\n",
      "evaluation/path length Max           151\n",
      "evaluation/path length Min            88\n",
      "evaluation/Rewards Mean                5.1898\n",
      "evaluation/Rewards Std                 0.377208\n",
      "evaluation/Rewards Max                 6.69791\n",
      "evaluation/Rewards Min                 4.72892\n",
      "evaluation/Returns Mean              565.688\n",
      "evaluation/Returns Std               100.4\n",
      "evaluation/Returns Max               780.534\n",
      "evaluation/Returns Min               461.955\n",
      "evaluation/Estimation Bias Mean      123.372\n",
      "evaluation/Estimation Bias Std       114.488\n",
      "evaluation/EB/Q_True Mean             35.7602\n",
      "evaluation/EB/Q_True Std              97.8295\n",
      "evaluation/EB/Q_Pred Mean            159.132\n",
      "evaluation/EB/Q_Pred Std              65.516\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           565.688\n",
      "evaluation/Actions Mean                0.123724\n",
      "evaluation/Actions Std                 0.599112\n",
      "evaluation/Actions Max                 0.998666\n",
      "evaluation/Actions Min                -0.993703\n",
      "time/backward_policy (s)               8.57551\n",
      "time/backward_zf1 (s)                  9.84409\n",
      "time/backward_zf2 (s)                  9.61921\n",
      "time/data sampling (s)                 1.52052\n",
      "time/data storing (s)                  0.0844013\n",
      "time/evaluation sampling (s)           0.701893\n",
      "time/exploration sampling (s)          2.5384\n",
      "time/logging (s)                       0.00179766\n",
      "time/preback_alpha (s)                 0.00613335\n",
      "time/preback_policy (s)               14.8822\n",
      "time/preback_start (s)                 0.957447\n",
      "time/preback_zf (s)                   36.2908\n",
      "time/saving (s)                        2.60799e-06\n",
      "time/training (s)                     10.7204\n",
      "time/epoch (s)                        95.7428\n",
      "time/total (s)                       200.962\n",
      "Epoch                                  1\n",
      "---------------------------------  ---------------\n",
      "2024-11-01 23:09:35.635170 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 2 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 25000\n",
      "trainer/ZF1 Loss                      13.393\n",
      "trainer/ZF2 Loss                      13.5043\n",
      "trainer/ZF Expert Reward              11.736\n",
      "trainer/ZF Policy Reward               2.94752\n",
      "trainer/ZF CHI2 Term                  22.875\n",
      "trainer/Policy Loss                 -146.929\n",
      "trainer/expert_lambda Loss            16.9198\n",
      "trainer/expert_lambda Value           10.7781\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              22.7808\n",
      "trainer/Policy Param Norm             19.1976\n",
      "trainer/Zf1 Grad Norm                977.294\n",
      "trainer/Zf1 Param Norm                46.2284\n",
      "trainer/Zf2 Grad Norm               1305.63\n",
      "trainer/Zf2 Param Norm                45.841\n",
      "trainer/Z Expert Predictions Mean    232.113\n",
      "trainer/Z Expert Predictions Std      23.333\n",
      "trainer/Z Expert Predictions Max     260.651\n",
      "trainer/Z Expert Predictions Min      18.0656\n",
      "trainer/Z Policy Predictions Mean    144.479\n",
      "trainer/Z Policy Predictions Std      68.5916\n",
      "trainer/Z Policy Predictions Max     216.003\n",
      "trainer/Z Policy Predictions Min      -0.392044\n",
      "trainer/Z Expert Targets Mean        220.377\n",
      "trainer/Z Expert Targets Std          24.6701\n",
      "trainer/Z Expert Targets Max         252.272\n",
      "trainer/Z Expert Targets Min           0\n",
      "trainer/Z Policy Targets Mean        141.532\n",
      "trainer/Z Policy Targets Std          69.2552\n",
      "trainer/Z Policy Targets Max         217.287\n",
      "trainer/Z Policy Targets Min          -3.89079\n",
      "trainer/Log Pis Mean                   9.36416\n",
      "trainer/Log Pis Std                    9.53384\n",
      "trainer/Policy mu Mean                 0.250505\n",
      "trainer/Policy mu Std                  1.28778\n",
      "trainer/Policy log std Mean           -0.665939\n",
      "trainer/Policy log std Std             0.222587\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        24601\n",
      "exploration/num paths total          613\n",
      "evaluation/num steps total          2665\n",
      "evaluation/num paths total            30\n",
      "evaluation/path length Mean           76.9\n",
      "evaluation/path length Std             4.1821\n",
      "evaluation/path length Max            83\n",
      "evaluation/path length Min            70\n",
      "evaluation/Rewards Mean                4.98807\n",
      "evaluation/Rewards Std                 0.334605\n",
      "evaluation/Rewards Max                 6.10426\n",
      "evaluation/Rewards Min                 4.02823\n",
      "evaluation/Returns Mean              383.583\n",
      "evaluation/Returns Std                15.6795\n",
      "evaluation/Returns Max               413.519\n",
      "evaluation/Returns Min               359.503\n",
      "evaluation/Estimation Bias Mean      119.862\n",
      "evaluation/Estimation Bias Std        97.9727\n",
      "evaluation/EB/Q_True Mean             16.5842\n",
      "evaluation/EB/Q_True Std              54.1259\n",
      "evaluation/EB/Q_Pred Mean            136.447\n",
      "evaluation/EB/Q_Pred Std              85.0971\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           383.583\n",
      "evaluation/Actions Mean                0.159941\n",
      "evaluation/Actions Std                 0.597895\n",
      "evaluation/Actions Max                 0.999625\n",
      "evaluation/Actions Min                -0.994261\n",
      "time/backward_policy (s)               8.55775\n",
      "time/backward_zf1 (s)                 10.008\n",
      "time/backward_zf2 (s)                  9.60961\n",
      "time/data sampling (s)                 1.62746\n",
      "time/data storing (s)                  0.0858473\n",
      "time/evaluation sampling (s)           0.451063\n",
      "time/exploration sampling (s)          2.49842\n",
      "time/logging (s)                       0.00155805\n",
      "time/preback_alpha (s)                 0.00625694\n",
      "time/preback_policy (s)               15.386\n",
      "time/preback_start (s)                 1.00648\n",
      "time/preback_zf (s)                   36.7617\n",
      "time/saving (s)                        3.569e-06\n",
      "time/training (s)                     11.3447\n",
      "time/epoch (s)                        97.3448\n",
      "time/total (s)                       298.308\n",
      "Epoch                                  2\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:11:12.696332 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 3 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 30000\n",
      "trainer/ZF1 Loss                      11.6736\n",
      "trainer/ZF2 Loss                      18.7118\n",
      "trainer/ZF Expert Reward              13.1283\n",
      "trainer/ZF Policy Reward               4.2122\n",
      "trainer/ZF CHI2 Term                  25.0147\n",
      "trainer/Policy Loss                 -166.904\n",
      "trainer/expert_lambda Loss            13.5603\n",
      "trainer/expert_lambda Value           11.2019\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              29.3797\n",
      "trainer/Policy Param Norm             19.8721\n",
      "trainer/Zf1 Grad Norm               1828.31\n",
      "trainer/Zf1 Param Norm                50.0266\n",
      "trainer/Zf2 Grad Norm               3560.4\n",
      "trainer/Zf2 Param Norm                49.6503\n",
      "trainer/Z Expert Predictions Mean    334.659\n",
      "trainer/Z Expert Predictions Std      12.1302\n",
      "trainer/Z Expert Predictions Max     351.666\n",
      "trainer/Z Expert Predictions Min     295.015\n",
      "trainer/Z Policy Predictions Mean    162.711\n",
      "trainer/Z Policy Predictions Std      86.1778\n",
      "trainer/Z Policy Predictions Max     259.198\n",
      "trainer/Z Policy Predictions Min       4.77408\n",
      "trainer/Z Expert Targets Mean        321.53\n",
      "trainer/Z Expert Targets Std          13.6214\n",
      "trainer/Z Expert Targets Max         342.701\n",
      "trainer/Z Expert Targets Min         276.001\n",
      "trainer/Z Policy Targets Mean        158.498\n",
      "trainer/Z Policy Targets Std          87.316\n",
      "trainer/Z Policy Targets Max         270.918\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  12.1326\n",
      "trainer/Log Pis Std                   10.2735\n",
      "trainer/Policy mu Mean                 0.361997\n",
      "trainer/Policy mu Std                  1.38059\n",
      "trainer/Policy log std Mean           -0.696839\n",
      "trainer/Policy log std Std             0.206608\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        29570\n",
      "exploration/num paths total          670\n",
      "evaluation/num steps total          3581\n",
      "evaluation/num paths total            40\n",
      "evaluation/path length Mean           91.6\n",
      "evaluation/path length Std            19.1478\n",
      "evaluation/path length Max           127\n",
      "evaluation/path length Min            69\n",
      "evaluation/Rewards Mean                4.66638\n",
      "evaluation/Rewards Std                 0.578154\n",
      "evaluation/Rewards Max                 5.95855\n",
      "evaluation/Rewards Min                 3.6237\n",
      "evaluation/Returns Mean              427.441\n",
      "evaluation/Returns Std                91.1053\n",
      "evaluation/Returns Max               589.393\n",
      "evaluation/Returns Min               319.462\n",
      "evaluation/Estimation Bias Mean       72.2284\n",
      "evaluation/Estimation Bias Std       106.28\n",
      "evaluation/EB/Q_True Mean             27.3311\n",
      "evaluation/EB/Q_True Std              76.4461\n",
      "evaluation/EB/Q_Pred Mean             99.5595\n",
      "evaluation/EB/Q_Pred Std              78.7639\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           427.441\n",
      "evaluation/Actions Mean                0.0137429\n",
      "evaluation/Actions Std                 0.67871\n",
      "evaluation/Actions Max                 0.999992\n",
      "evaluation/Actions Min                -0.998818\n",
      "time/backward_policy (s)               8.66862\n",
      "time/backward_zf1 (s)                  9.9898\n",
      "time/backward_zf2 (s)                  9.75136\n",
      "time/data sampling (s)                 1.57582\n",
      "time/data storing (s)                  0.0852728\n",
      "time/evaluation sampling (s)           0.663745\n",
      "time/exploration sampling (s)          2.5277\n",
      "time/logging (s)                       0.00186021\n",
      "time/preback_alpha (s)                 0.00619116\n",
      "time/preback_policy (s)               15.0455\n",
      "time/preback_start (s)                 0.980041\n",
      "time/preback_zf (s)                   36.5105\n",
      "time/saving (s)                        2.8e-06\n",
      "time/training (s)                     11.0508\n",
      "time/epoch (s)                        96.8573\n",
      "time/total (s)                       395.167\n",
      "Epoch                                  3\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:12:49.775578 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 4 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 35000\n",
      "trainer/ZF1 Loss                      13.4359\n",
      "trainer/ZF2 Loss                      10.6683\n",
      "trainer/ZF Expert Reward              13.6138\n",
      "trainer/ZF Policy Reward               2.08742\n",
      "trainer/ZF CHI2 Term                  21.795\n",
      "trainer/Policy Loss                 -147.875\n",
      "trainer/expert_lambda Loss            11.9066\n",
      "trainer/expert_lambda Value           11.6722\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              29.2412\n",
      "trainer/Policy Param Norm             20.3145\n",
      "trainer/Zf1 Grad Norm               3530.75\n",
      "trainer/Zf1 Param Norm                52.8446\n",
      "trainer/Zf2 Grad Norm               1702.27\n",
      "trainer/Zf2 Param Norm                52.5868\n",
      "trainer/Z Expert Predictions Mean    392.545\n",
      "trainer/Z Expert Predictions Std      42.1374\n",
      "trainer/Z Expert Predictions Max     422.6\n",
      "trainer/Z Expert Predictions Min      15.7457\n",
      "trainer/Z Policy Predictions Mean    142.476\n",
      "trainer/Z Policy Predictions Std      96.9598\n",
      "trainer/Z Policy Predictions Max     264.989\n",
      "trainer/Z Policy Predictions Min      -0.467387\n",
      "trainer/Z Expert Targets Mean        378.931\n",
      "trainer/Z Expert Targets Std          41.9468\n",
      "trainer/Z Expert Targets Max         414.208\n",
      "trainer/Z Expert Targets Min           0\n",
      "trainer/Z Policy Targets Mean        140.389\n",
      "trainer/Z Policy Targets Std          98.0362\n",
      "trainer/Z Policy Targets Max         266.053\n",
      "trainer/Z Policy Targets Min          -2.97211\n",
      "trainer/Log Pis Mean                  11.923\n",
      "trainer/Log Pis Std                    8.83577\n",
      "trainer/Policy mu Mean                 0.349343\n",
      "trainer/Policy mu Std                  1.33347\n",
      "trainer/Policy log std Mean           -0.78037\n",
      "trainer/Policy log std Std             0.276894\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        34437\n",
      "exploration/num paths total          723\n",
      "evaluation/num steps total          4418\n",
      "evaluation/num paths total            50\n",
      "evaluation/path length Mean           83.7\n",
      "evaluation/path length Std            20.1695\n",
      "evaluation/path length Max           132\n",
      "evaluation/path length Min            65\n",
      "evaluation/Rewards Mean                4.86444\n",
      "evaluation/Rewards Std                 0.307956\n",
      "evaluation/Rewards Max                 5.71768\n",
      "evaluation/Rewards Min                 3.55326\n",
      "evaluation/Returns Mean              407.153\n",
      "evaluation/Returns Std                92.4948\n",
      "evaluation/Returns Max               625.49\n",
      "evaluation/Returns Min               320.548\n",
      "evaluation/Estimation Bias Mean      143.463\n",
      "evaluation/Estimation Bias Std       115.541\n",
      "evaluation/EB/Q_True Mean             32.3217\n",
      "evaluation/EB/Q_True Std              85.9518\n",
      "evaluation/EB/Q_Pred Mean            175.785\n",
      "evaluation/EB/Q_Pred Std              85.6437\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           407.153\n",
      "evaluation/Actions Mean                0.156685\n",
      "evaluation/Actions Std                 0.603281\n",
      "evaluation/Actions Max                 0.999994\n",
      "evaluation/Actions Min                -0.998214\n",
      "time/backward_policy (s)               8.28093\n",
      "time/backward_zf1 (s)                  9.69765\n",
      "time/backward_zf2 (s)                  9.3715\n",
      "time/data sampling (s)                 1.59428\n",
      "time/data storing (s)                  0.0852242\n",
      "time/evaluation sampling (s)           0.707982\n",
      "time/exploration sampling (s)          2.54018\n",
      "time/logging (s)                       0.00246851\n",
      "time/preback_alpha (s)                 0.00618071\n",
      "time/preback_policy (s)               15.6567\n",
      "time/preback_start (s)                 0.975116\n",
      "time/preback_zf (s)                   36.6092\n",
      "time/saving (s)                        3.382e-06\n",
      "time/training (s)                     11.345\n",
      "time/epoch (s)                        96.8724\n",
      "time/total (s)                       492.042\n",
      "Epoch                                  4\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:14:25.604915 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 5 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 40000\n",
      "trainer/ZF1 Loss                      19.7312\n",
      "trainer/ZF2 Loss                      19.3458\n",
      "trainer/ZF Expert Reward              15.5176\n",
      "trainer/ZF Policy Reward               2.99426\n",
      "trainer/ZF CHI2 Term                  31.1868\n",
      "trainer/Policy Loss                 -141.767\n",
      "trainer/expert_lambda Loss            26.2409\n",
      "trainer/expert_lambda Value           12.1251\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              28.1928\n",
      "trainer/Policy Param Norm             20.6696\n",
      "trainer/Zf1 Grad Norm               2746\n",
      "trainer/Zf1 Param Norm                55.4371\n",
      "trainer/Zf2 Grad Norm               1907.14\n",
      "trainer/Zf2 Param Norm                55.0416\n",
      "trainer/Z Expert Predictions Mean    399.414\n",
      "trainer/Z Expert Predictions Std      31.7315\n",
      "trainer/Z Expert Predictions Max     442.492\n",
      "trainer/Z Expert Predictions Min     152.446\n",
      "trainer/Z Policy Predictions Mean    138.908\n",
      "trainer/Z Policy Predictions Std     101.608\n",
      "trainer/Z Policy Predictions Max     284.929\n",
      "trainer/Z Policy Predictions Min       2.18032\n",
      "trainer/Z Expert Targets Mean        383.896\n",
      "trainer/Z Expert Targets Std          31.3427\n",
      "trainer/Z Expert Targets Max         430.234\n",
      "trainer/Z Expert Targets Min         157.555\n",
      "trainer/Z Policy Targets Mean        135.913\n",
      "trainer/Z Policy Targets Std         103.15\n",
      "trainer/Z Policy Targets Max         277.302\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                  10.9465\n",
      "trainer/Log Pis Std                    9.09638\n",
      "trainer/Policy mu Mean                 0.159514\n",
      "trainer/Policy mu Std                  1.30095\n",
      "trainer/Policy log std Mean           -0.867432\n",
      "trainer/Policy log std Std             0.292263\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        39658\n",
      "exploration/num paths total          798\n",
      "evaluation/num steps total          5142\n",
      "evaluation/num paths total            60\n",
      "evaluation/path length Mean           72.4\n",
      "evaluation/path length Std             5.67803\n",
      "evaluation/path length Max            79\n",
      "evaluation/path length Min            60\n",
      "evaluation/Rewards Mean                5.25214\n",
      "evaluation/Rewards Std                 0.374905\n",
      "evaluation/Rewards Max                 6.28887\n",
      "evaluation/Rewards Min                 4.65936\n",
      "evaluation/Returns Mean              380.255\n",
      "evaluation/Returns Std                33.3573\n",
      "evaluation/Returns Max               418.482\n",
      "evaluation/Returns Min               305.879\n",
      "evaluation/Estimation Bias Mean      116.507\n",
      "evaluation/Estimation Bias Std       108.223\n",
      "evaluation/EB/Q_True Mean             18.6854\n",
      "evaluation/EB/Q_True Std              59.6226\n",
      "evaluation/EB/Q_Pred Mean            135.193\n",
      "evaluation/EB/Q_Pred Std              96.6554\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           380.255\n",
      "evaluation/Actions Mean                0.0766681\n",
      "evaluation/Actions Std                 0.551346\n",
      "evaluation/Actions Max                 0.999738\n",
      "evaluation/Actions Min                -0.997093\n",
      "time/backward_policy (s)               8.28619\n",
      "time/backward_zf1 (s)                  9.58422\n",
      "time/backward_zf2 (s)                  9.31399\n",
      "time/data sampling (s)                 1.56674\n",
      "time/data storing (s)                  0.0841444\n",
      "time/evaluation sampling (s)           0.339975\n",
      "time/exploration sampling (s)          2.29646\n",
      "time/logging (s)                       0.00423395\n",
      "time/preback_alpha (s)                 0.00615067\n",
      "time/preback_policy (s)               15.5417\n",
      "time/preback_start (s)                 0.956249\n",
      "time/preback_zf (s)                   36.4684\n",
      "time/saving (s)                        4.963e-06\n",
      "time/training (s)                     11.1783\n",
      "time/epoch (s)                        95.6268\n",
      "time/total (s)                       587.67\n",
      "Epoch                                  5\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:16:04.608182 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 6 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 45000\n",
      "trainer/ZF1 Loss                      11.0286\n",
      "trainer/ZF2 Loss                      10.4748\n",
      "trainer/ZF Expert Reward              14.8911\n",
      "trainer/ZF Policy Reward               4.89976\n",
      "trainer/ZF CHI2 Term                  22.793\n",
      "trainer/Policy Loss                 -138.672\n",
      "trainer/expert_lambda Loss            11.3894\n",
      "trainer/expert_lambda Value           12.5584\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              24.2457\n",
      "trainer/Policy Param Norm             21.0171\n",
      "trainer/Zf1 Grad Norm               2571.16\n",
      "trainer/Zf1 Param Norm                57.3317\n",
      "trainer/Zf2 Grad Norm               2412.25\n",
      "trainer/Zf2 Param Norm                56.8286\n",
      "trainer/Z Expert Predictions Mean    293.234\n",
      "trainer/Z Expert Predictions Std      24.74\n",
      "trainer/Z Expert Predictions Max     325.9\n",
      "trainer/Z Expert Predictions Min      16.3539\n",
      "trainer/Z Policy Predictions Mean    137.342\n",
      "trainer/Z Policy Predictions Std      90.1842\n",
      "trainer/Z Policy Predictions Max     264.528\n",
      "trainer/Z Policy Predictions Min       1.59588\n",
      "trainer/Z Expert Targets Mean        278.343\n",
      "trainer/Z Expert Targets Std          25.6024\n",
      "trainer/Z Expert Targets Max         312.751\n",
      "trainer/Z Expert Targets Min           0\n",
      "trainer/Z Policy Targets Mean        132.442\n",
      "trainer/Z Policy Targets Std          90.3602\n",
      "trainer/Z Policy Targets Max         261.582\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                   7.43133\n",
      "trainer/Log Pis Std                    7.62823\n",
      "trainer/Policy mu Mean                 0.181867\n",
      "trainer/Policy mu Std                  1.21857\n",
      "trainer/Policy log std Mean           -0.702686\n",
      "trainer/Policy log std Std             0.252175\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        44708\n",
      "exploration/num paths total          869\n",
      "evaluation/num steps total          5826\n",
      "evaluation/num paths total            70\n",
      "evaluation/path length Mean           68.4\n",
      "evaluation/path length Std             6.43739\n",
      "evaluation/path length Max            80\n",
      "evaluation/path length Min            60\n",
      "evaluation/Rewards Mean                5.45624\n",
      "evaluation/Rewards Std                 0.449467\n",
      "evaluation/Rewards Max                 6.70137\n",
      "evaluation/Rewards Min                 4.87472\n",
      "evaluation/Returns Mean              373.207\n",
      "evaluation/Returns Std                34.2089\n",
      "evaluation/Returns Max               430.127\n",
      "evaluation/Returns Min               323.719\n",
      "evaluation/Estimation Bias Mean       87.9863\n",
      "evaluation/Estimation Bias Std        92.5831\n",
      "evaluation/EB/Q_True Mean             20.4929\n",
      "evaluation/EB/Q_True Std              63.0854\n",
      "evaluation/EB/Q_Pred Mean            108.479\n",
      "evaluation/EB/Q_Pred Std              72.3336\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           373.207\n",
      "evaluation/Actions Mean                0.0247052\n",
      "evaluation/Actions Std                 0.612232\n",
      "evaluation/Actions Max                 0.999829\n",
      "evaluation/Actions Min                -0.995678\n",
      "time/backward_policy (s)               8.81401\n",
      "time/backward_zf1 (s)                 10.423\n",
      "time/backward_zf2 (s)                  9.95525\n",
      "time/data sampling (s)                 1.70066\n",
      "time/data storing (s)                  0.0880788\n",
      "time/evaluation sampling (s)           0.349147\n",
      "time/exploration sampling (s)          2.42956\n",
      "time/logging (s)                       0.00294152\n",
      "time/preback_alpha (s)                 0.00656964\n",
      "time/preback_policy (s)               15.3541\n",
      "time/preback_start (s)                 1.02067\n",
      "time/preback_zf (s)                   37.1049\n",
      "time/saving (s)                        5.325e-06\n",
      "time/training (s)                     11.5364\n",
      "time/epoch (s)                        98.7852\n",
      "time/total (s)                       686.459\n",
      "Epoch                                  6\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:17:44.357859 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 7 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 50000\n",
      "trainer/ZF1 Loss                      10.9506\n",
      "trainer/ZF2 Loss                      12.7604\n",
      "trainer/ZF Expert Reward              15.8953\n",
      "trainer/ZF Policy Reward               3.03762\n",
      "trainer/ZF CHI2 Term                  24.5615\n",
      "trainer/Policy Loss                 -121.87\n",
      "trainer/expert_lambda Loss            22.1132\n",
      "trainer/expert_lambda Value           12.9954\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              29.8292\n",
      "trainer/Policy Param Norm             21.4617\n",
      "trainer/Zf1 Grad Norm               2538.78\n",
      "trainer/Zf1 Param Norm                58.9811\n",
      "trainer/Zf2 Grad Norm               2458.2\n",
      "trainer/Zf2 Param Norm                58.71\n",
      "trainer/Z Expert Predictions Mean    323.935\n",
      "trainer/Z Expert Predictions Std      21.0469\n",
      "trainer/Z Expert Predictions Max     361.811\n",
      "trainer/Z Expert Predictions Min     252.846\n",
      "trainer/Z Policy Predictions Mean    120.065\n",
      "trainer/Z Policy Predictions Std      77.6696\n",
      "trainer/Z Policy Predictions Max     241.089\n",
      "trainer/Z Policy Predictions Min      -2.68574\n",
      "trainer/Z Expert Targets Mean        308.04\n",
      "trainer/Z Expert Targets Std          21.9103\n",
      "trainer/Z Expert Targets Max         347.935\n",
      "trainer/Z Expert Targets Min         220\n",
      "trainer/Z Policy Targets Mean        117.027\n",
      "trainer/Z Policy Targets Std          78.4052\n",
      "trainer/Z Policy Targets Max         240.168\n",
      "trainer/Z Policy Targets Min          -2.72122\n",
      "trainer/Log Pis Mean                   7.29453\n",
      "trainer/Log Pis Std                    7.45891\n",
      "trainer/Policy mu Mean                 0.283234\n",
      "trainer/Policy mu Std                  1.21772\n",
      "trainer/Policy log std Mean           -0.71511\n",
      "trainer/Policy log std Std             0.236161\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        49531\n",
      "exploration/num paths total          929\n",
      "evaluation/num steps total          6835\n",
      "evaluation/num paths total            80\n",
      "evaluation/path length Mean          100.9\n",
      "evaluation/path length Std            22.3984\n",
      "evaluation/path length Max           142\n",
      "evaluation/path length Min            71\n",
      "evaluation/Rewards Mean                5.24909\n",
      "evaluation/Rewards Std                 0.453953\n",
      "evaluation/Rewards Max                 6.65734\n",
      "evaluation/Rewards Min                 3.32277\n",
      "evaluation/Returns Mean              529.633\n",
      "evaluation/Returns Std                94.8079\n",
      "evaluation/Returns Max               670.239\n",
      "evaluation/Returns Min               390.356\n",
      "evaluation/Estimation Bias Mean      102.748\n",
      "evaluation/Estimation Bias Std       110.474\n",
      "evaluation/EB/Q_True Mean             30.3678\n",
      "evaluation/EB/Q_True Std              85.3489\n",
      "evaluation/EB/Q_Pred Mean            133.115\n",
      "evaluation/EB/Q_Pred Std              70.0251\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           529.633\n",
      "evaluation/Actions Mean                0.206748\n",
      "evaluation/Actions Std                 0.518155\n",
      "evaluation/Actions Max                 0.999882\n",
      "evaluation/Actions Min                -0.983935\n",
      "time/backward_policy (s)               8.71078\n",
      "time/backward_zf1 (s)                 10.3703\n",
      "time/backward_zf2 (s)                  9.84035\n",
      "time/data sampling (s)                 1.74402\n",
      "time/data storing (s)                  0.0911488\n",
      "time/evaluation sampling (s)           0.792542\n",
      "time/exploration sampling (s)          2.67184\n",
      "time/logging (s)                       0.00414674\n",
      "time/preback_alpha (s)                 0.00668111\n",
      "time/preback_policy (s)               15.3821\n",
      "time/preback_start (s)                 1.04636\n",
      "time/preback_zf (s)                   37.3756\n",
      "time/saving (s)                        4.904e-06\n",
      "time/training (s)                     11.4954\n",
      "time/epoch (s)                        99.5313\n",
      "time/total (s)                       785.992\n",
      "Epoch                                  7\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:19:23.088780 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 8 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 55000\n",
      "trainer/ZF1 Loss                       5.53869\n",
      "trainer/ZF2 Loss                       4.25173\n",
      "trainer/ZF Expert Reward              15.1784\n",
      "trainer/ZF Policy Reward               3.93721\n",
      "trainer/ZF CHI2 Term                  17.2467\n",
      "trainer/Policy Loss                 -116.5\n",
      "trainer/expert_lambda Loss            18.3396\n",
      "trainer/expert_lambda Value           13.4313\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              20.3116\n",
      "trainer/Policy Param Norm             21.8609\n",
      "trainer/Zf1 Grad Norm                781.554\n",
      "trainer/Zf1 Param Norm                60.7631\n",
      "trainer/Zf2 Grad Norm               1200.89\n",
      "trainer/Zf2 Param Norm                60.6184\n",
      "trainer/Z Expert Predictions Mean    292.4\n",
      "trainer/Z Expert Predictions Std      36.6115\n",
      "trainer/Z Expert Predictions Max     347.308\n",
      "trainer/Z Expert Predictions Min      23.5978\n",
      "trainer/Z Policy Predictions Mean    115.22\n",
      "trainer/Z Policy Predictions Std      68.5211\n",
      "trainer/Z Policy Predictions Max     239.406\n",
      "trainer/Z Policy Predictions Min       0.219909\n",
      "trainer/Z Expert Targets Mean        277.221\n",
      "trainer/Z Expert Targets Std          37.0552\n",
      "trainer/Z Expert Targets Max         331.553\n",
      "trainer/Z Expert Targets Min           2.9755\n",
      "trainer/Z Policy Targets Mean        111.283\n",
      "trainer/Z Policy Targets Std          68.8487\n",
      "trainer/Z Policy Targets Max         237.565\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                   6.40512\n",
      "trainer/Log Pis Std                    7.14131\n",
      "trainer/Policy mu Mean                 0.312187\n",
      "trainer/Policy mu Std                  1.0994\n",
      "trainer/Policy log std Mean           -0.764554\n",
      "trainer/Policy log std Std             0.293784\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        54468\n",
      "exploration/num paths total          977\n",
      "evaluation/num steps total          7603\n",
      "evaluation/num paths total            90\n",
      "evaluation/path length Mean           76.8\n",
      "evaluation/path length Std            15.263\n",
      "evaluation/path length Max           118\n",
      "evaluation/path length Min            63\n",
      "evaluation/Rewards Mean                5.08841\n",
      "evaluation/Rewards Std                 0.333963\n",
      "evaluation/Rewards Max                 6.17228\n",
      "evaluation/Rewards Min                 4.58534\n",
      "evaluation/Returns Mean              390.79\n",
      "evaluation/Returns Std                77.8062\n",
      "evaluation/Returns Max               596.344\n",
      "evaluation/Returns Min               310.482\n",
      "evaluation/Estimation Bias Mean      120.509\n",
      "evaluation/Estimation Bias Std       114.157\n",
      "evaluation/EB/Q_True Mean             33.1244\n",
      "evaluation/EB/Q_True Std              86.3174\n",
      "evaluation/EB/Q_Pred Mean            153.634\n",
      "evaluation/EB/Q_Pred Std              87.8658\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           390.79\n",
      "evaluation/Actions Mean                0.116988\n",
      "evaluation/Actions Std                 0.566641\n",
      "evaluation/Actions Max                 0.997579\n",
      "evaluation/Actions Min                -0.991675\n",
      "time/backward_policy (s)               8.57965\n",
      "time/backward_zf1 (s)                 10.2077\n",
      "time/backward_zf2 (s)                  9.65995\n",
      "time/data sampling (s)                 1.75174\n",
      "time/data storing (s)                  0.0907542\n",
      "time/evaluation sampling (s)           0.562771\n",
      "time/exploration sampling (s)          2.56853\n",
      "time/logging (s)                       0.00377383\n",
      "time/preback_alpha (s)                 0.00641459\n",
      "time/preback_policy (s)               15.3691\n",
      "time/preback_start (s)                 1.04218\n",
      "time/preback_zf (s)                   37.2232\n",
      "time/saving (s)                        4.791e-06\n",
      "time/training (s)                     11.4444\n",
      "time/epoch (s)                        98.5101\n",
      "time/total (s)                       884.505\n",
      "Epoch                                  8\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:21:01.907403 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 9 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 60000\n",
      "trainer/ZF1 Loss                       9.78406\n",
      "trainer/ZF2 Loss                      10.1055\n",
      "trainer/ZF Expert Reward              14.4887\n",
      "trainer/ZF Policy Reward               2.97086\n",
      "trainer/ZF CHI2 Term                  20.8274\n",
      "trainer/Policy Loss                 -119.006\n",
      "trainer/expert_lambda Loss            15.0788\n",
      "trainer/expert_lambda Value           13.8762\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              21.5666\n",
      "trainer/Policy Param Norm             22.4043\n",
      "trainer/Zf1 Grad Norm               2315.78\n",
      "trainer/Zf1 Param Norm                62.545\n",
      "trainer/Zf2 Grad Norm               2355.89\n",
      "trainer/Zf2 Param Norm                62.4078\n",
      "trainer/Z Expert Predictions Mean    368.132\n",
      "trainer/Z Expert Predictions Std      23.4556\n",
      "trainer/Z Expert Predictions Max     400.269\n",
      "trainer/Z Expert Predictions Min     274.826\n",
      "trainer/Z Policy Predictions Mean    117.123\n",
      "trainer/Z Policy Predictions Std      73.6032\n",
      "trainer/Z Policy Predictions Max     253.221\n",
      "trainer/Z Policy Predictions Min      -0.392046\n",
      "trainer/Z Expert Targets Mean        353.644\n",
      "trainer/Z Expert Targets Std          22.086\n",
      "trainer/Z Expert Targets Max         384.574\n",
      "trainer/Z Expert Targets Min         261.004\n",
      "trainer/Z Policy Targets Mean        114.152\n",
      "trainer/Z Policy Targets Std          74.9698\n",
      "trainer/Z Policy Targets Max         245.446\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                   6.19075\n",
      "trainer/Log Pis Std                    6.95983\n",
      "trainer/Policy mu Mean                 0.23389\n",
      "trainer/Policy mu Std                  1.10788\n",
      "trainer/Policy log std Mean           -0.761592\n",
      "trainer/Policy log std Std             0.340344\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        59390\n",
      "exploration/num paths total         1030\n",
      "evaluation/num steps total          8810\n",
      "evaluation/num paths total           100\n",
      "evaluation/path length Mean          120.7\n",
      "evaluation/path length Std            22.0955\n",
      "evaluation/path length Max           156\n",
      "evaluation/path length Min            90\n",
      "evaluation/Rewards Mean                5.19877\n",
      "evaluation/Rewards Std                 0.40265\n",
      "evaluation/Rewards Max                 6.43312\n",
      "evaluation/Rewards Min                 3.3617\n",
      "evaluation/Returns Mean              627.492\n",
      "evaluation/Returns Std               110.115\n",
      "evaluation/Returns Max               800.638\n",
      "evaluation/Returns Min               478.216\n",
      "evaluation/Estimation Bias Mean      139.021\n",
      "evaluation/Estimation Bias Std       125.371\n",
      "evaluation/EB/Q_True Mean             33.4769\n",
      "evaluation/EB/Q_True Std              95.3775\n",
      "evaluation/EB/Q_Pred Mean            172.498\n",
      "evaluation/EB/Q_Pred Std              97.6406\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           627.492\n",
      "evaluation/Actions Mean                0.152052\n",
      "evaluation/Actions Std                 0.539569\n",
      "evaluation/Actions Max                 0.999984\n",
      "evaluation/Actions Min                -0.995828\n",
      "time/backward_policy (s)               8.41263\n",
      "time/backward_zf1 (s)                 10.045\n",
      "time/backward_zf2 (s)                  9.49167\n",
      "time/data sampling (s)                 1.74939\n",
      "time/data storing (s)                  0.0876013\n",
      "time/evaluation sampling (s)           0.742198\n",
      "time/exploration sampling (s)          2.48643\n",
      "time/logging (s)                       0.00420249\n",
      "time/preback_alpha (s)                 0.00635948\n",
      "time/preback_policy (s)               15.8227\n",
      "time/preback_start (s)                 1.04926\n",
      "time/preback_zf (s)                   37.0865\n",
      "time/saving (s)                        5.246e-06\n",
      "time/training (s)                     11.6156\n",
      "time/epoch (s)                        98.5996\n",
      "time/total (s)                       983.108\n",
      "Epoch                                  9\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:22:40.465756 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 10 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 65000\n",
      "trainer/ZF1 Loss                      11.8172\n",
      "trainer/ZF2 Loss                       8.88309\n",
      "trainer/ZF Expert Reward              15.155\n",
      "trainer/ZF Policy Reward               2.0641\n",
      "trainer/ZF CHI2 Term                  21.1614\n",
      "trainer/Policy Loss                 -127.767\n",
      "trainer/expert_lambda Loss            17.1557\n",
      "trainer/expert_lambda Value           14.3361\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              21.4151\n",
      "trainer/Policy Param Norm             22.9846\n",
      "trainer/Zf1 Grad Norm               3611.24\n",
      "trainer/Zf1 Param Norm                65.2356\n",
      "trainer/Zf2 Grad Norm               2127.25\n",
      "trainer/Zf2 Param Norm                64.9252\n",
      "trainer/Z Expert Predictions Mean    442.62\n",
      "trainer/Z Expert Predictions Std      47.3382\n",
      "trainer/Z Expert Predictions Max     480.288\n",
      "trainer/Z Expert Predictions Min      45.7352\n",
      "trainer/Z Policy Predictions Mean    126.222\n",
      "trainer/Z Policy Predictions Std      79.476\n",
      "trainer/Z Policy Predictions Max     306.044\n",
      "trainer/Z Policy Predictions Min      -2.42368\n",
      "trainer/Z Expert Targets Mean        427.465\n",
      "trainer/Z Expert Targets Std          45.7056\n",
      "trainer/Z Expert Targets Max         467.831\n",
      "trainer/Z Expert Targets Min          67.9633\n",
      "trainer/Z Policy Targets Mean        124.158\n",
      "trainer/Z Policy Targets Std          80.412\n",
      "trainer/Z Policy Targets Max         305.695\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                   8.24125\n",
      "trainer/Log Pis Std                    7.9734\n",
      "trainer/Policy mu Mean                 0.391518\n",
      "trainer/Policy mu Std                  1.12142\n",
      "trainer/Policy log std Mean           -0.815312\n",
      "trainer/Policy log std Std             0.481376\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        64648\n",
      "exploration/num paths total         1084\n",
      "evaluation/num steps total          9804\n",
      "evaluation/num paths total           110\n",
      "evaluation/path length Mean           99.4\n",
      "evaluation/path length Std             9.94183\n",
      "evaluation/path length Max           112\n",
      "evaluation/path length Min            76\n",
      "evaluation/Rewards Mean                5.04266\n",
      "evaluation/Rewards Std                 0.335632\n",
      "evaluation/Rewards Max                 6.61863\n",
      "evaluation/Rewards Min                 4.30303\n",
      "evaluation/Returns Mean              501.241\n",
      "evaluation/Returns Std                41.0836\n",
      "evaluation/Returns Max               566.296\n",
      "evaluation/Returns Min               409.28\n",
      "evaluation/Estimation Bias Mean      114.744\n",
      "evaluation/Estimation Bias Std       109.146\n",
      "evaluation/EB/Q_True Mean             23.2768\n",
      "evaluation/EB/Q_True Std              72.7745\n",
      "evaluation/EB/Q_Pred Mean            138.021\n",
      "evaluation/EB/Q_Pred Std              82.47\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           501.241\n",
      "evaluation/Actions Mean                0.126533\n",
      "evaluation/Actions Std                 0.548639\n",
      "evaluation/Actions Max                 0.997779\n",
      "evaluation/Actions Min                -0.98851\n",
      "time/backward_policy (s)               8.51947\n",
      "time/backward_zf1 (s)                 10.0658\n",
      "time/backward_zf2 (s)                  9.60339\n",
      "time/data sampling (s)                 1.7013\n",
      "time/data storing (s)                  0.0894819\n",
      "time/evaluation sampling (s)           0.552236\n",
      "time/exploration sampling (s)          2.54952\n",
      "time/logging (s)                       0.00397922\n",
      "time/preback_alpha (s)                 0.00636782\n",
      "time/preback_policy (s)               15.6483\n",
      "time/preback_start (s)                 1.01552\n",
      "time/preback_zf (s)                   37.0518\n",
      "time/saving (s)                        4.921e-06\n",
      "time/training (s)                     11.5356\n",
      "time/epoch (s)                        98.3428\n",
      "time/total (s)                      1081.45\n",
      "Epoch                                 10\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:24:17.458075 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 11 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 70000\n",
      "trainer/ZF1 Loss                       9.25653\n",
      "trainer/ZF2 Loss                      12.7521\n",
      "trainer/ZF Expert Reward              17.0789\n",
      "trainer/ZF Policy Reward               3.84081\n",
      "trainer/ZF CHI2 Term                  24.1324\n",
      "trainer/Policy Loss                 -132.869\n",
      "trainer/expert_lambda Loss            24.8427\n",
      "trainer/expert_lambda Value           14.7913\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              26.0909\n",
      "trainer/Policy Param Norm             23.5404\n",
      "trainer/Zf1 Grad Norm               1795.15\n",
      "trainer/Zf1 Param Norm                67.3892\n",
      "trainer/Zf2 Grad Norm               2405.04\n",
      "trainer/Zf2 Param Norm                67.0394\n",
      "trainer/Z Expert Predictions Mean    404.663\n",
      "trainer/Z Expert Predictions Std      37.6716\n",
      "trainer/Z Expert Predictions Max     463.084\n",
      "trainer/Z Expert Predictions Min     278.018\n",
      "trainer/Z Policy Predictions Mean    131.138\n",
      "trainer/Z Policy Predictions Std      74.0178\n",
      "trainer/Z Policy Predictions Max     264.634\n",
      "trainer/Z Policy Predictions Min      -2.85049\n",
      "trainer/Z Expert Targets Mean        387.584\n",
      "trainer/Z Expert Targets Std          38.1455\n",
      "trainer/Z Expert Targets Max         452.033\n",
      "trainer/Z Expert Targets Min         266.689\n",
      "trainer/Z Policy Targets Mean        127.298\n",
      "trainer/Z Policy Targets Std          73.6517\n",
      "trainer/Z Policy Targets Max         260.795\n",
      "trainer/Z Policy Targets Min          -4.64521\n",
      "trainer/Log Pis Mean                   6.97055\n",
      "trainer/Log Pis Std                    7.21474\n",
      "trainer/Policy mu Mean                 0.233918\n",
      "trainer/Policy mu Std                  1.09951\n",
      "trainer/Policy log std Mean           -0.783155\n",
      "trainer/Policy log std Std             0.410433\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        69362\n",
      "exploration/num paths total         1130\n",
      "evaluation/num steps total         10814\n",
      "evaluation/num paths total           120\n",
      "evaluation/path length Mean          101\n",
      "evaluation/path length Std            19.199\n",
      "evaluation/path length Max           130\n",
      "evaluation/path length Min            73\n",
      "evaluation/Rewards Mean                4.71371\n",
      "evaluation/Rewards Std                 0.538883\n",
      "evaluation/Rewards Max                 6.51357\n",
      "evaluation/Rewards Min                 3.10771\n",
      "evaluation/Returns Mean              476.084\n",
      "evaluation/Returns Std               108.872\n",
      "evaluation/Returns Max               631.839\n",
      "evaluation/Returns Min               324.003\n",
      "evaluation/Estimation Bias Mean       99.0631\n",
      "evaluation/Estimation Bias Std        84.0112\n",
      "evaluation/EB/Q_True Mean             15.4582\n",
      "evaluation/EB/Q_True Std              46.5617\n",
      "evaluation/EB/Q_Pred Mean            114.521\n",
      "evaluation/EB/Q_Pred Std              76.041\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           476.084\n",
      "evaluation/Actions Mean                0.128695\n",
      "evaluation/Actions Std                 0.614047\n",
      "evaluation/Actions Max                 0.999987\n",
      "evaluation/Actions Min                -0.999323\n",
      "time/backward_policy (s)               8.19141\n",
      "time/backward_zf1 (s)                  9.63616\n",
      "time/backward_zf2 (s)                  9.21343\n",
      "time/data sampling (s)                 1.64544\n",
      "time/data storing (s)                  0.0851494\n",
      "time/evaluation sampling (s)           0.797106\n",
      "time/exploration sampling (s)          2.50113\n",
      "time/logging (s)                       0.00485226\n",
      "time/preback_alpha (s)                 0.00618777\n",
      "time/preback_policy (s)               15.7286\n",
      "time/preback_start (s)                 0.981541\n",
      "time/preback_zf (s)                   36.593\n",
      "time/saving (s)                        6.523e-06\n",
      "time/training (s)                     11.3991\n",
      "time/epoch (s)                        96.7831\n",
      "time/total (s)                      1178.24\n",
      "Epoch                                 11\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:25:53.233841 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 12 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 75000\n",
      "trainer/ZF1 Loss                       5.107\n",
      "trainer/ZF2 Loss                       5.22445\n",
      "trainer/ZF Expert Reward              17.0724\n",
      "trainer/ZF Policy Reward               3.35558\n",
      "trainer/ZF CHI2 Term                  18.5867\n",
      "trainer/Policy Loss                 -110.652\n",
      "trainer/expert_lambda Loss            19.9047\n",
      "trainer/expert_lambda Value           15.2331\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              18.6402\n",
      "trainer/Policy Param Norm             24.0509\n",
      "trainer/Zf1 Grad Norm               1290.04\n",
      "trainer/Zf1 Param Norm                69.0655\n",
      "trainer/Zf2 Grad Norm               1372.2\n",
      "trainer/Zf2 Param Norm                68.8716\n",
      "trainer/Z Expert Predictions Mean    375.154\n",
      "trainer/Z Expert Predictions Std      38.6893\n",
      "trainer/Z Expert Predictions Max     434.395\n",
      "trainer/Z Expert Predictions Min      28.7177\n",
      "trainer/Z Policy Predictions Mean    108.906\n",
      "trainer/Z Policy Predictions Std      68.5719\n",
      "trainer/Z Policy Predictions Max     258.135\n",
      "trainer/Z Policy Predictions Min       0.18387\n",
      "trainer/Z Expert Targets Mean        358.082\n",
      "trainer/Z Expert Targets Std          38.7487\n",
      "trainer/Z Expert Targets Max         410.78\n",
      "trainer/Z Expert Targets Min          21.3771\n",
      "trainer/Z Policy Targets Mean        105.55\n",
      "trainer/Z Policy Targets Std          69.6135\n",
      "trainer/Z Policy Targets Max         250.92\n",
      "trainer/Z Policy Targets Min          -1.74502\n",
      "trainer/Log Pis Mean                   8.08083\n",
      "trainer/Log Pis Std                    8.06025\n",
      "trainer/Policy mu Mean                 0.272912\n",
      "trainer/Policy mu Std                  1.19397\n",
      "trainer/Policy log std Mean           -0.724876\n",
      "trainer/Policy log std Std             0.418076\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        74488\n",
      "exploration/num paths total         1179\n",
      "evaluation/num steps total         12312\n",
      "evaluation/num paths total           130\n",
      "evaluation/path length Mean          149.8\n",
      "evaluation/path length Std            35.7765\n",
      "evaluation/path length Max           196\n",
      "evaluation/path length Min           101\n",
      "evaluation/Rewards Mean                4.79748\n",
      "evaluation/Rewards Std                 0.489839\n",
      "evaluation/Rewards Max                 6.04856\n",
      "evaluation/Rewards Min                 2.90148\n",
      "evaluation/Returns Mean              718.662\n",
      "evaluation/Returns Std               174.507\n",
      "evaluation/Returns Max               962.874\n",
      "evaluation/Returns Min               500.145\n",
      "evaluation/Estimation Bias Mean      100.103\n",
      "evaluation/Estimation Bias Std       121.683\n",
      "evaluation/EB/Q_True Mean             36.0836\n",
      "evaluation/EB/Q_True Std             102.157\n",
      "evaluation/EB/Q_Pred Mean            136.186\n",
      "evaluation/EB/Q_Pred Std              77.8\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           718.662\n",
      "evaluation/Actions Mean                0.0654824\n",
      "evaluation/Actions Std                 0.55239\n",
      "evaluation/Actions Max                 0.999972\n",
      "evaluation/Actions Min                -0.999987\n",
      "time/backward_policy (s)               7.73575\n",
      "time/backward_zf1 (s)                  9.05973\n",
      "time/backward_zf2 (s)                  8.67763\n",
      "time/data sampling (s)                 1.59791\n",
      "time/data storing (s)                  0.0840829\n",
      "time/evaluation sampling (s)           1.0178\n",
      "time/exploration sampling (s)          2.4203\n",
      "time/logging (s)                       0.00562037\n",
      "time/preback_alpha (s)                 0.006122\n",
      "time/preback_policy (s)               16.1985\n",
      "time/preback_start (s)                 0.961926\n",
      "time/preback_zf (s)                   36.3058\n",
      "time/saving (s)                        5.053e-06\n",
      "time/training (s)                     11.5006\n",
      "time/epoch (s)                        95.5718\n",
      "time/total (s)                      1273.81\n",
      "Epoch                                 12\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:27:32.058753 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 13 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 80000\n",
      "trainer/ZF1 Loss                       1.6794\n",
      "trainer/ZF2 Loss                       4.64135\n",
      "trainer/ZF Expert Reward              18.0343\n",
      "trainer/ZF Policy Reward               4.90503\n",
      "trainer/ZF CHI2 Term                  17.052\n",
      "trainer/Policy Loss                 -115.536\n",
      "trainer/expert_lambda Loss            16.6153\n",
      "trainer/expert_lambda Value           15.6838\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              22.3755\n",
      "trainer/Policy Param Norm             24.5275\n",
      "trainer/Zf1 Grad Norm               1045.77\n",
      "trainer/Zf1 Param Norm                71.1104\n",
      "trainer/Zf2 Grad Norm               1252.75\n",
      "trainer/Zf2 Param Norm                70.8616\n",
      "trainer/Z Expert Predictions Mean    423.43\n",
      "trainer/Z Expert Predictions Std      43.8653\n",
      "trainer/Z Expert Predictions Max     474.282\n",
      "trainer/Z Expert Predictions Min      32.8162\n",
      "trainer/Z Policy Predictions Mean    114.2\n",
      "trainer/Z Policy Predictions Std      67.3725\n",
      "trainer/Z Policy Predictions Max     239.976\n",
      "trainer/Z Policy Predictions Min       3.81809\n",
      "trainer/Z Expert Targets Mean        405.396\n",
      "trainer/Z Expert Targets Std          44.2516\n",
      "trainer/Z Expert Targets Max         459.104\n",
      "trainer/Z Expert Targets Min          10.2911\n",
      "trainer/Z Policy Targets Mean        109.295\n",
      "trainer/Z Policy Targets Std          66.8077\n",
      "trainer/Z Policy Targets Max         234.05\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                   6.40911\n",
      "trainer/Log Pis Std                    7.66336\n",
      "trainer/Policy mu Mean                 0.290208\n",
      "trainer/Policy mu Std                  1.11478\n",
      "trainer/Policy log std Mean           -0.735155\n",
      "trainer/Policy log std Std             0.445906\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        79260\n",
      "exploration/num paths total         1219\n",
      "evaluation/num steps total         13473\n",
      "evaluation/num paths total           140\n",
      "evaluation/path length Mean          116.1\n",
      "evaluation/path length Std            28.1938\n",
      "evaluation/path length Max           164\n",
      "evaluation/path length Min            85\n",
      "evaluation/Rewards Mean                5.31919\n",
      "evaluation/Rewards Std                 0.363066\n",
      "evaluation/Rewards Max                 6.34715\n",
      "evaluation/Rewards Min                 4.84298\n",
      "evaluation/Returns Mean              617.558\n",
      "evaluation/Returns Std               151.375\n",
      "evaluation/Returns Max               880.153\n",
      "evaluation/Returns Min               451.195\n",
      "evaluation/Estimation Bias Mean      137.029\n",
      "evaluation/Estimation Bias Std       148.57\n",
      "evaluation/EB/Q_True Mean             39.8914\n",
      "evaluation/EB/Q_True Std             107.625\n",
      "evaluation/EB/Q_Pred Mean            176.92\n",
      "evaluation/EB/Q_Pred Std             100.061\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           617.558\n",
      "evaluation/Actions Mean                0.147839\n",
      "evaluation/Actions Std                 0.575527\n",
      "evaluation/Actions Max                 0.99841\n",
      "evaluation/Actions Min                -0.995939\n",
      "time/backward_policy (s)               8.66193\n",
      "time/backward_zf1 (s)                 10.1748\n",
      "time/backward_zf2 (s)                  9.71663\n",
      "time/data sampling (s)                 1.76138\n",
      "time/data storing (s)                  0.0886541\n",
      "time/evaluation sampling (s)           0.843028\n",
      "time/exploration sampling (s)          2.59943\n",
      "time/logging (s)                       0.00269328\n",
      "time/preback_alpha (s)                 0.00642498\n",
      "time/preback_policy (s)               15.3003\n",
      "time/preback_start (s)                 1.03948\n",
      "time/preback_zf (s)                   36.9859\n",
      "time/saving (s)                        3.921e-06\n",
      "time/training (s)                     11.4232\n",
      "time/epoch (s)                        98.6038\n",
      "time/total (s)                      1372.42\n",
      "Epoch                                 13\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:29:09.100088 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 14 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 85000\n",
      "trainer/ZF1 Loss                       5.85509\n",
      "trainer/ZF2 Loss                       4.20363\n",
      "trainer/ZF Expert Reward              18.5584\n",
      "trainer/ZF Policy Reward               3.17014\n",
      "trainer/ZF CHI2 Term                  18.5914\n",
      "trainer/Policy Loss                 -119.765\n",
      "trainer/expert_lambda Loss            14.1355\n",
      "trainer/expert_lambda Value           16.1375\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              18.9515\n",
      "trainer/Policy Param Norm             25.0552\n",
      "trainer/Zf1 Grad Norm               2686.01\n",
      "trainer/Zf1 Param Norm                73.7043\n",
      "trainer/Zf2 Grad Norm               1482.36\n",
      "trainer/Zf2 Param Norm                73.366\n",
      "trainer/Z Expert Predictions Mean    506.84\n",
      "trainer/Z Expert Predictions Std      49.283\n",
      "trainer/Z Expert Predictions Max     545.476\n",
      "trainer/Z Expert Predictions Min      23.1692\n",
      "trainer/Z Policy Predictions Mean    118.176\n",
      "trainer/Z Policy Predictions Std      65.5613\n",
      "trainer/Z Policy Predictions Max     236.835\n",
      "trainer/Z Policy Predictions Min       4.56505\n",
      "trainer/Z Expert Targets Mean        488.282\n",
      "trainer/Z Expert Targets Std          49.7492\n",
      "trainer/Z Expert Targets Max         530.818\n",
      "trainer/Z Expert Targets Min           8.31836\n",
      "trainer/Z Policy Targets Mean        115.006\n",
      "trainer/Z Policy Targets Std          65.6907\n",
      "trainer/Z Policy Targets Max         233.755\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                   6.7437\n",
      "trainer/Log Pis Std                    6.72606\n",
      "trainer/Policy mu Mean                 0.270581\n",
      "trainer/Policy mu Std                  1.09425\n",
      "trainer/Policy log std Mean           -0.810311\n",
      "trainer/Policy log std Std             0.443956\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        84487\n",
      "exploration/num paths total         1271\n",
      "evaluation/num steps total         14426\n",
      "evaluation/num paths total           150\n",
      "evaluation/path length Mean           95.3\n",
      "evaluation/path length Std            13.2291\n",
      "evaluation/path length Max           122\n",
      "evaluation/path length Min            73\n",
      "evaluation/Rewards Mean                5.13689\n",
      "evaluation/Rewards Std                 0.17444\n",
      "evaluation/Rewards Max                 5.90478\n",
      "evaluation/Rewards Min                 4.83514\n",
      "evaluation/Returns Mean              489.545\n",
      "evaluation/Returns Std                72.1067\n",
      "evaluation/Returns Max               636.454\n",
      "evaluation/Returns Min               368.351\n",
      "evaluation/Estimation Bias Mean      127.486\n",
      "evaluation/Estimation Bias Std       107.148\n",
      "evaluation/EB/Q_True Mean             20.5439\n",
      "evaluation/EB/Q_True Std              63.0855\n",
      "evaluation/EB/Q_Pred Mean            148.03\n",
      "evaluation/EB/Q_Pred Std              89.8121\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           489.545\n",
      "evaluation/Actions Mean                0.136865\n",
      "evaluation/Actions Std                 0.577358\n",
      "evaluation/Actions Max                 0.995837\n",
      "evaluation/Actions Min                -0.991523\n",
      "time/backward_policy (s)               8.10156\n",
      "time/backward_zf1 (s)                  9.54475\n",
      "time/backward_zf2 (s)                  9.09253\n",
      "time/data sampling (s)                 1.67355\n",
      "time/data storing (s)                  0.086457\n",
      "time/evaluation sampling (s)           1.01902\n",
      "time/exploration sampling (s)          2.4559\n",
      "time/logging (s)                       0.00418478\n",
      "time/preback_alpha (s)                 0.00621223\n",
      "time/preback_policy (s)               15.7872\n",
      "time/preback_start (s)                 1.01128\n",
      "time/preback_zf (s)                   36.5979\n",
      "time/saving (s)                        5.397e-06\n",
      "time/training (s)                     11.4533\n",
      "time/epoch (s)                        96.8339\n",
      "time/total (s)                      1469.26\n",
      "Epoch                                 14\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:30:47.935696 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 15 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 90000\n",
      "trainer/ZF1 Loss                       2.48159\n",
      "trainer/ZF2 Loss                       3.75046\n",
      "trainer/ZF Expert Reward              18.4293\n",
      "trainer/ZF Policy Reward               3.12044\n",
      "trainer/ZF CHI2 Term                  15.7362\n",
      "trainer/Policy Loss                 -123.133\n",
      "trainer/expert_lambda Loss            10.3823\n",
      "trainer/expert_lambda Value           16.5965\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              19.3482\n",
      "trainer/Policy Param Norm             25.6472\n",
      "trainer/Zf1 Grad Norm               2047.18\n",
      "trainer/Zf1 Param Norm                76.4247\n",
      "trainer/Zf2 Grad Norm               1818.58\n",
      "trainer/Zf2 Param Norm                75.9623\n",
      "trainer/Z Expert Predictions Mean    591.078\n",
      "trainer/Z Expert Predictions Std      40.5938\n",
      "trainer/Z Expert Predictions Max     631.223\n",
      "trainer/Z Expert Predictions Min     273.808\n",
      "trainer/Z Policy Predictions Mean    121.13\n",
      "trainer/Z Policy Predictions Std      74.664\n",
      "trainer/Z Policy Predictions Max     294.116\n",
      "trainer/Z Policy Predictions Min       3.7529\n",
      "trainer/Z Expert Targets Mean        572.649\n",
      "trainer/Z Expert Targets Std          41.3394\n",
      "trainer/Z Expert Targets Max         613.238\n",
      "trainer/Z Expert Targets Min         256.203\n",
      "trainer/Z Policy Targets Mean        118.009\n",
      "trainer/Z Policy Targets Std          75.4501\n",
      "trainer/Z Policy Targets Max         279.91\n",
      "trainer/Z Policy Targets Min           0\n",
      "trainer/Log Pis Mean                   6.60637\n",
      "trainer/Log Pis Std                    6.75967\n",
      "trainer/Policy mu Mean                 0.279538\n",
      "trainer/Policy mu Std                  1.12101\n",
      "trainer/Policy log std Mean           -0.74457\n",
      "trainer/Policy log std Std             0.419258\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        88986\n",
      "exploration/num paths total         1304\n",
      "evaluation/num steps total         15870\n",
      "evaluation/num paths total           160\n",
      "evaluation/path length Mean          144.4\n",
      "evaluation/path length Std            46.3513\n",
      "evaluation/path length Max           250\n",
      "evaluation/path length Min           104\n",
      "evaluation/Rewards Mean                5.29052\n",
      "evaluation/Rewards Std                 0.259904\n",
      "evaluation/Rewards Max                 6.37821\n",
      "evaluation/Rewards Min                 4.85741\n",
      "evaluation/Returns Mean              763.951\n",
      "evaluation/Returns Std               246.373\n",
      "evaluation/Returns Max              1324.83\n",
      "evaluation/Returns Min               545.837\n",
      "evaluation/Estimation Bias Mean      185.666\n",
      "evaluation/Estimation Bias Std       179.981\n",
      "evaluation/EB/Q_True Mean             40.4766\n",
      "evaluation/EB/Q_True Std              99.4125\n",
      "evaluation/EB/Q_Pred Mean            226.142\n",
      "evaluation/EB/Q_Pred Std             144.709\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           763.951\n",
      "evaluation/Actions Mean                0.0517402\n",
      "evaluation/Actions Std                 0.528793\n",
      "evaluation/Actions Max                 0.997536\n",
      "evaluation/Actions Min                -0.990073\n",
      "time/backward_policy (s)               8.34292\n",
      "time/backward_zf1 (s)                  9.87774\n",
      "time/backward_zf2 (s)                  9.41459\n",
      "time/data sampling (s)                 1.72002\n",
      "time/data storing (s)                  0.0881333\n",
      "time/evaluation sampling (s)           1.39372\n",
      "time/exploration sampling (s)          2.51146\n",
      "time/logging (s)                       0.00657879\n",
      "time/preback_alpha (s)                 0.00630652\n",
      "time/preback_policy (s)               15.6944\n",
      "time/preback_start (s)                 1.0218\n",
      "time/preback_zf (s)                   37.0689\n",
      "time/saving (s)                        6.322e-06\n",
      "time/training (s)                     11.4759\n",
      "time/epoch (s)                        98.6225\n",
      "time/total (s)                      1567.88\n",
      "Epoch                                 15\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:32:26.869457 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 16 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 95000\n",
      "trainer/ZF1 Loss                      14.3574\n",
      "trainer/ZF2 Loss                      19.035\n",
      "trainer/ZF Expert Reward              18.7624\n",
      "trainer/ZF Policy Reward               1.33976\n",
      "trainer/ZF CHI2 Term                  29.1963\n",
      "trainer/Policy Loss                 -142.058\n",
      "trainer/expert_lambda Loss            10.8987\n",
      "trainer/expert_lambda Value           17.0488\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              36.4878\n",
      "trainer/Policy Param Norm             26.3165\n",
      "trainer/Zf1 Grad Norm               5133.77\n",
      "trainer/Zf1 Param Norm                79.1109\n",
      "trainer/Zf2 Grad Norm               4424.86\n",
      "trainer/Zf2 Param Norm                78.515\n",
      "trainer/Z Expert Predictions Mean    634.754\n",
      "trainer/Z Expert Predictions Std      48.2613\n",
      "trainer/Z Expert Predictions Max     683.778\n",
      "trainer/Z Expert Predictions Min     324.809\n",
      "trainer/Z Policy Predictions Mean    139.768\n",
      "trainer/Z Policy Predictions Std      88.0916\n",
      "trainer/Z Policy Predictions Max     345.762\n",
      "trainer/Z Policy Predictions Min      -0.371006\n",
      "trainer/Z Expert Targets Mean        615.991\n",
      "trainer/Z Expert Targets Std          48.1528\n",
      "trainer/Z Expert Targets Max         670.106\n",
      "trainer/Z Expert Targets Min         307.626\n",
      "trainer/Z Policy Targets Mean        138.429\n",
      "trainer/Z Policy Targets Std          89.5689\n",
      "trainer/Z Policy Targets Max         364.562\n",
      "trainer/Z Policy Targets Min          -4.41177\n",
      "trainer/Log Pis Mean                   9.33458\n",
      "trainer/Log Pis Std                    8.01775\n",
      "trainer/Policy mu Mean                 0.202264\n",
      "trainer/Policy mu Std                  1.1525\n",
      "trainer/Policy log std Mean           -0.902537\n",
      "trainer/Policy log std Std             0.544664\n",
      "trainer/Alpha                          0.05\n",
      "exploration/num steps total        93659\n",
      "exploration/num paths total         1324\n",
      "evaluation/num steps total         18427\n",
      "evaluation/num paths total           170\n",
      "evaluation/path length Mean          255.7\n",
      "evaluation/path length Std            62.981\n",
      "evaluation/path length Max           361\n",
      "evaluation/path length Min           168\n",
      "evaluation/Rewards Mean                5.16117\n",
      "evaluation/Rewards Std                 0.314959\n",
      "evaluation/Rewards Max                 6.4504\n",
      "evaluation/Rewards Min                 3.44329\n",
      "evaluation/Returns Mean             1319.71\n",
      "evaluation/Returns Std               308.757\n",
      "evaluation/Returns Max              1828.14\n",
      "evaluation/Returns Min               871.458\n",
      "evaluation/Estimation Bias Mean      226.559\n",
      "evaluation/Estimation Bias Std       194.959\n",
      "evaluation/EB/Q_True Mean             41.1316\n",
      "evaluation/EB/Q_True Std             116.66\n",
      "evaluation/EB/Q_Pred Mean            267.691\n",
      "evaluation/EB/Q_Pred Std             142.103\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns          1319.71\n",
      "evaluation/Actions Mean                0.115645\n",
      "evaluation/Actions Std                 0.552136\n",
      "evaluation/Actions Max                 0.999579\n",
      "evaluation/Actions Min                -0.999887\n",
      "time/backward_policy (s)               8.19904\n",
      "time/backward_zf1 (s)                  9.62118\n",
      "time/backward_zf2 (s)                  9.18388\n",
      "time/data sampling (s)                 1.71453\n",
      "time/data storing (s)                  0.0865658\n",
      "time/evaluation sampling (s)           2.74615\n",
      "time/exploration sampling (s)          2.54157\n",
      "time/logging (s)                       0.00361011\n",
      "time/preback_alpha (s)                 0.00622777\n",
      "time/preback_policy (s)               15.6784\n",
      "time/preback_start (s)                 1.00506\n",
      "time/preback_zf (s)                   36.5732\n",
      "time/saving (s)                        2.642e-06\n",
      "time/training (s)                     11.3551\n",
      "time/epoch (s)                        98.7144\n",
      "time/total (s)                      1666.6\n",
      "Epoch                                 16\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:34:06.015793 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 17 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 100000\n",
      "trainer/ZF1 Loss                       20.739\n",
      "trainer/ZF2 Loss                       24.0176\n",
      "trainer/ZF Expert Reward               18.4459\n",
      "trainer/ZF Policy Reward                4.62186\n",
      "trainer/ZF CHI2 Term                   34.1085\n",
      "trainer/Policy Loss                  -163.74\n",
      "trainer/expert_lambda Loss              7.99425\n",
      "trainer/expert_lambda Value            17.4872\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               42.7742\n",
      "trainer/Policy Param Norm              26.8995\n",
      "trainer/Zf1 Grad Norm                4482.65\n",
      "trainer/Zf1 Param Norm                 81.9503\n",
      "trainer/Zf2 Grad Norm                3806.25\n",
      "trainer/Zf2 Param Norm                 81.1276\n",
      "trainer/Z Expert Predictions Mean     680.899\n",
      "trainer/Z Expert Predictions Std       59.9574\n",
      "trainer/Z Expert Predictions Max      730.529\n",
      "trainer/Z Expert Predictions Min      375.346\n",
      "trainer/Z Policy Predictions Mean     159.73\n",
      "trainer/Z Policy Predictions Std      126.259\n",
      "trainer/Z Policy Predictions Max      553.718\n",
      "trainer/Z Policy Predictions Min       -1.93412\n",
      "trainer/Z Expert Targets Mean         662.453\n",
      "trainer/Z Expert Targets Std           59.2825\n",
      "trainer/Z Expert Targets Max          715.47\n",
      "trainer/Z Expert Targets Min          364.006\n",
      "trainer/Z Policy Targets Mean         155.108\n",
      "trainer/Z Policy Targets Std          126.753\n",
      "trainer/Z Policy Targets Max          557.916\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   10.4088\n",
      "trainer/Log Pis Std                     9.14583\n",
      "trainer/Policy mu Mean                  0.320616\n",
      "trainer/Policy mu Std                   1.15984\n",
      "trainer/Policy log std Mean            -0.924092\n",
      "trainer/Policy log std Std              0.615173\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total         98092\n",
      "exploration/num paths total          1345\n",
      "evaluation/num steps total          20308\n",
      "evaluation/num paths total            180\n",
      "evaluation/path length Mean           188.1\n",
      "evaluation/path length Std             70.4833\n",
      "evaluation/path length Max            280\n",
      "evaluation/path length Min             82\n",
      "evaluation/Rewards Mean                 5.2641\n",
      "evaluation/Rewards Std                  0.190297\n",
      "evaluation/Rewards Max                  6.18371\n",
      "evaluation/Rewards Min                  4.70287\n",
      "evaluation/Returns Mean               990.178\n",
      "evaluation/Returns Std                366.35\n",
      "evaluation/Returns Max               1474.5\n",
      "evaluation/Returns Min                448.319\n",
      "evaluation/Estimation Bias Mean       259.293\n",
      "evaluation/Estimation Bias Std        247.937\n",
      "evaluation/EB/Q_True Mean              52.6633\n",
      "evaluation/EB/Q_True Std              136.216\n",
      "evaluation/EB/Q_Pred Mean             311.956\n",
      "evaluation/EB/Q_Pred Std              188.831\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns            990.178\n",
      "evaluation/Actions Mean                 0.095391\n",
      "evaluation/Actions Std                  0.568256\n",
      "evaluation/Actions Max                  0.998624\n",
      "evaluation/Actions Min                 -0.998042\n",
      "time/backward_policy (s)                8.61033\n",
      "time/backward_zf1 (s)                  10.1311\n",
      "time/backward_zf2 (s)                   9.67246\n",
      "time/data sampling (s)                  1.81708\n",
      "time/data storing (s)                   0.0881381\n",
      "time/evaluation sampling (s)            0.947906\n",
      "time/exploration sampling (s)           2.63686\n",
      "time/logging (s)                        0.00574984\n",
      "time/preback_alpha (s)                  0.00642336\n",
      "time/preback_policy (s)                15.4481\n",
      "time/preback_start (s)                  1.03174\n",
      "time/preback_zf (s)                    36.9994\n",
      "time/saving (s)                         3.505e-06\n",
      "time/training (s)                      11.5372\n",
      "time/epoch (s)                         98.9325\n",
      "time/total (s)                       1765.54\n",
      "Epoch                                  17\n",
      "---------------------------------  ---------------\n",
      "2024-11-01 23:35:47.116596 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 18 finished\n",
      "---------------------------------  ----------------\n",
      "replay_buffer/size                 105000\n",
      "trainer/ZF1 Loss                       51.2193\n",
      "trainer/ZF2 Loss                       51.0818\n",
      "trainer/ZF Expert Reward               18.474\n",
      "trainer/ZF Policy Reward                4.74428\n",
      "trainer/ZF CHI2 Term                   62.6379\n",
      "trainer/Policy Loss                  -174.12\n",
      "trainer/expert_lambda Loss              8.81188\n",
      "trainer/expert_lambda Value            17.9295\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               42.5181\n",
      "trainer/Policy Param Norm              27.5144\n",
      "trainer/Zf1 Grad Norm                7508.63\n",
      "trainer/Zf1 Param Norm                 84.651\n",
      "trainer/Zf2 Grad Norm                8914.31\n",
      "trainer/Zf2 Param Norm                 83.731\n",
      "trainer/Z Expert Predictions Mean     707.601\n",
      "trainer/Z Expert Predictions Std       85.703\n",
      "trainer/Z Expert Predictions Max      762.611\n",
      "trainer/Z Expert Predictions Min       10.3931\n",
      "trainer/Z Policy Predictions Mean     171.845\n",
      "trainer/Z Policy Predictions Std      145.675\n",
      "trainer/Z Policy Predictions Max      612.697\n",
      "trainer/Z Policy Predictions Min        3.68193\n",
      "trainer/Z Expert Targets Mean         689.127\n",
      "trainer/Z Expert Targets Std           85.8221\n",
      "trainer/Z Expert Targets Max          746.298\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean         167.101\n",
      "trainer/Z Policy Targets Std          147.423\n",
      "trainer/Z Policy Targets Max          646.932\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   10.2736\n",
      "trainer/Log Pis Std                     8.91906\n",
      "trainer/Policy mu Mean                  0.301751\n",
      "trainer/Policy mu Std                   1.15244\n",
      "trainer/Policy log std Mean            -0.937941\n",
      "trainer/Policy log std Std              0.582511\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        102667\n",
      "exploration/num paths total          1363\n",
      "evaluation/num steps total          22823\n",
      "evaluation/num paths total            190\n",
      "evaluation/path length Mean           251.5\n",
      "evaluation/path length Std            133.109\n",
      "evaluation/path length Max            456\n",
      "evaluation/path length Min            105\n",
      "evaluation/Rewards Mean                 5.14118\n",
      "evaluation/Rewards Std                  0.391299\n",
      "evaluation/Rewards Max                  6.46127\n",
      "evaluation/Rewards Min                  3.64889\n",
      "evaluation/Returns Mean              1293.01\n",
      "evaluation/Returns Std                745.128\n",
      "evaluation/Returns Max               2426.59\n",
      "evaluation/Returns Min                485.915\n",
      "evaluation/Estimation Bias Mean       328.984\n",
      "evaluation/Estimation Bias Std        358.391\n",
      "evaluation/EB/Q_True Mean              76.4788\n",
      "evaluation/EB/Q_True Std              171.113\n",
      "evaluation/EB/Q_Pred Mean             405.463\n",
      "evaluation/EB/Q_Pred Std              267.023\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           1293.01\n",
      "evaluation/Actions Mean                 0.0891539\n",
      "evaluation/Actions Std                  0.593161\n",
      "evaluation/Actions Max                  0.999896\n",
      "evaluation/Actions Min                 -0.999552\n",
      "time/backward_policy (s)                8.70481\n",
      "time/backward_zf1 (s)                  10.346\n",
      "time/backward_zf2 (s)                   9.86683\n",
      "time/data sampling (s)                  1.81589\n",
      "time/data storing (s)                   0.0873033\n",
      "time/evaluation sampling (s)            2.21872\n",
      "time/exploration sampling (s)           2.57914\n",
      "time/logging (s)                        0.00574076\n",
      "time/preback_alpha (s)                  0.00645347\n",
      "time/preback_policy (s)                15.5104\n",
      "time/preback_start (s)                  1.03403\n",
      "time/preback_zf (s)                    37.158\n",
      "time/saving (s)                         5.30099e-06\n",
      "time/training (s)                      11.5489\n",
      "time/epoch (s)                        100.882\n",
      "time/total (s)                       1866.42\n",
      "Epoch                                  18\n",
      "---------------------------------  ----------------\n",
      "2024-11-01 23:37:30.163029 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 19 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 110000\n",
      "trainer/ZF1 Loss                       46.3508\n",
      "trainer/ZF2 Loss                       49.8498\n",
      "trainer/ZF Expert Reward               19.5272\n",
      "trainer/ZF Policy Reward                3.53829\n",
      "trainer/ZF CHI2 Term                   59.6605\n",
      "trainer/Policy Loss                  -216.294\n",
      "trainer/expert_lambda Loss             14.755\n",
      "trainer/expert_lambda Value            18.357\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               55.9221\n",
      "trainer/Policy Param Norm              28.1671\n",
      "trainer/Zf1 Grad Norm                7100.16\n",
      "trainer/Zf1 Param Norm                 87.6292\n",
      "trainer/Zf2 Grad Norm               13543\n",
      "trainer/Zf2 Param Norm                 86.6398\n",
      "trainer/Z Expert Predictions Mean     801.592\n",
      "trainer/Z Expert Predictions Std       67.7928\n",
      "trainer/Z Expert Predictions Max      853.368\n",
      "trainer/Z Expert Predictions Min      119.794\n",
      "trainer/Z Policy Predictions Mean     212.965\n",
      "trainer/Z Policy Predictions Std      191.468\n",
      "trainer/Z Policy Predictions Max      798.433\n",
      "trainer/Z Policy Predictions Min        3.41455\n",
      "trainer/Z Expert Targets Mean         782.065\n",
      "trainer/Z Expert Targets Std           66.0237\n",
      "trainer/Z Expert Targets Max          836.723\n",
      "trainer/Z Expert Targets Min          150.836\n",
      "trainer/Z Policy Targets Mean         209.427\n",
      "trainer/Z Policy Targets Std          192.444\n",
      "trainer/Z Policy Targets Max          780.847\n",
      "trainer/Z Policy Targets Min           -1.47954\n",
      "trainer/Log Pis Mean                   13.5046\n",
      "trainer/Log Pis Std                     8.52981\n",
      "trainer/Policy mu Mean                  0.282194\n",
      "trainer/Policy mu Std                   1.23248\n",
      "trainer/Policy log std Mean            -1.04365\n",
      "trainer/Policy log std Std              0.637407\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        104078\n",
      "exploration/num paths total          1366\n",
      "evaluation/num steps total          30109\n",
      "evaluation/num paths total            203\n",
      "evaluation/path length Mean           560.462\n",
      "evaluation/path length Std            415.787\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            101\n",
      "evaluation/Rewards Mean                 5.12463\n",
      "evaluation/Rewards Std                  0.268549\n",
      "evaluation/Rewards Max                  6.31765\n",
      "evaluation/Rewards Min                  2.65897\n",
      "evaluation/Returns Mean              2872.16\n",
      "evaluation/Returns Std               2194.86\n",
      "evaluation/Returns Max               5196.95\n",
      "evaluation/Returns Min                457.714\n",
      "evaluation/Estimation Bias Mean       622.456\n",
      "evaluation/Estimation Bias Std        325.856\n",
      "evaluation/EB/Q_True Mean              64.3436\n",
      "evaluation/EB/Q_True Std              165.767\n",
      "evaluation/EB/Q_Pred Mean             686.8\n",
      "evaluation/EB/Q_Pred Std              193.591\n",
      "evaluation/Num Paths                   13\n",
      "evaluation/Average Returns           2872.16\n",
      "evaluation/Actions Mean                 0.0702186\n",
      "evaluation/Actions Std                  0.516628\n",
      "evaluation/Actions Max                  0.999692\n",
      "evaluation/Actions Min                 -0.999256\n",
      "time/backward_policy (s)                8.62001\n",
      "time/backward_zf1 (s)                  10.2121\n",
      "time/backward_zf2 (s)                   9.67601\n",
      "time/data sampling (s)                  1.75322\n",
      "time/data storing (s)                   0.088242\n",
      "time/evaluation sampling (s)            4.89874\n",
      "time/exploration sampling (s)           2.52997\n",
      "time/logging (s)                        0.0112458\n",
      "time/preback_alpha (s)                  0.00641559\n",
      "time/preback_policy (s)                15.432\n",
      "time/preback_start (s)                  1.04392\n",
      "time/preback_zf (s)                    37.0886\n",
      "time/saving (s)                         3.338e-06\n",
      "time/training (s)                      11.4729\n",
      "time/epoch (s)                        102.833\n",
      "time/total (s)                       1969.26\n",
      "Epoch                                  19\n",
      "---------------------------------  ---------------\n",
      "2024-11-01 23:39:07.966872 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 20 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 115000\n",
      "trainer/ZF1 Loss                       59.1462\n",
      "trainer/ZF2 Loss                       65.5006\n",
      "trainer/ZF Expert Reward               19.0308\n",
      "trainer/ZF Policy Reward               -0.391645\n",
      "trainer/ZF CHI2 Term                   72.6134\n",
      "trainer/Policy Loss                  -282.34\n",
      "trainer/expert_lambda Loss             10.3526\n",
      "trainer/expert_lambda Value            18.7735\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               74.8502\n",
      "trainer/Policy Param Norm              28.7688\n",
      "trainer/Zf1 Grad Norm               14944.4\n",
      "trainer/Zf1 Param Norm                 90.4194\n",
      "trainer/Zf2 Grad Norm               21041.5\n",
      "trainer/Zf2 Param Norm                 89.3951\n",
      "trainer/Z Expert Predictions Mean     882.055\n",
      "trainer/Z Expert Predictions Std       64.8603\n",
      "trainer/Z Expert Predictions Max      942.339\n",
      "trainer/Z Expert Predictions Min      112.124\n",
      "trainer/Z Policy Predictions Mean     274.254\n",
      "trainer/Z Policy Predictions Std      233.095\n",
      "trainer/Z Policy Predictions Max      863.519\n",
      "trainer/Z Policy Predictions Min        2.49723\n",
      "trainer/Z Expert Targets Mean         863.024\n",
      "trainer/Z Expert Targets Std           63.1526\n",
      "trainer/Z Expert Targets Max          924.282\n",
      "trainer/Z Expert Targets Min          117.414\n",
      "trainer/Z Policy Targets Mean         274.645\n",
      "trainer/Z Policy Targets Std          237.001\n",
      "trainer/Z Policy Targets Max          878.827\n",
      "trainer/Z Policy Targets Min           -1.28694\n",
      "trainer/Log Pis Mean                   14.6826\n",
      "trainer/Log Pis Std                     8.94195\n",
      "trainer/Policy mu Mean                  0.209695\n",
      "trainer/Policy mu Std                   1.2344\n",
      "trainer/Policy log std Mean            -1.14185\n",
      "trainer/Policy log std Std              0.65384\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        111639\n",
      "exploration/num paths total          1374\n",
      "evaluation/num steps total          33084\n",
      "evaluation/num paths total            213\n",
      "evaluation/path length Mean           297.5\n",
      "evaluation/path length Std             78.5089\n",
      "evaluation/path length Max            414\n",
      "evaluation/path length Min            107\n",
      "evaluation/Rewards Mean                 5.40522\n",
      "evaluation/Rewards Std                  0.316617\n",
      "evaluation/Rewards Max                  6.34538\n",
      "evaluation/Rewards Min                  2.77305\n",
      "evaluation/Returns Mean              1608.05\n",
      "evaluation/Returns Std                450.678\n",
      "evaluation/Returns Max               2258.04\n",
      "evaluation/Returns Min                486.854\n",
      "evaluation/Estimation Bias Mean       558.432\n",
      "evaluation/Estimation Bias Std        334.851\n",
      "evaluation/EB/Q_True Mean              45.7594\n",
      "evaluation/EB/Q_True Std              128.082\n",
      "evaluation/EB/Q_Pred Mean             604.191\n",
      "evaluation/EB/Q_Pred Std              310.681\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           1608.05\n",
      "evaluation/Actions Mean                 0.0729871\n",
      "evaluation/Actions Std                  0.591244\n",
      "evaluation/Actions Max                  0.999885\n",
      "evaluation/Actions Min                 -0.999811\n",
      "time/backward_policy (s)                8.06712\n",
      "time/backward_zf1 (s)                   9.46878\n",
      "time/backward_zf2 (s)                   9.02446\n",
      "time/data sampling (s)                  1.8704\n",
      "time/data storing (s)                   0.085831\n",
      "time/evaluation sampling (s)            2.0105\n",
      "time/exploration sampling (s)           2.47193\n",
      "time/logging (s)                        0.00573013\n",
      "time/preback_alpha (s)                  0.00617475\n",
      "time/preback_policy (s)                15.7985\n",
      "time/preback_start (s)                  1.00138\n",
      "time/preback_zf (s)                    36.4451\n",
      "time/saving (s)                         2.684e-06\n",
      "time/training (s)                      11.3341\n",
      "time/epoch (s)                         97.59\n",
      "time/total (s)                       2066.85\n",
      "Epoch                                  20\n",
      "---------------------------------  ---------------\n",
      "2024-11-01 23:40:47.746638 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 21 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 120000\n",
      "trainer/ZF1 Loss                       80.9588\n",
      "trainer/ZF2 Loss                       82.2957\n",
      "trainer/ZF Expert Reward               19.681\n",
      "trainer/ZF Policy Reward                2.95209\n",
      "trainer/ZF CHI2 Term                   91.911\n",
      "trainer/Policy Loss                  -303.372\n",
      "trainer/expert_lambda Loss              8.05732\n",
      "trainer/expert_lambda Value            19.1586\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               75.2819\n",
      "trainer/Policy Param Norm              29.326\n",
      "trainer/Zf1 Grad Norm               13250.4\n",
      "trainer/Zf1 Param Norm                 93.1213\n",
      "trainer/Zf2 Grad Norm               10065.7\n",
      "trainer/Zf2 Param Norm                 92.0553\n",
      "trainer/Z Expert Predictions Mean     945.298\n",
      "trainer/Z Expert Predictions Std       54.4358\n",
      "trainer/Z Expert Predictions Max     1009.32\n",
      "trainer/Z Expert Predictions Min      614.95\n",
      "trainer/Z Policy Predictions Mean     296.484\n",
      "trainer/Z Policy Predictions Std      274.692\n",
      "trainer/Z Policy Predictions Max      932.217\n",
      "trainer/Z Policy Predictions Min       -1.00421\n",
      "trainer/Z Expert Targets Mean         925.617\n",
      "trainer/Z Expert Targets Std           54.0274\n",
      "trainer/Z Expert Targets Max          993.149\n",
      "trainer/Z Expert Targets Min          609.387\n",
      "trainer/Z Policy Targets Mean         293.532\n",
      "trainer/Z Policy Targets Std          275.43\n",
      "trainer/Z Policy Targets Max          952.617\n",
      "trainer/Z Policy Targets Min           -2.05678\n",
      "trainer/Log Pis Mean                   16.7932\n",
      "trainer/Log Pis Std                     9.91571\n",
      "trainer/Policy mu Mean                  0.371159\n",
      "trainer/Policy mu Std                   1.28953\n",
      "trainer/Policy log std Mean            -1.13466\n",
      "trainer/Policy log std Std              0.747328\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        113291\n",
      "exploration/num paths total          1376\n",
      "evaluation/num steps total          41271\n",
      "evaluation/num paths total            223\n",
      "evaluation/path length Mean           818.7\n",
      "evaluation/path length Std            362.606\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             89\n",
      "evaluation/Rewards Mean                 5.31273\n",
      "evaluation/Rewards Std                  0.152171\n",
      "evaluation/Rewards Max                  5.52301\n",
      "evaluation/Rewards Min                  3.21178\n",
      "evaluation/Returns Mean              4349.53\n",
      "evaluation/Returns Std               1960.47\n",
      "evaluation/Returns Max               5339.06\n",
      "evaluation/Returns Min                407.608\n",
      "evaluation/Estimation Bias Mean       899.671\n",
      "evaluation/Estimation Bias Std        236.394\n",
      "evaluation/EB/Q_True Mean              58.7634\n",
      "evaluation/EB/Q_True Std              161.851\n",
      "evaluation/EB/Q_Pred Mean             958.434\n",
      "evaluation/EB/Q_Pred Std              123.502\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4349.53\n",
      "evaluation/Actions Mean                 0.0408581\n",
      "evaluation/Actions Std                  0.522713\n",
      "evaluation/Actions Max                  0.999999\n",
      "evaluation/Actions Min                 -0.999819\n",
      "time/backward_policy (s)                8.16296\n",
      "time/backward_zf1 (s)                   9.51835\n",
      "time/backward_zf2 (s)                   9.16133\n",
      "time/data sampling (s)                  1.74519\n",
      "time/data storing (s)                   0.0906238\n",
      "time/evaluation sampling (s)            3.71641\n",
      "time/exploration sampling (s)           2.47249\n",
      "time/logging (s)                        0.0117561\n",
      "time/preback_alpha (s)                  0.00616898\n",
      "time/preback_policy (s)                15.7593\n",
      "time/preback_start (s)                  0.984523\n",
      "time/preback_zf (s)                    36.5519\n",
      "time/saving (s)                         2.95e-06\n",
      "time/training (s)                      11.3949\n",
      "time/epoch (s)                         99.5758\n",
      "time/total (s)                       2166.43\n",
      "Epoch                                  21\n",
      "---------------------------------  ---------------\n",
      "2024-11-01 23:42:28.055946 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 22 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 125000\n",
      "trainer/ZF1 Loss                      118.65\n",
      "trainer/ZF2 Loss                       97.5405\n",
      "trainer/ZF Expert Reward               19.6559\n",
      "trainer/ZF Policy Reward                3.50557\n",
      "trainer/ZF CHI2 Term                  117.545\n",
      "trainer/Policy Loss                  -392.929\n",
      "trainer/expert_lambda Loss             14.828\n",
      "trainer/expert_lambda Value            19.5113\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               85.7692\n",
      "trainer/Policy Param Norm              29.8562\n",
      "trainer/Zf1 Grad Norm               14526.7\n",
      "trainer/Zf1 Param Norm                 95.8439\n",
      "trainer/Zf2 Grad Norm               12475.7\n",
      "trainer/Zf2 Param Norm                 94.7959\n",
      "trainer/Z Expert Predictions Mean    1026.16\n",
      "trainer/Z Expert Predictions Std       61.5077\n",
      "trainer/Z Expert Predictions Max     1085.57\n",
      "trainer/Z Expert Predictions Min      669.212\n",
      "trainer/Z Policy Predictions Mean     386.281\n",
      "trainer/Z Policy Predictions Std      324.438\n",
      "trainer/Z Policy Predictions Max     1037.12\n",
      "trainer/Z Policy Predictions Min        0.786112\n",
      "trainer/Z Expert Targets Mean        1006.5\n",
      "trainer/Z Expert Targets Std           61.2035\n",
      "trainer/Z Expert Targets Max         1068.12\n",
      "trainer/Z Expert Targets Min          662.78\n",
      "trainer/Z Policy Targets Mean         382.775\n",
      "trainer/Z Policy Targets Std          321.874\n",
      "trainer/Z Policy Targets Max         1007.4\n",
      "trainer/Z Policy Targets Min           -3.08101\n",
      "trainer/Log Pis Mean                   18.9167\n",
      "trainer/Log Pis Std                     9.83329\n",
      "trainer/Policy mu Mean                  0.359322\n",
      "trainer/Policy mu Std                   1.29444\n",
      "trainer/Policy log std Mean            -1.29292\n",
      "trainer/Policy log std Std              0.739597\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        123790\n",
      "exploration/num paths total          1390\n",
      "evaluation/num steps total          43131\n",
      "evaluation/num paths total            233\n",
      "evaluation/path length Mean           186\n",
      "evaluation/path length Std             12.0748\n",
      "evaluation/path length Max            211\n",
      "evaluation/path length Min            163\n",
      "evaluation/Rewards Mean                 5.36184\n",
      "evaluation/Rewards Std                  0.286043\n",
      "evaluation/Rewards Max                  6.51057\n",
      "evaluation/Rewards Min                  4.85321\n",
      "evaluation/Returns Mean               997.303\n",
      "evaluation/Returns Std                 66.9142\n",
      "evaluation/Returns Max               1130.56\n",
      "evaluation/Returns Min                870.371\n",
      "evaluation/Estimation Bias Mean       609.241\n",
      "evaluation/Estimation Bias Std        408.237\n",
      "evaluation/EB/Q_True Mean              36.2089\n",
      "evaluation/EB/Q_True Std              109.996\n",
      "evaluation/EB/Q_Pred Mean             645.449\n",
      "evaluation/EB/Q_Pred Std              403.659\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns            997.303\n",
      "evaluation/Actions Mean                 0.100346\n",
      "evaluation/Actions Std                  0.596328\n",
      "evaluation/Actions Max                  0.998593\n",
      "evaluation/Actions Min                 -0.999847\n",
      "time/backward_policy (s)                8.47572\n",
      "time/backward_zf1 (s)                  10.1526\n",
      "time/backward_zf2 (s)                   9.48407\n",
      "time/data sampling (s)                  2.17039\n",
      "time/data storing (s)                   0.0903813\n",
      "time/evaluation sampling (s)            0.737202\n",
      "time/exploration sampling (s)           2.51749\n",
      "time/logging (s)                        0.00266557\n",
      "time/preback_alpha (s)                  0.0065686\n",
      "time/preback_policy (s)                16.0021\n",
      "time/preback_start (s)                  1.08292\n",
      "time/preback_zf (s)                    37.592\n",
      "time/saving (s)                         2.836e-06\n",
      "time/training (s)                      11.7526\n",
      "time/epoch (s)                        100.067\n",
      "time/total (s)                       2266.5\n",
      "Epoch                                  22\n",
      "---------------------------------  ---------------\n",
      "2024-11-01 23:44:09.258278 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 23 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 130000\n",
      "trainer/ZF1 Loss                      233.661\n",
      "trainer/ZF2 Loss                      211.592\n",
      "trainer/ZF Expert Reward               21.6571\n",
      "trainer/ZF Policy Reward                5.50416\n",
      "trainer/ZF CHI2 Term                  233.35\n",
      "trainer/Policy Loss                  -462.061\n",
      "trainer/expert_lambda Loss             33.7845\n",
      "trainer/expert_lambda Value            19.8321\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               97.8307\n",
      "trainer/Policy Param Norm              30.3596\n",
      "trainer/Zf1 Grad Norm               16971.9\n",
      "trainer/Zf1 Param Norm                 98.6495\n",
      "trainer/Zf2 Grad Norm               15956.5\n",
      "trainer/Zf2 Param Norm                 97.5387\n",
      "trainer/Z Expert Predictions Mean    1098.5\n",
      "trainer/Z Expert Predictions Std       88.7474\n",
      "trainer/Z Expert Predictions Max     1160.25\n",
      "trainer/Z Expert Predictions Min       20.2309\n",
      "trainer/Z Policy Predictions Mean     452.533\n",
      "trainer/Z Policy Predictions Std      366.632\n",
      "trainer/Z Policy Predictions Max     1070.86\n",
      "trainer/Z Policy Predictions Min       -1.90006\n",
      "trainer/Z Expert Targets Mean        1076.84\n",
      "trainer/Z Expert Targets Std           87.7529\n",
      "trainer/Z Expert Targets Max         1139.26\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean         447.029\n",
      "trainer/Z Policy Targets Std          365.576\n",
      "trainer/Z Policy Targets Max         1069.88\n",
      "trainer/Z Policy Targets Min           -9.28233\n",
      "trainer/Log Pis Mean                   19.0954\n",
      "trainer/Log Pis Std                     8.39698\n",
      "trainer/Policy mu Mean                  0.28574\n",
      "trainer/Policy mu Std                   1.28445\n",
      "trainer/Policy log std Mean            -1.37972\n",
      "trainer/Policy log std Std              0.750124\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        127930\n",
      "exploration/num paths total          1410\n",
      "evaluation/num steps total          53131\n",
      "evaluation/num paths total            243\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.36819\n",
      "evaluation/Rewards Std                  0.0803919\n",
      "evaluation/Rewards Max                  5.5444\n",
      "evaluation/Rewards Min                  4.85677\n",
      "evaluation/Returns Mean              5368.19\n",
      "evaluation/Returns Std                 10.6527\n",
      "evaluation/Returns Max               5384.93\n",
      "evaluation/Returns Min               5351.95\n",
      "evaluation/Estimation Bias Mean      1089.36\n",
      "evaluation/Estimation Bias Std        160.573\n",
      "evaluation/EB/Q_True Mean              48.5283\n",
      "evaluation/EB/Q_True Std              149.466\n",
      "evaluation/EB/Q_Pred Mean            1137.89\n",
      "evaluation/EB/Q_Pred Std               60.2912\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5368.19\n",
      "evaluation/Actions Mean                 0.0706985\n",
      "evaluation/Actions Std                  0.516574\n",
      "evaluation/Actions Max                  0.998714\n",
      "evaluation/Actions Min                 -0.999693\n",
      "time/backward_policy (s)                8.26705\n",
      "time/backward_zf1 (s)                   9.88254\n",
      "time/backward_zf2 (s)                   9.31678\n",
      "time/data sampling (s)                  1.81191\n",
      "time/data storing (s)                   0.0903461\n",
      "time/evaluation sampling (s)            3.21258\n",
      "time/exploration sampling (s)           2.44263\n",
      "time/logging (s)                        0.0138212\n",
      "time/preback_alpha (s)                  0.00643364\n",
      "time/preback_policy (s)                16.0019\n",
      "time/preback_start (s)                  1.04446\n",
      "time/preback_zf (s)                    37.2569\n",
      "time/saving (s)                         3.574e-06\n",
      "time/training (s)                      11.645\n",
      "time/epoch (s)                        100.992\n",
      "time/total (s)                       2367.5\n",
      "Epoch                                  23\n",
      "---------------------------------  ---------------\n",
      "2024-11-01 23:46:01.924523 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 24 finished\n",
      "---------------------------------  ----------------\n",
      "replay_buffer/size                 135000\n",
      "trainer/ZF1 Loss                      125.091\n",
      "trainer/ZF2 Loss                      149.554\n",
      "trainer/ZF Expert Reward               21.2033\n",
      "trainer/ZF Policy Reward                4.68361\n",
      "trainer/ZF CHI2 Term                  147.106\n",
      "trainer/Policy Loss                  -432.901\n",
      "trainer/expert_lambda Loss             11.9821\n",
      "trainer/expert_lambda Value            20.1616\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              114.489\n",
      "trainer/Policy Param Norm              30.8303\n",
      "trainer/Zf1 Grad Norm               18955.3\n",
      "trainer/Zf1 Param Norm                101.232\n",
      "trainer/Zf2 Grad Norm               23917.8\n",
      "trainer/Zf2 Param Norm                100.051\n",
      "trainer/Z Expert Predictions Mean    1148.3\n",
      "trainer/Z Expert Predictions Std       92.1809\n",
      "trainer/Z Expert Predictions Max     1225.62\n",
      "trainer/Z Expert Predictions Min      -15.0747\n",
      "trainer/Z Policy Predictions Mean     425.566\n",
      "trainer/Z Policy Predictions Std      384.173\n",
      "trainer/Z Policy Predictions Max     1151.5\n",
      "trainer/Z Policy Predictions Min      -14.0492\n",
      "trainer/Z Expert Targets Mean        1127.1\n",
      "trainer/Z Expert Targets Std           92.2494\n",
      "trainer/Z Expert Targets Max         1202.09\n",
      "trainer/Z Expert Targets Min          -34.3068\n",
      "trainer/Z Policy Targets Mean         420.882\n",
      "trainer/Z Policy Targets Std          385.202\n",
      "trainer/Z Policy Targets Max         1134.08\n",
      "trainer/Z Policy Targets Min          -15.9775\n",
      "trainer/Log Pis Mean                   19.7278\n",
      "trainer/Log Pis Std                    10.4089\n",
      "trainer/Policy mu Mean                  0.356329\n",
      "trainer/Policy mu Std                   1.35259\n",
      "trainer/Policy log std Mean            -1.28604\n",
      "trainer/Policy log std Std              0.796441\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        128719\n",
      "exploration/num paths total          1413\n",
      "evaluation/num steps total          61343\n",
      "evaluation/num paths total            253\n",
      "evaluation/path length Mean           821.2\n",
      "evaluation/path length Std            357.663\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             91\n",
      "evaluation/Rewards Mean                 5.31128\n",
      "evaluation/Rewards Std                  0.092788\n",
      "evaluation/Rewards Max                  6.54324\n",
      "evaluation/Rewards Min                  4.86017\n",
      "evaluation/Returns Mean              4361.62\n",
      "evaluation/Returns Std               1893.44\n",
      "evaluation/Returns Max               5349.16\n",
      "evaluation/Returns Min                495.78\n",
      "evaluation/Estimation Bias Mean      1073.75\n",
      "evaluation/Estimation Bias Std        276.301\n",
      "evaluation/EB/Q_True Mean              58.2211\n",
      "evaluation/EB/Q_True Std              160.488\n",
      "evaluation/EB/Q_Pred Mean            1131.97\n",
      "evaluation/EB/Q_Pred Std              160.09\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4361.62\n",
      "evaluation/Actions Mean                 0.0581389\n",
      "evaluation/Actions Std                  0.517441\n",
      "evaluation/Actions Max                  0.999762\n",
      "evaluation/Actions Min                 -0.999975\n",
      "time/backward_policy (s)                9.6009\n",
      "time/backward_zf1 (s)                  12.3086\n",
      "time/backward_zf2 (s)                  11.4439\n",
      "time/data sampling (s)                  2.1506\n",
      "time/data storing (s)                   0.101033\n",
      "time/evaluation sampling (s)            4.68547\n",
      "time/exploration sampling (s)           2.76514\n",
      "time/logging (s)                        0.0110144\n",
      "time/preback_alpha (s)                  0.00725011\n",
      "time/preback_policy (s)                15.9032\n",
      "time/preback_start (s)                  1.19867\n",
      "time/preback_zf (s)                    40.1734\n",
      "time/saving (s)                         3.21701e-06\n",
      "time/training (s)                      12.0396\n",
      "time/epoch (s)                        112.389\n",
      "time/total (s)                       2479.89\n",
      "Epoch                                  24\n",
      "---------------------------------  ----------------\n",
      "2024-11-01 23:47:50.254454 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 25 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 140000\n",
      "trainer/ZF1 Loss                      138.311\n",
      "trainer/ZF2 Loss                      119.465\n",
      "trainer/ZF Expert Reward               19.7294\n",
      "trainer/ZF Policy Reward                1.21694\n",
      "trainer/ZF CHI2 Term                  136.673\n",
      "trainer/Policy Loss                  -549.183\n",
      "trainer/expert_lambda Loss              9.58845\n",
      "trainer/expert_lambda Value            20.4898\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              132.918\n",
      "trainer/Policy Param Norm              31.3253\n",
      "trainer/Zf1 Grad Norm               26283.8\n",
      "trainer/Zf1 Param Norm                103.615\n",
      "trainer/Zf2 Grad Norm               19389.5\n",
      "trainer/Zf2 Param Norm                102.351\n",
      "trainer/Z Expert Predictions Mean    1196.66\n",
      "trainer/Z Expert Predictions Std       92.5739\n",
      "trainer/Z Expert Predictions Max     1268.25\n",
      "trainer/Z Expert Predictions Min      -27.4392\n",
      "trainer/Z Policy Predictions Mean     535.339\n",
      "trainer/Z Policy Predictions Std      417.773\n",
      "trainer/Z Policy Predictions Max     1169.75\n",
      "trainer/Z Policy Predictions Min        4.75599\n",
      "trainer/Z Expert Targets Mean        1176.93\n",
      "trainer/Z Expert Targets Std           92.8459\n",
      "trainer/Z Expert Targets Max         1252.95\n",
      "trainer/Z Expert Targets Min          -60.6645\n",
      "trainer/Z Policy Targets Mean         534.122\n",
      "trainer/Z Policy Targets Std          419.178\n",
      "trainer/Z Policy Targets Max         1165.48\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   20.7161\n",
      "trainer/Log Pis Std                    10.0994\n",
      "trainer/Policy mu Mean                  0.244018\n",
      "trainer/Policy mu Std                   1.29621\n",
      "trainer/Policy log std Mean            -1.4777\n",
      "trainer/Policy log std Std              0.771434\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        136033\n",
      "exploration/num paths total          1423\n",
      "evaluation/num steps total          67121\n",
      "evaluation/num paths total            266\n",
      "evaluation/path length Mean           444.462\n",
      "evaluation/path length Std            439.203\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             93\n",
      "evaluation/Rewards Mean                 5.34283\n",
      "evaluation/Rewards Std                  0.171195\n",
      "evaluation/Rewards Max                  6.5146\n",
      "evaluation/Rewards Min                  4.84719\n",
      "evaluation/Returns Mean              2374.68\n",
      "evaluation/Returns Std               2341.62\n",
      "evaluation/Returns Max               5343.32\n",
      "evaluation/Returns Min                500.787\n",
      "evaluation/Estimation Bias Mean      1019.13\n",
      "evaluation/Estimation Bias Std        468.894\n",
      "evaluation/EB/Q_True Mean              83.3567\n",
      "evaluation/EB/Q_True Std              187.485\n",
      "evaluation/EB/Q_Pred Mean            1102.49\n",
      "evaluation/EB/Q_Pred Std              303.548\n",
      "evaluation/Num Paths                   13\n",
      "evaluation/Average Returns           2374.68\n",
      "evaluation/Actions Mean                 0.0660821\n",
      "evaluation/Actions Std                  0.540343\n",
      "evaluation/Actions Max                  0.999675\n",
      "evaluation/Actions Min                 -0.999872\n",
      "time/backward_policy (s)                9.45318\n",
      "time/backward_zf1 (s)                  11.4377\n",
      "time/backward_zf2 (s)                  10.7836\n",
      "time/data sampling (s)                  2.31557\n",
      "time/data storing (s)                   0.0975893\n",
      "time/evaluation sampling (s)            3.90406\n",
      "time/exploration sampling (s)           2.63974\n",
      "time/logging (s)                        0.00832357\n",
      "time/preback_alpha (s)                  0.00688035\n",
      "time/preback_policy (s)                15.4973\n",
      "time/preback_start (s)                  1.11014\n",
      "time/preback_zf (s)                    38.9614\n",
      "time/saving (s)                         3.207e-06\n",
      "time/training (s)                      11.863\n",
      "time/epoch (s)                        108.079\n",
      "time/total (s)                       2587.97\n",
      "Epoch                                  25\n",
      "---------------------------------  ---------------\n",
      "2024-11-01 23:49:34.828914 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 26 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 145000\n",
      "trainer/ZF1 Loss                      195.186\n",
      "trainer/ZF2 Loss                      157.235\n",
      "trainer/ZF Expert Reward               21.8594\n",
      "trainer/ZF Policy Reward                1.77477\n",
      "trainer/ZF CHI2 Term                  185.667\n",
      "trainer/Policy Loss                  -544.002\n",
      "trainer/expert_lambda Loss             12.7607\n",
      "trainer/expert_lambda Value            20.7914\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              160.944\n",
      "trainer/Policy Param Norm              31.7941\n",
      "trainer/Zf1 Grad Norm               42680.9\n",
      "trainer/Zf1 Param Norm                105.98\n",
      "trainer/Zf2 Grad Norm               26801.6\n",
      "trainer/Zf2 Param Norm                104.587\n",
      "trainer/Z Expert Predictions Mean    1247.1\n",
      "trainer/Z Expert Predictions Std       56.6518\n",
      "trainer/Z Expert Predictions Max     1316.47\n",
      "trainer/Z Expert Predictions Min      916.227\n",
      "trainer/Z Policy Predictions Mean     534.267\n",
      "trainer/Z Policy Predictions Std      464.089\n",
      "trainer/Z Policy Predictions Max     1258.98\n",
      "trainer/Z Policy Predictions Min      -15.6493\n",
      "trainer/Z Expert Targets Mean        1225.24\n",
      "trainer/Z Expert Targets Std           56.8727\n",
      "trainer/Z Expert Targets Max         1299.07\n",
      "trainer/Z Expert Targets Min          902.458\n",
      "trainer/Z Policy Targets Mean         532.492\n",
      "trainer/Z Policy Targets Std          462.879\n",
      "trainer/Z Policy Targets Max         1241.59\n",
      "trainer/Z Policy Targets Min          -14.442\n",
      "trainer/Log Pis Mean                   21.7752\n",
      "trainer/Log Pis Std                     9.89007\n",
      "trainer/Policy mu Mean                  0.273071\n",
      "trainer/Policy mu Std                   1.36662\n",
      "trainer/Policy log std Mean            -1.42956\n",
      "trainer/Policy log std Std              0.83675\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        139203\n",
      "exploration/num paths total          1427\n",
      "evaluation/num steps total          75806\n",
      "evaluation/num paths total            277\n",
      "evaluation/path length Mean           789.545\n",
      "evaluation/path length Std            343.827\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            131\n",
      "evaluation/Rewards Mean                 5.26274\n",
      "evaluation/Rewards Std                  0.138192\n",
      "evaluation/Rewards Max                  6.02351\n",
      "evaluation/Rewards Min                  3.4625\n",
      "evaluation/Returns Mean              4155.17\n",
      "evaluation/Returns Std               1825.76\n",
      "evaluation/Returns Max               5291.68\n",
      "evaluation/Returns Min                710.532\n",
      "evaluation/Estimation Bias Mean      1161.79\n",
      "evaluation/Estimation Bias Std        333.042\n",
      "evaluation/EB/Q_True Mean              54.9363\n",
      "evaluation/EB/Q_True Std              156.417\n",
      "evaluation/EB/Q_Pred Mean            1216.72\n",
      "evaluation/EB/Q_Pred Std              220.085\n",
      "evaluation/Num Paths                   11\n",
      "evaluation/Average Returns           4155.17\n",
      "evaluation/Actions Mean                 0.0659065\n",
      "evaluation/Actions Std                  0.508699\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999996\n",
      "time/backward_policy (s)                8.52386\n",
      "time/backward_zf1 (s)                  10.285\n",
      "time/backward_zf2 (s)                   9.72408\n",
      "time/data sampling (s)                  2.01798\n",
      "time/data storing (s)                   0.0913567\n",
      "time/evaluation sampling (s)            4.35513\n",
      "time/exploration sampling (s)           2.54789\n",
      "time/logging (s)                        0.0117763\n",
      "time/preback_alpha (s)                  0.00645658\n",
      "time/preback_policy (s)                16.1813\n",
      "time/preback_start (s)                  1.02875\n",
      "time/preback_zf (s)                    37.8766\n",
      "time/saving (s)                         3.4e-06\n",
      "time/training (s)                      11.6996\n",
      "time/epoch (s)                        104.35\n",
      "time/total (s)                       2692.33\n",
      "Epoch                                  26\n",
      "---------------------------------  ---------------\n",
      "2024-11-01 23:51:16.405139 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 27 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 150000\n",
      "trainer/ZF1 Loss                      107.919\n",
      "trainer/ZF2 Loss                      129.916\n",
      "trainer/ZF Expert Reward               25.5705\n",
      "trainer/ZF Policy Reward                4.88508\n",
      "trainer/ZF CHI2 Term                  131.278\n",
      "trainer/Policy Loss                  -612.614\n",
      "trainer/expert_lambda Loss             21.2856\n",
      "trainer/expert_lambda Value            21.0442\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              126.279\n",
      "trainer/Policy Param Norm              32.2997\n",
      "trainer/Zf1 Grad Norm               10717.6\n",
      "trainer/Zf1 Param Norm                108.275\n",
      "trainer/Zf2 Grad Norm               15128.2\n",
      "trainer/Zf2 Param Norm                106.709\n",
      "trainer/Z Expert Predictions Mean    1324.98\n",
      "trainer/Z Expert Predictions Std       98.6202\n",
      "trainer/Z Expert Predictions Max     1392.06\n",
      "trainer/Z Expert Predictions Min       17.3453\n",
      "trainer/Z Policy Predictions Mean     603.558\n",
      "trainer/Z Policy Predictions Std      488.028\n",
      "trainer/Z Policy Predictions Max     1327.32\n",
      "trainer/Z Policy Predictions Min       -0.482142\n",
      "trainer/Z Expert Targets Mean        1299.41\n",
      "trainer/Z Expert Targets Std           98.3543\n",
      "trainer/Z Expert Targets Max         1372.32\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean         598.673\n",
      "trainer/Z Policy Targets Std          484.816\n",
      "trainer/Z Policy Targets Max         1312.92\n",
      "trainer/Z Policy Targets Min           -6.52986\n",
      "trainer/Log Pis Mean                   23.032\n",
      "trainer/Log Pis Std                     8.38375\n",
      "trainer/Policy mu Mean                  0.245583\n",
      "trainer/Policy mu Std                   1.4139\n",
      "trainer/Policy log std Mean            -1.45294\n",
      "trainer/Policy log std Std              0.904741\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        145203\n",
      "exploration/num paths total          1433\n",
      "evaluation/num steps total          85732\n",
      "evaluation/num paths total            287\n",
      "evaluation/path length Mean           992.6\n",
      "evaluation/path length Std             22.2\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            926\n",
      "evaluation/Rewards Mean                 5.38769\n",
      "evaluation/Rewards Std                  0.121827\n",
      "evaluation/Rewards Max                  5.75305\n",
      "evaluation/Rewards Min                  4.11163\n",
      "evaluation/Returns Mean              5347.82\n",
      "evaluation/Returns Std                135.63\n",
      "evaluation/Returns Max               5413.21\n",
      "evaluation/Returns Min               4941.95\n",
      "evaluation/Estimation Bias Mean      1293.43\n",
      "evaluation/Estimation Bias Std        221.427\n",
      "evaluation/EB/Q_True Mean              49.1579\n",
      "evaluation/EB/Q_True Std              150.731\n",
      "evaluation/EB/Q_Pred Mean            1342.59\n",
      "evaluation/EB/Q_Pred Std              138.217\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5347.82\n",
      "evaluation/Actions Mean                 0.0530353\n",
      "evaluation/Actions Std                  0.537589\n",
      "evaluation/Actions Max                  0.999953\n",
      "evaluation/Actions Min                 -0.999988\n",
      "time/backward_policy (s)                8.78646\n",
      "time/backward_zf1 (s)                  10.1879\n",
      "time/backward_zf2 (s)                   9.89451\n",
      "time/data sampling (s)                  1.92368\n",
      "time/data storing (s)                   0.0853407\n",
      "time/evaluation sampling (s)            4.55264\n",
      "time/exploration sampling (s)           2.51357\n",
      "time/logging (s)                        0.0174981\n",
      "time/preback_alpha (s)                  0.0062873\n",
      "time/preback_policy (s)                14.8533\n",
      "time/preback_start (s)                  0.995435\n",
      "time/preback_zf (s)                    36.5338\n",
      "time/saving (s)                         3.131e-06\n",
      "time/training (s)                      11.0218\n",
      "time/epoch (s)                        101.372\n",
      "time/total (s)                       2793.71\n",
      "Epoch                                  27\n",
      "---------------------------------  --------------\n",
      "2024-11-01 23:52:58.687990 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 28 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 155000\n",
      "trainer/ZF1 Loss                      137.928\n",
      "trainer/ZF2 Loss                      130.602\n",
      "trainer/ZF Expert Reward               20.6312\n",
      "trainer/ZF Policy Reward                5.58285\n",
      "trainer/ZF CHI2 Term                  141.128\n",
      "trainer/Policy Loss                  -643.852\n",
      "trainer/expert_lambda Loss             14.5461\n",
      "trainer/expert_lambda Value            21.2674\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              157.317\n",
      "trainer/Policy Param Norm              32.7723\n",
      "trainer/Zf1 Grad Norm               12550\n",
      "trainer/Zf1 Param Norm                110.519\n",
      "trainer/Zf2 Grad Norm               15051.3\n",
      "trainer/Zf2 Param Norm                108.834\n",
      "trainer/Z Expert Predictions Mean    1380.79\n",
      "trainer/Z Expert Predictions Std      110.353\n",
      "trainer/Z Expert Predictions Max     1456.34\n",
      "trainer/Z Expert Predictions Min      -55.106\n",
      "trainer/Z Policy Predictions Mean     636.571\n",
      "trainer/Z Policy Predictions Std      506.852\n",
      "trainer/Z Policy Predictions Max     1374.23\n",
      "trainer/Z Policy Predictions Min       -6.71575\n",
      "trainer/Z Expert Targets Mean        1360.16\n",
      "trainer/Z Expert Targets Std          110.568\n",
      "trainer/Z Expert Targets Max         1441.96\n",
      "trainer/Z Expert Targets Min          -85.5909\n",
      "trainer/Z Policy Targets Mean         630.988\n",
      "trainer/Z Policy Targets Std          506.108\n",
      "trainer/Z Policy Targets Max         1362.84\n",
      "trainer/Z Policy Targets Min           -3.81434\n",
      "trainer/Log Pis Mean                   22.6296\n",
      "trainer/Log Pis Std                     8.54891\n",
      "trainer/Policy mu Mean                  0.27809\n",
      "trainer/Policy mu Std                   1.36817\n",
      "trainer/Policy log std Mean            -1.47809\n",
      "trainer/Policy log std Std              0.856387\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        150851\n",
      "exploration/num paths total          1442\n",
      "evaluation/num steps total          95732\n",
      "evaluation/num paths total            297\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.35149\n",
      "evaluation/Rewards Std                  0.0701886\n",
      "evaluation/Rewards Max                  5.69839\n",
      "evaluation/Rewards Min                  4.86292\n",
      "evaluation/Returns Mean              5351.49\n",
      "evaluation/Returns Std                  4.75159\n",
      "evaluation/Returns Max               5358.22\n",
      "evaluation/Returns Min               5344.48\n",
      "evaluation/Estimation Bias Mean      1261.4\n",
      "evaluation/Estimation Bias Std        165.009\n",
      "evaluation/EB/Q_True Mean              48.343\n",
      "evaluation/EB/Q_True Std              148.894\n",
      "evaluation/EB/Q_Pred Mean            1309.74\n",
      "evaluation/EB/Q_Pred Std               70.4949\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5351.49\n",
      "evaluation/Actions Mean                 0.0556218\n",
      "evaluation/Actions Std                  0.505045\n",
      "evaluation/Actions Max                  0.999329\n",
      "evaluation/Actions Min                 -0.99982\n",
      "time/backward_policy (s)                8.93313\n",
      "time/backward_zf1 (s)                  10.3296\n",
      "time/backward_zf2 (s)                  10.0481\n",
      "time/data sampling (s)                  1.84942\n",
      "time/data storing (s)                   0.0873244\n",
      "time/evaluation sampling (s)            4.81329\n",
      "time/exploration sampling (s)           2.51753\n",
      "time/logging (s)                        0.0126729\n",
      "time/preback_alpha (s)                  0.00628261\n",
      "time/preback_policy (s)                14.7902\n",
      "time/preback_start (s)                  1.01004\n",
      "time/preback_zf (s)                    36.6642\n",
      "time/saving (s)                         3.002e-06\n",
      "time/training (s)                      11.0022\n",
      "time/epoch (s)                        102.064\n",
      "time/total (s)                       2895.78\n",
      "Epoch                                  28\n",
      "---------------------------------  ---------------\n",
      "2024-11-01 23:54:39.126924 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 29 finished\n",
      "---------------------------------  ----------------\n",
      "replay_buffer/size                 160000\n",
      "trainer/ZF1 Loss                      159.177\n",
      "trainer/ZF2 Loss                      171.868\n",
      "trainer/ZF Expert Reward               23.105\n",
      "trainer/ZF Policy Reward                3.95665\n",
      "trainer/ZF CHI2 Term                  174.347\n",
      "trainer/Policy Loss                  -658.144\n",
      "trainer/expert_lambda Loss             15.3178\n",
      "trainer/expert_lambda Value            21.47\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              138.258\n",
      "trainer/Policy Param Norm              33.1637\n",
      "trainer/Zf1 Grad Norm               12641.4\n",
      "trainer/Zf1 Param Norm                112.644\n",
      "trainer/Zf2 Grad Norm               22268.2\n",
      "trainer/Zf2 Param Norm                110.862\n",
      "trainer/Z Expert Predictions Mean    1433.25\n",
      "trainer/Z Expert Predictions Std       85.2515\n",
      "trainer/Z Expert Predictions Max     1518.09\n",
      "trainer/Z Expert Predictions Min      988.532\n",
      "trainer/Z Policy Predictions Mean     649.668\n",
      "trainer/Z Policy Predictions Std      516.431\n",
      "trainer/Z Policy Predictions Max     1424.73\n",
      "trainer/Z Policy Predictions Min      -15.225\n",
      "trainer/Z Expert Targets Mean        1410.15\n",
      "trainer/Z Expert Targets Std           85.9771\n",
      "trainer/Z Expert Targets Max         1496.16\n",
      "trainer/Z Expert Targets Min          968.744\n",
      "trainer/Z Policy Targets Mean         645.711\n",
      "trainer/Z Policy Targets Std          517.532\n",
      "trainer/Z Policy Targets Max         1426.23\n",
      "trainer/Z Policy Targets Min          -14.7354\n",
      "trainer/Log Pis Mean                   23.0828\n",
      "trainer/Log Pis Std                     9.47928\n",
      "trainer/Policy mu Mean                  0.333571\n",
      "trainer/Policy mu Std                   1.41319\n",
      "trainer/Policy log std Mean            -1.46579\n",
      "trainer/Policy log std Std              0.885828\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        154851\n",
      "exploration/num paths total          1446\n",
      "evaluation/num steps total         104849\n",
      "evaluation/num paths total            307\n",
      "evaluation/path length Mean           911.7\n",
      "evaluation/path length Std            264.9\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            117\n",
      "evaluation/Rewards Mean                 5.28723\n",
      "evaluation/Rewards Std                  0.0682485\n",
      "evaluation/Rewards Max                  5.83394\n",
      "evaluation/Rewards Min                  4.85719\n",
      "evaluation/Returns Mean              4820.37\n",
      "evaluation/Returns Std               1398.25\n",
      "evaluation/Returns Max               5296.21\n",
      "evaluation/Returns Min                625.652\n",
      "evaluation/Estimation Bias Mean      1399\n",
      "evaluation/Estimation Bias Std        217.768\n",
      "evaluation/EB/Q_True Mean              52.2983\n",
      "evaluation/EB/Q_True Std              153.003\n",
      "evaluation/EB/Q_Pred Mean            1451.29\n",
      "evaluation/EB/Q_Pred Std              126.234\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4820.37\n",
      "evaluation/Actions Mean                 0.0490844\n",
      "evaluation/Actions Std                  0.514469\n",
      "evaluation/Actions Max                  0.999648\n",
      "evaluation/Actions Min                 -0.999159\n",
      "time/backward_policy (s)                8.35262\n",
      "time/backward_zf1 (s)                   9.67199\n",
      "time/backward_zf2 (s)                   9.38369\n",
      "time/data sampling (s)                  1.73443\n",
      "time/data storing (s)                   0.0843085\n",
      "time/evaluation sampling (s)            4.84332\n",
      "time/exploration sampling (s)           2.50357\n",
      "time/logging (s)                        0.0113566\n",
      "time/preback_alpha (s)                  0.00613015\n",
      "time/preback_policy (s)                15.2603\n",
      "time/preback_start (s)                  0.972537\n",
      "time/preback_zf (s)                    36.3639\n",
      "time/saving (s)                         2.77199e-06\n",
      "time/training (s)                      11.0402\n",
      "time/epoch (s)                        100.228\n",
      "time/total (s)                       2996.01\n",
      "Epoch                                  29\n",
      "---------------------------------  ----------------\n",
      "2024-11-01 23:56:19.508446 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 30 finished\n",
      "---------------------------------  ----------------\n",
      "replay_buffer/size                 165000\n",
      "trainer/ZF1 Loss                      238.088\n",
      "trainer/ZF2 Loss                      229.113\n",
      "trainer/ZF Expert Reward               22.1116\n",
      "trainer/ZF Policy Reward                7.33257\n",
      "trainer/ZF CHI2 Term                  240.95\n",
      "trainer/Policy Loss                  -759.672\n",
      "trainer/expert_lambda Loss             26.571\n",
      "trainer/expert_lambda Value            21.6776\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              152.694\n",
      "trainer/Policy Param Norm              33.5649\n",
      "trainer/Zf1 Grad Norm               23330.1\n",
      "trainer/Zf1 Param Norm                114.687\n",
      "trainer/Zf2 Grad Norm               23185.8\n",
      "trainer/Zf2 Param Norm                112.802\n",
      "trainer/Z Expert Predictions Mean    1483.62\n",
      "trainer/Z Expert Predictions Std       79.8892\n",
      "trainer/Z Expert Predictions Max     1574.2\n",
      "trainer/Z Expert Predictions Min     1013.35\n",
      "trainer/Z Policy Predictions Mean     745.672\n",
      "trainer/Z Policy Predictions Std      523.835\n",
      "trainer/Z Policy Predictions Max     1532.51\n",
      "trainer/Z Policy Predictions Min      -12.5998\n",
      "trainer/Z Expert Targets Mean        1461.51\n",
      "trainer/Z Expert Targets Std           79.6431\n",
      "trainer/Z Expert Targets Max         1560.26\n",
      "trainer/Z Expert Targets Min          991.507\n",
      "trainer/Z Policy Targets Mean         738.34\n",
      "trainer/Z Policy Targets Std          526.083\n",
      "trainer/Z Policy Targets Max         1521.79\n",
      "trainer/Z Policy Targets Min          -13.7346\n",
      "trainer/Log Pis Mean                   24.7461\n",
      "trainer/Log Pis Std                     9.42214\n",
      "trainer/Policy mu Mean                  0.340341\n",
      "trainer/Policy mu Std                   1.36875\n",
      "trainer/Policy log std Mean            -1.62416\n",
      "trainer/Policy log std Std              0.793266\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        160851\n",
      "exploration/num paths total          1452\n",
      "evaluation/num steps total         114242\n",
      "evaluation/num paths total            317\n",
      "evaluation/path length Mean           939.3\n",
      "evaluation/path length Std            182.1\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            393\n",
      "evaluation/Rewards Mean                 5.33645\n",
      "evaluation/Rewards Std                  0.0862286\n",
      "evaluation/Rewards Max                  6.42801\n",
      "evaluation/Rewards Min                  4.85595\n",
      "evaluation/Returns Mean              5012.53\n",
      "evaluation/Returns Std                964.721\n",
      "evaluation/Returns Max               5346.74\n",
      "evaluation/Returns Min               2118.43\n",
      "evaluation/Estimation Bias Mean      1441.09\n",
      "evaluation/Estimation Bias Std        228.888\n",
      "evaluation/EB/Q_True Mean              51.3492\n",
      "evaluation/EB/Q_True Std              152.772\n",
      "evaluation/EB/Q_Pred Mean            1492.43\n",
      "evaluation/EB/Q_Pred Std              134.061\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5012.53\n",
      "evaluation/Actions Mean                 0.0688159\n",
      "evaluation/Actions Std                  0.493771\n",
      "evaluation/Actions Max                  0.999512\n",
      "evaluation/Actions Min                 -0.999892\n",
      "time/backward_policy (s)                8.28886\n",
      "time/backward_zf1 (s)                   9.62687\n",
      "time/backward_zf2 (s)                   9.33142\n",
      "time/data sampling (s)                  1.87526\n",
      "time/data storing (s)                   0.0852158\n",
      "time/evaluation sampling (s)            4.78193\n",
      "time/exploration sampling (s)           2.46308\n",
      "time/logging (s)                        0.0132653\n",
      "time/preback_alpha (s)                  0.0061332\n",
      "time/preback_policy (s)                15.3379\n",
      "time/preback_start (s)                  0.967217\n",
      "time/preback_zf (s)                    36.3633\n",
      "time/saving (s)                         2.78001e-06\n",
      "time/training (s)                      11.0337\n",
      "time/epoch (s)                        100.174\n",
      "time/total (s)                       3096.19\n",
      "Epoch                                  30\n",
      "---------------------------------  ----------------\n",
      "2024-11-01 23:58:00.008379 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 31 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 170000\n",
      "trainer/ZF1 Loss                      189.699\n",
      "trainer/ZF2 Loss                      171.905\n",
      "trainer/ZF Expert Reward               23.9569\n",
      "trainer/ZF Policy Reward                4.22111\n",
      "trainer/ZF CHI2 Term                  189.646\n",
      "trainer/Policy Loss                  -826.07\n",
      "trainer/expert_lambda Loss             27.5459\n",
      "trainer/expert_lambda Value            21.8577\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              146.144\n",
      "trainer/Policy Param Norm              33.8932\n",
      "trainer/Zf1 Grad Norm               16525.8\n",
      "trainer/Zf1 Param Norm                116.662\n",
      "trainer/Zf2 Grad Norm               25346.7\n",
      "trainer/Zf2 Param Norm                114.682\n",
      "trainer/Z Expert Predictions Mean    1516.85\n",
      "trainer/Z Expert Predictions Std       83.3542\n",
      "trainer/Z Expert Predictions Max     1603.17\n",
      "trainer/Z Expert Predictions Min     1048.2\n",
      "trainer/Z Policy Predictions Mean     814.006\n",
      "trainer/Z Policy Predictions Std      534.109\n",
      "trainer/Z Policy Predictions Max     1577\n",
      "trainer/Z Policy Predictions Min      -19.3696\n",
      "trainer/Z Expert Targets Mean        1492.89\n",
      "trainer/Z Expert Targets Std           83.6248\n",
      "trainer/Z Expert Targets Max         1578.64\n",
      "trainer/Z Expert Targets Min         1033.23\n",
      "trainer/Z Policy Targets Mean         809.785\n",
      "trainer/Z Policy Targets Std          533.545\n",
      "trainer/Z Policy Targets Max         1521.64\n",
      "trainer/Z Policy Targets Min          -25.5448\n",
      "trainer/Log Pis Mean                   25.215\n",
      "trainer/Log Pis Std                     8.51337\n",
      "trainer/Policy mu Mean                  0.365444\n",
      "trainer/Policy mu Std                   1.33023\n",
      "trainer/Policy log std Mean            -1.68822\n",
      "trainer/Policy log std Std              0.824087\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        164851\n",
      "exploration/num paths total          1456\n",
      "evaluation/num steps total         122987\n",
      "evaluation/num paths total            327\n",
      "evaluation/path length Mean           874.5\n",
      "evaluation/path length Std            254.023\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            159\n",
      "evaluation/Rewards Mean                 5.30148\n",
      "evaluation/Rewards Std                  0.128408\n",
      "evaluation/Rewards Max                  5.86755\n",
      "evaluation/Rewards Min                  3.59635\n",
      "evaluation/Returns Mean              4636.14\n",
      "evaluation/Returns Std               1366.37\n",
      "evaluation/Returns Max               5348\n",
      "evaluation/Returns Min                797.608\n",
      "evaluation/Estimation Bias Mean      1375.13\n",
      "evaluation/Estimation Bias Std        306.989\n",
      "evaluation/EB/Q_True Mean              55.0618\n",
      "evaluation/EB/Q_True Std              157.377\n",
      "evaluation/EB/Q_Pred Mean            1430.19\n",
      "evaluation/EB/Q_Pred Std              222.48\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4636.14\n",
      "evaluation/Actions Mean                 0.0522138\n",
      "evaluation/Actions Std                  0.483127\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999945\n",
      "time/backward_policy (s)                8.42192\n",
      "time/backward_zf1 (s)                   9.72342\n",
      "time/backward_zf2 (s)                   9.45375\n",
      "time/data sampling (s)                  1.81304\n",
      "time/data storing (s)                   0.0854887\n",
      "time/evaluation sampling (s)            4.66304\n",
      "time/exploration sampling (s)           2.45641\n",
      "time/logging (s)                        0.0136126\n",
      "time/preback_alpha (s)                  0.00619753\n",
      "time/preback_policy (s)                15.2272\n",
      "time/preback_start (s)                  0.973699\n",
      "time/preback_zf (s)                    36.4042\n",
      "time/saving (s)                         3.606e-06\n",
      "time/training (s)                      11.0493\n",
      "time/epoch (s)                        100.291\n",
      "time/total (s)                       3196.49\n",
      "Epoch                                  31\n",
      "---------------------------------  ---------------\n",
      "2024-11-01 23:59:40.471881 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 32 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 175000\n",
      "trainer/ZF1 Loss                      263.178\n",
      "trainer/ZF2 Loss                      260.43\n",
      "trainer/ZF Expert Reward               25.0186\n",
      "trainer/ZF Policy Reward                7.20739\n",
      "trainer/ZF CHI2 Term                  271.69\n",
      "trainer/Policy Loss                  -804.185\n",
      "trainer/expert_lambda Loss             98.1893\n",
      "trainer/expert_lambda Value            21.9994\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              212.657\n",
      "trainer/Policy Param Norm              34.1987\n",
      "trainer/Zf1 Grad Norm               15657.2\n",
      "trainer/Zf1 Param Norm                118.594\n",
      "trainer/Zf2 Grad Norm               15998.1\n",
      "trainer/Zf2 Param Norm                116.546\n",
      "trainer/Z Expert Predictions Mean    1519.44\n",
      "trainer/Z Expert Predictions Std      105.7\n",
      "trainer/Z Expert Predictions Max     1622.14\n",
      "trainer/Z Expert Predictions Min      515.721\n",
      "trainer/Z Policy Predictions Mean     794.428\n",
      "trainer/Z Policy Predictions Std      569.625\n",
      "trainer/Z Policy Predictions Max     1573.98\n",
      "trainer/Z Policy Predictions Min      -11.5576\n",
      "trainer/Z Expert Targets Mean        1494.42\n",
      "trainer/Z Expert Targets Std          113.087\n",
      "trainer/Z Expert Targets Max         1601.57\n",
      "trainer/Z Expert Targets Min          319.313\n",
      "trainer/Z Policy Targets Mean         787.221\n",
      "trainer/Z Policy Targets Std          566.378\n",
      "trainer/Z Policy Targets Max         1525.36\n",
      "trainer/Z Policy Targets Min          -17.1831\n",
      "trainer/Log Pis Mean                   24.5832\n",
      "trainer/Log Pis Std                     8.73147\n",
      "trainer/Policy mu Mean                  0.271339\n",
      "trainer/Policy mu Std                   1.36943\n",
      "trainer/Policy log std Mean            -1.68062\n",
      "trainer/Policy log std Std              0.903194\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        170851\n",
      "exploration/num paths total          1462\n",
      "evaluation/num steps total         132987\n",
      "evaluation/num paths total            337\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.3111\n",
      "evaluation/Rewards Std                  0.0619726\n",
      "evaluation/Rewards Max                  5.47066\n",
      "evaluation/Rewards Min                  4.85368\n",
      "evaluation/Returns Mean              5311.1\n",
      "evaluation/Returns Std                  5.82815\n",
      "evaluation/Returns Max               5322.22\n",
      "evaluation/Returns Min               5300.68\n",
      "evaluation/Estimation Bias Mean      1448.66\n",
      "evaluation/Estimation Bias Std        170.023\n",
      "evaluation/EB/Q_True Mean              48.0134\n",
      "evaluation/EB/Q_True Std              147.878\n",
      "evaluation/EB/Q_Pred Mean            1496.67\n",
      "evaluation/EB/Q_Pred Std               81.16\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5311.1\n",
      "evaluation/Actions Mean                 0.073043\n",
      "evaluation/Actions Std                  0.510686\n",
      "evaluation/Actions Max                  0.998097\n",
      "evaluation/Actions Min                 -0.999536\n",
      "time/backward_policy (s)                8.28999\n",
      "time/backward_zf1 (s)                   9.61224\n",
      "time/backward_zf2 (s)                   9.31341\n",
      "time/data sampling (s)                  1.8509\n",
      "time/data storing (s)                   0.0855492\n",
      "time/evaluation sampling (s)            4.70574\n",
      "time/exploration sampling (s)           2.46617\n",
      "time/logging (s)                        0.0126148\n",
      "time/preback_alpha (s)                  0.00607342\n",
      "time/preback_policy (s)                15.4603\n",
      "time/preback_start (s)                  0.965378\n",
      "time/preback_zf (s)                    36.3455\n",
      "time/saving (s)                         2.802e-06\n",
      "time/training (s)                      11.1418\n",
      "time/epoch (s)                        100.256\n",
      "time/total (s)                       3296.75\n",
      "Epoch                                  32\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:01:21.284798 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 33 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 180000\n",
      "trainer/ZF1 Loss                      200.624\n",
      "trainer/ZF2 Loss                      195.534\n",
      "trainer/ZF Expert Reward               25.5151\n",
      "trainer/ZF Policy Reward                9.06329\n",
      "trainer/ZF CHI2 Term                  208.195\n",
      "trainer/Policy Loss                  -844.07\n",
      "trainer/expert_lambda Loss             32.6695\n",
      "trainer/expert_lambda Value            22.1198\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              271.656\n",
      "trainer/Policy Param Norm              34.5035\n",
      "trainer/Zf1 Grad Norm               18668.4\n",
      "trainer/Zf1 Param Norm                120.61\n",
      "trainer/Zf2 Grad Norm               19242.6\n",
      "trainer/Zf2 Param Norm                118.392\n",
      "trainer/Z Expert Predictions Mean    1546.38\n",
      "trainer/Z Expert Predictions Std       75.9882\n",
      "trainer/Z Expert Predictions Max     1633.82\n",
      "trainer/Z Expert Predictions Min     1106.4\n",
      "trainer/Z Policy Predictions Mean     836.351\n",
      "trainer/Z Policy Predictions Std      584.184\n",
      "trainer/Z Policy Predictions Max     1563\n",
      "trainer/Z Policy Predictions Min      -30.9811\n",
      "trainer/Z Expert Targets Mean        1520.86\n",
      "trainer/Z Expert Targets Std           76.7529\n",
      "trainer/Z Expert Targets Max         1612.27\n",
      "trainer/Z Expert Targets Min         1107.53\n",
      "trainer/Z Policy Targets Mean         827.288\n",
      "trainer/Z Policy Targets Std          582.061\n",
      "trainer/Z Policy Targets Max         1570.91\n",
      "trainer/Z Policy Targets Min          -24.4558\n",
      "trainer/Log Pis Mean                   25.1443\n",
      "trainer/Log Pis Std                     9.74671\n",
      "trainer/Policy mu Mean                  0.321639\n",
      "trainer/Policy mu Std                   1.38652\n",
      "trainer/Policy log std Mean            -1.68432\n",
      "trainer/Policy log std Std              0.954015\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        174851\n",
      "exploration/num paths total          1466\n",
      "evaluation/num steps total         142987\n",
      "evaluation/num paths total            347\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.33524\n",
      "evaluation/Rewards Std                  0.0732781\n",
      "evaluation/Rewards Max                  5.50153\n",
      "evaluation/Rewards Min                  4.8461\n",
      "evaluation/Returns Mean              5335.24\n",
      "evaluation/Returns Std                  4.92296\n",
      "evaluation/Returns Max               5345.71\n",
      "evaluation/Returns Min               5329.53\n",
      "evaluation/Estimation Bias Mean      1472.28\n",
      "evaluation/Estimation Bias Std        170.848\n",
      "evaluation/EB/Q_True Mean              48.1685\n",
      "evaluation/EB/Q_True Std              148.355\n",
      "evaluation/EB/Q_Pred Mean            1520.44\n",
      "evaluation/EB/Q_Pred Std               79.2918\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5335.24\n",
      "evaluation/Actions Mean                 0.0726607\n",
      "evaluation/Actions Std                  0.50688\n",
      "evaluation/Actions Max                  0.999112\n",
      "evaluation/Actions Min                 -0.998083\n",
      "time/backward_policy (s)                8.79563\n",
      "time/backward_zf1 (s)                  10.0814\n",
      "time/backward_zf2 (s)                   9.86552\n",
      "time/data sampling (s)                  1.78725\n",
      "time/data storing (s)                   0.0873843\n",
      "time/evaluation sampling (s)            4.6355\n",
      "time/exploration sampling (s)           2.48513\n",
      "time/logging (s)                        0.012967\n",
      "time/preback_alpha (s)                  0.00614703\n",
      "time/preback_policy (s)                14.6991\n",
      "time/preback_start (s)                  0.989698\n",
      "time/preback_zf (s)                    36.4544\n",
      "time/saving (s)                         2.568e-06\n",
      "time/training (s)                      10.7022\n",
      "time/epoch (s)                        100.602\n",
      "time/total (s)                       3397.36\n",
      "Epoch                                  33\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:03:00.938127 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 34 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 185000\n",
      "trainer/ZF1 Loss                      223.955\n",
      "trainer/ZF2 Loss                      215.149\n",
      "trainer/ZF Expert Reward               22.0602\n",
      "trainer/ZF Policy Reward                5.48733\n",
      "trainer/ZF CHI2 Term                  226.321\n",
      "trainer/Policy Loss                  -833.735\n",
      "trainer/expert_lambda Loss             47.7384\n",
      "trainer/expert_lambda Value            22.2121\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              279.325\n",
      "trainer/Policy Param Norm              34.8157\n",
      "trainer/Zf1 Grad Norm               22207\n",
      "trainer/Zf1 Param Norm                122.456\n",
      "trainer/Zf2 Grad Norm               17290.3\n",
      "trainer/Zf2 Param Norm                120.164\n",
      "trainer/Z Expert Predictions Mean    1529.14\n",
      "trainer/Z Expert Predictions Std      134.992\n",
      "trainer/Z Expert Predictions Max     1643.13\n",
      "trainer/Z Expert Predictions Min     -171.193\n",
      "trainer/Z Policy Predictions Mean     822.673\n",
      "trainer/Z Policy Predictions Std      559.271\n",
      "trainer/Z Policy Predictions Max     1545.63\n",
      "trainer/Z Policy Predictions Min      -42.8866\n",
      "trainer/Z Expert Targets Mean        1507.08\n",
      "trainer/Z Expert Targets Std          133.014\n",
      "trainer/Z Expert Targets Max         1619.37\n",
      "trainer/Z Expert Targets Min         -181.1\n",
      "trainer/Z Policy Targets Mean         817.186\n",
      "trainer/Z Policy Targets Std          560.811\n",
      "trainer/Z Policy Targets Max         1534.13\n",
      "trainer/Z Policy Targets Min          -42.5716\n",
      "trainer/Log Pis Mean                   27.1998\n",
      "trainer/Log Pis Std                     9.33047\n",
      "trainer/Policy mu Mean                  0.330513\n",
      "trainer/Policy mu Std                   1.45475\n",
      "trainer/Policy log std Mean            -1.71261\n",
      "trainer/Policy log std Std              0.893013\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        180851\n",
      "exploration/num paths total          1472\n",
      "evaluation/num steps total         152987\n",
      "evaluation/num paths total            357\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.31079\n",
      "evaluation/Rewards Std                  0.0754615\n",
      "evaluation/Rewards Max                  5.5037\n",
      "evaluation/Rewards Min                  4.87239\n",
      "evaluation/Returns Mean              5310.79\n",
      "evaluation/Returns Std                  5.11341\n",
      "evaluation/Returns Max               5320.43\n",
      "evaluation/Returns Min               5302.92\n",
      "evaluation/Estimation Bias Mean      1459.36\n",
      "evaluation/Estimation Bias Std        169.279\n",
      "evaluation/EB/Q_True Mean              48.0178\n",
      "evaluation/EB/Q_True Std              147.901\n",
      "evaluation/EB/Q_Pred Mean            1507.38\n",
      "evaluation/EB/Q_Pred Std               80.6139\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5310.79\n",
      "evaluation/Actions Mean                 0.0500803\n",
      "evaluation/Actions Std                  0.508175\n",
      "evaluation/Actions Max                  0.998616\n",
      "evaluation/Actions Min                 -0.999046\n",
      "time/backward_policy (s)                7.81785\n",
      "time/backward_zf1 (s)                   9.10368\n",
      "time/backward_zf2 (s)                   8.76581\n",
      "time/data sampling (s)                  1.75627\n",
      "time/data storing (s)                   0.0846832\n",
      "time/evaluation sampling (s)            4.58391\n",
      "time/exploration sampling (s)           2.45432\n",
      "time/logging (s)                        0.0118975\n",
      "time/preback_alpha (s)                  0.00603787\n",
      "time/preback_policy (s)                16.1386\n",
      "time/preback_start (s)                  0.956723\n",
      "time/preback_zf (s)                    36.2915\n",
      "time/saving (s)                         2.608e-06\n",
      "time/training (s)                      11.4746\n",
      "time/epoch (s)                         99.4459\n",
      "time/total (s)                       3496.81\n",
      "Epoch                                  34\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:04:41.331367 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 35 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 190000\n",
      "trainer/ZF1 Loss                      233.584\n",
      "trainer/ZF2 Loss                      196.027\n",
      "trainer/ZF Expert Reward               23.7436\n",
      "trainer/ZF Policy Reward                8.95531\n",
      "trainer/ZF CHI2 Term                  223.292\n",
      "trainer/Policy Loss                  -951.215\n",
      "trainer/expert_lambda Loss             39.2357\n",
      "trainer/expert_lambda Value            22.3096\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              252.13\n",
      "trainer/Policy Param Norm              35.1127\n",
      "trainer/Zf1 Grad Norm               19370.4\n",
      "trainer/Zf1 Param Norm                124.29\n",
      "trainer/Zf2 Grad Norm                9741.63\n",
      "trainer/Zf2 Param Norm                121.832\n",
      "trainer/Z Expert Predictions Mean    1531\n",
      "trainer/Z Expert Predictions Std      112.56\n",
      "trainer/Z Expert Predictions Max     1654.75\n",
      "trainer/Z Expert Predictions Min      629.084\n",
      "trainer/Z Policy Predictions Mean     941.649\n",
      "trainer/Z Policy Predictions Std      543.906\n",
      "trainer/Z Policy Predictions Max     1572.72\n",
      "trainer/Z Policy Predictions Min      -16.8602\n",
      "trainer/Z Expert Targets Mean        1507.25\n",
      "trainer/Z Expert Targets Std          117.253\n",
      "trainer/Z Expert Targets Max         1639.08\n",
      "trainer/Z Expert Targets Min          514.62\n",
      "trainer/Z Policy Targets Mean         932.693\n",
      "trainer/Z Policy Targets Std          542.542\n",
      "trainer/Z Policy Targets Max         1522.99\n",
      "trainer/Z Policy Targets Min          -23.3622\n",
      "trainer/Log Pis Mean                   27.6805\n",
      "trainer/Log Pis Std                     9.42652\n",
      "trainer/Policy mu Mean                  0.253426\n",
      "trainer/Policy mu Std                   1.39717\n",
      "trainer/Policy log std Mean            -1.91027\n",
      "trainer/Policy log std Std              0.891406\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        184851\n",
      "exploration/num paths total          1476\n",
      "evaluation/num steps total         162987\n",
      "evaluation/num paths total            367\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.30355\n",
      "evaluation/Rewards Std                  0.0695689\n",
      "evaluation/Rewards Max                  5.48901\n",
      "evaluation/Rewards Min                  4.83945\n",
      "evaluation/Returns Mean              5303.55\n",
      "evaluation/Returns Std                  3.89384\n",
      "evaluation/Returns Max               5308.18\n",
      "evaluation/Returns Min               5296.35\n",
      "evaluation/Estimation Bias Mean      1444.5\n",
      "evaluation/Estimation Bias Std        171.847\n",
      "evaluation/EB/Q_True Mean              47.8876\n",
      "evaluation/EB/Q_True Std              147.472\n",
      "evaluation/EB/Q_Pred Mean            1492.38\n",
      "evaluation/EB/Q_Pred Std               86.7862\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5303.55\n",
      "evaluation/Actions Mean                 0.0793723\n",
      "evaluation/Actions Std                  0.507007\n",
      "evaluation/Actions Max                  0.999536\n",
      "evaluation/Actions Min                 -0.999237\n",
      "time/backward_policy (s)                8.64721\n",
      "time/backward_zf1 (s)                   9.92722\n",
      "time/backward_zf2 (s)                   9.70273\n",
      "time/data sampling (s)                  1.74561\n",
      "time/data storing (s)                   0.0848859\n",
      "time/evaluation sampling (s)            4.60876\n",
      "time/exploration sampling (s)           2.49098\n",
      "time/logging (s)                        0.0127571\n",
      "time/preback_alpha (s)                  0.00617865\n",
      "time/preback_policy (s)                14.8235\n",
      "time/preback_start (s)                  0.977133\n",
      "time/preback_zf (s)                    36.422\n",
      "time/saving (s)                         2.964e-06\n",
      "time/training (s)                      10.7361\n",
      "time/epoch (s)                        100.185\n",
      "time/total (s)                       3597\n",
      "Epoch                                  35\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:06:23.104119 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 36 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 195000\n",
      "trainer/ZF1 Loss                      202.703\n",
      "trainer/ZF2 Loss                      234.175\n",
      "trainer/ZF Expert Reward               24.4093\n",
      "trainer/ZF Policy Reward                8.44024\n",
      "trainer/ZF CHI2 Term                  227.529\n",
      "trainer/Policy Loss                  -860.803\n",
      "trainer/expert_lambda Loss             34.7257\n",
      "trainer/expert_lambda Value            22.3788\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              246.341\n",
      "trainer/Policy Param Norm              35.4131\n",
      "trainer/Zf1 Grad Norm               12933.1\n",
      "trainer/Zf1 Param Norm                126.188\n",
      "trainer/Zf2 Grad Norm               16852.2\n",
      "trainer/Zf2 Param Norm                123.646\n",
      "trainer/Z Expert Predictions Mean    1538.62\n",
      "trainer/Z Expert Predictions Std       86.2211\n",
      "trainer/Z Expert Predictions Max     1651.73\n",
      "trainer/Z Expert Predictions Min     1192.66\n",
      "trainer/Z Policy Predictions Mean     851.978\n",
      "trainer/Z Policy Predictions Std      568.014\n",
      "trainer/Z Policy Predictions Max     1553.69\n",
      "trainer/Z Policy Predictions Min      -34.4254\n",
      "trainer/Z Expert Targets Mean        1514.21\n",
      "trainer/Z Expert Targets Std           85.9975\n",
      "trainer/Z Expert Targets Max         1627.57\n",
      "trainer/Z Expert Targets Min         1164.43\n",
      "trainer/Z Policy Targets Mean         843.537\n",
      "trainer/Z Policy Targets Std          566.831\n",
      "trainer/Z Policy Targets Max         1532.48\n",
      "trainer/Z Policy Targets Min          -30.1472\n",
      "trainer/Log Pis Mean                   27.9032\n",
      "trainer/Log Pis Std                     9.16467\n",
      "trainer/Policy mu Mean                  0.29065\n",
      "trainer/Policy mu Std                   1.43367\n",
      "trainer/Policy log std Mean            -1.81425\n",
      "trainer/Policy log std Std              0.980448\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        190851\n",
      "exploration/num paths total          1482\n",
      "evaluation/num steps total         172987\n",
      "evaluation/num paths total            377\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.29849\n",
      "evaluation/Rewards Std                  0.0674272\n",
      "evaluation/Rewards Max                  5.49866\n",
      "evaluation/Rewards Min                  4.86209\n",
      "evaluation/Returns Mean              5298.49\n",
      "evaluation/Returns Std                  4.18055\n",
      "evaluation/Returns Max               5305.23\n",
      "evaluation/Returns Min               5291.04\n",
      "evaluation/Estimation Bias Mean      1383.97\n",
      "evaluation/Estimation Bias Std        170.095\n",
      "evaluation/EB/Q_True Mean              47.7751\n",
      "evaluation/EB/Q_True Std              147.148\n",
      "evaluation/EB/Q_Pred Mean            1431.74\n",
      "evaluation/EB/Q_Pred Std               82.8002\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5298.49\n",
      "evaluation/Actions Mean                 0.047756\n",
      "evaluation/Actions Std                  0.511523\n",
      "evaluation/Actions Max                  0.997885\n",
      "evaluation/Actions Min                 -0.998144\n",
      "time/backward_policy (s)                9.07498\n",
      "time/backward_zf1 (s)                  10.465\n",
      "time/backward_zf2 (s)                  10.2141\n",
      "time/data sampling (s)                  1.82104\n",
      "time/data storing (s)                   0.0867421\n",
      "time/evaluation sampling (s)            4.477\n",
      "time/exploration sampling (s)           2.50655\n",
      "time/logging (s)                        0.0119009\n",
      "time/preback_alpha (s)                  0.00629814\n",
      "time/preback_policy (s)                14.5301\n",
      "time/preback_start (s)                  0.989522\n",
      "time/preback_zf (s)                    36.5618\n",
      "time/saving (s)                         2.931e-06\n",
      "time/training (s)                      10.8163\n",
      "time/epoch (s)                        101.561\n",
      "time/total (s)                       3698.57\n",
      "Epoch                                  36\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:08:03.016216 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 37 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 200000\n",
      "trainer/ZF1 Loss                      198.378\n",
      "trainer/ZF2 Loss                      189.222\n",
      "trainer/ZF Expert Reward               20.1612\n",
      "trainer/ZF Policy Reward                2.86241\n",
      "trainer/ZF CHI2 Term                  198.559\n",
      "trainer/Policy Loss                  -920.724\n",
      "trainer/expert_lambda Loss             26.5029\n",
      "trainer/expert_lambda Value            22.4575\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              224.345\n",
      "trainer/Policy Param Norm              35.733\n",
      "trainer/Zf1 Grad Norm               13373.2\n",
      "trainer/Zf1 Param Norm                128.008\n",
      "trainer/Zf2 Grad Norm               13040.1\n",
      "trainer/Zf2 Param Norm                125.371\n",
      "trainer/Z Expert Predictions Mean    1547.42\n",
      "trainer/Z Expert Predictions Std      149.187\n",
      "trainer/Z Expert Predictions Max     1677.24\n",
      "trainer/Z Expert Predictions Min     -170.325\n",
      "trainer/Z Policy Predictions Mean     913.704\n",
      "trainer/Z Policy Predictions Std      546.649\n",
      "trainer/Z Policy Predictions Max     1607.41\n",
      "trainer/Z Policy Predictions Min       -5.92379\n",
      "trainer/Z Expert Targets Mean        1527.26\n",
      "trainer/Z Expert Targets Std          149.721\n",
      "trainer/Z Expert Targets Max         1654.39\n",
      "trainer/Z Expert Targets Min         -204.584\n",
      "trainer/Z Policy Targets Mean         910.842\n",
      "trainer/Z Policy Targets Std          547.988\n",
      "trainer/Z Policy Targets Max         1616.64\n",
      "trainer/Z Policy Targets Min          -11.118\n",
      "trainer/Log Pis Mean                   28.2802\n",
      "trainer/Log Pis Std                     9.54334\n",
      "trainer/Policy mu Mean                  0.25627\n",
      "trainer/Policy mu Std                   1.37211\n",
      "trainer/Policy log std Mean            -1.89411\n",
      "trainer/Policy log std Std              0.910431\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        194851\n",
      "exploration/num paths total          1486\n",
      "evaluation/num steps total         182987\n",
      "evaluation/num paths total            387\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.3176\n",
      "evaluation/Rewards Std                  0.0709747\n",
      "evaluation/Rewards Max                  5.52507\n",
      "evaluation/Rewards Min                  4.79327\n",
      "evaluation/Returns Mean              5317.6\n",
      "evaluation/Returns Std                  3.63079\n",
      "evaluation/Returns Max               5323.71\n",
      "evaluation/Returns Min               5312.11\n",
      "evaluation/Estimation Bias Mean      1490.66\n",
      "evaluation/Estimation Bias Std        174.513\n",
      "evaluation/EB/Q_True Mean              48.0231\n",
      "evaluation/EB/Q_True Std              147.897\n",
      "evaluation/EB/Q_Pred Mean            1538.69\n",
      "evaluation/EB/Q_Pred Std               87.9671\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5317.6\n",
      "evaluation/Actions Mean                 0.0682446\n",
      "evaluation/Actions Std                  0.498637\n",
      "evaluation/Actions Max                  0.998319\n",
      "evaluation/Actions Min                 -0.998217\n",
      "time/backward_policy (s)                7.99052\n",
      "time/backward_zf1 (s)                   9.29299\n",
      "time/backward_zf2 (s)                   8.96476\n",
      "time/data sampling (s)                  1.72092\n",
      "time/data storing (s)                   0.0842313\n",
      "time/evaluation sampling (s)            4.76989\n",
      "time/exploration sampling (s)           2.44966\n",
      "time/logging (s)                        0.0124259\n",
      "time/preback_alpha (s)                  0.00608005\n",
      "time/preback_policy (s)                15.8554\n",
      "time/preback_start (s)                  0.950247\n",
      "time/preback_zf (s)                    36.2605\n",
      "time/saving (s)                         2.895e-06\n",
      "time/training (s)                      11.3488\n",
      "time/epoch (s)                         99.7065\n",
      "time/total (s)                       3798.28\n",
      "Epoch                                  37\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:09:42.849218 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 38 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 205000\n",
      "trainer/ZF1 Loss                      208.763\n",
      "trainer/ZF2 Loss                      200.413\n",
      "trainer/ZF Expert Reward               23.0194\n",
      "trainer/ZF Policy Reward                8.24783\n",
      "trainer/ZF CHI2 Term                  212.243\n",
      "trainer/Policy Loss                  -957.517\n",
      "trainer/expert_lambda Loss             21.7309\n",
      "trainer/expert_lambda Value            22.5605\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              292.203\n",
      "trainer/Policy Param Norm              36.0198\n",
      "trainer/Zf1 Grad Norm               11826.7\n",
      "trainer/Zf1 Param Norm                129.741\n",
      "trainer/Zf2 Grad Norm               15165.1\n",
      "trainer/Zf2 Param Norm                127.091\n",
      "trainer/Z Expert Predictions Mean    1541.01\n",
      "trainer/Z Expert Predictions Std      196.719\n",
      "trainer/Z Expert Predictions Max     1682\n",
      "trainer/Z Expert Predictions Min      -88.5415\n",
      "trainer/Z Policy Predictions Mean     947.286\n",
      "trainer/Z Policy Predictions Std      546.245\n",
      "trainer/Z Policy Predictions Max     1646.06\n",
      "trainer/Z Policy Predictions Min      -28.1068\n",
      "trainer/Z Expert Targets Mean        1517.99\n",
      "trainer/Z Expert Targets Std          197.749\n",
      "trainer/Z Expert Targets Max         1670.29\n",
      "trainer/Z Expert Targets Min         -113.106\n",
      "trainer/Z Policy Targets Mean         939.038\n",
      "trainer/Z Policy Targets Std          545.745\n",
      "trainer/Z Policy Targets Max         1581.06\n",
      "trainer/Z Policy Targets Min          -25.7967\n",
      "trainer/Log Pis Mean                   27.6101\n",
      "trainer/Log Pis Std                     9.24839\n",
      "trainer/Policy mu Mean                  0.212035\n",
      "trainer/Policy mu Std                   1.37053\n",
      "trainer/Policy log std Mean            -1.91313\n",
      "trainer/Policy log std Std              0.952358\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        200851\n",
      "exploration/num paths total          1492\n",
      "evaluation/num steps total         192987\n",
      "evaluation/num paths total            397\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.34895\n",
      "evaluation/Rewards Std                  0.0898637\n",
      "evaluation/Rewards Max                  5.58396\n",
      "evaluation/Rewards Min                  4.77128\n",
      "evaluation/Returns Mean              5348.95\n",
      "evaluation/Returns Std                  7.64705\n",
      "evaluation/Returns Max               5360.5\n",
      "evaluation/Returns Min               5337.42\n",
      "evaluation/Estimation Bias Mean      1482.9\n",
      "evaluation/Estimation Bias Std        173.23\n",
      "evaluation/EB/Q_True Mean              48.3157\n",
      "evaluation/EB/Q_True Std              148.789\n",
      "evaluation/EB/Q_Pred Mean            1531.22\n",
      "evaluation/EB/Q_Pred Std               96.5439\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5348.95\n",
      "evaluation/Actions Mean                 0.0769751\n",
      "evaluation/Actions Std                  0.51353\n",
      "evaluation/Actions Max                  0.999651\n",
      "evaluation/Actions Min                 -0.999565\n",
      "time/backward_policy (s)                8.41533\n",
      "time/backward_zf1 (s)                   9.72406\n",
      "time/backward_zf2 (s)                   9.46192\n",
      "time/data sampling (s)                  1.76356\n",
      "time/data storing (s)                   0.0848642\n",
      "time/evaluation sampling (s)            4.46711\n",
      "time/exploration sampling (s)           2.46855\n",
      "time/logging (s)                        0.0118481\n",
      "time/preback_alpha (s)                  0.0061164\n",
      "time/preback_policy (s)                15.1462\n",
      "time/preback_start (s)                  0.962212\n",
      "time/preback_zf (s)                    36.2408\n",
      "time/saving (s)                         2.874e-06\n",
      "time/training (s)                      10.8747\n",
      "time/epoch (s)                         99.6274\n",
      "time/total (s)                       3897.92\n",
      "Epoch                                  38\n",
      "---------------------------------  --------------\n",
      "2024-11-02 00:11:22.778994 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 39 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 210000\n",
      "trainer/ZF1 Loss                      771.437\n",
      "trainer/ZF2 Loss                     1177.1\n",
      "trainer/ZF Expert Reward               23.7098\n",
      "trainer/ZF Policy Reward                8.81965\n",
      "trainer/ZF CHI2 Term                  982.29\n",
      "trainer/Policy Loss                 -1000.02\n",
      "trainer/expert_lambda Loss             28.5447\n",
      "trainer/expert_lambda Value            22.6195\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              363.551\n",
      "trainer/Policy Param Norm              36.3005\n",
      "trainer/Zf1 Grad Norm               22345.6\n",
      "trainer/Zf1 Param Norm                131.486\n",
      "trainer/Zf2 Grad Norm               26074.7\n",
      "trainer/Zf2 Param Norm                128.838\n",
      "trainer/Z Expert Predictions Mean    1575.29\n",
      "trainer/Z Expert Predictions Std      102.638\n",
      "trainer/Z Expert Predictions Max     1704.22\n",
      "trainer/Z Expert Predictions Min      971.365\n",
      "trainer/Z Policy Predictions Mean     990.044\n",
      "trainer/Z Policy Predictions Std      546.008\n",
      "trainer/Z Policy Predictions Max     1593.3\n",
      "trainer/Z Policy Predictions Min      -28.1507\n",
      "trainer/Z Expert Targets Mean        1551.58\n",
      "trainer/Z Expert Targets Std          102.144\n",
      "trainer/Z Expert Targets Max         1688.09\n",
      "trainer/Z Expert Targets Min          958.331\n",
      "trainer/Z Policy Targets Mean         981.225\n",
      "trainer/Z Policy Targets Std          550.572\n",
      "trainer/Z Policy Targets Max         1619.08\n",
      "trainer/Z Policy Targets Min          -42.6781\n",
      "trainer/Log Pis Mean                   27.5777\n",
      "trainer/Log Pis Std                     9.41882\n",
      "trainer/Policy mu Mean                  0.238832\n",
      "trainer/Policy mu Std                   1.28603\n",
      "trainer/Policy log std Mean            -2.0045\n",
      "trainer/Policy log std Std              0.936195\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        204851\n",
      "exploration/num paths total          1496\n",
      "evaluation/num steps total         201625\n",
      "evaluation/num paths total            407\n",
      "evaluation/path length Mean           863.8\n",
      "evaluation/path length Std            222.902\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            354\n",
      "evaluation/Rewards Mean                 5.31428\n",
      "evaluation/Rewards Std                  0.128433\n",
      "evaluation/Rewards Max                  6.00668\n",
      "evaluation/Rewards Min                  3.91291\n",
      "evaluation/Returns Mean              4590.47\n",
      "evaluation/Returns Std               1201.37\n",
      "evaluation/Returns Max               5332.55\n",
      "evaluation/Returns Min               1871.64\n",
      "evaluation/Estimation Bias Mean      1364\n",
      "evaluation/Estimation Bias Std        359.935\n",
      "evaluation/EB/Q_True Mean              55.5435\n",
      "evaluation/EB/Q_True Std              157.728\n",
      "evaluation/EB/Q_Pred Mean            1419.54\n",
      "evaluation/EB/Q_Pred Std              260.01\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4590.47\n",
      "evaluation/Actions Mean                 0.0776158\n",
      "evaluation/Actions Std                  0.524239\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -1\n",
      "time/backward_policy (s)                8.3744\n",
      "time/backward_zf1 (s)                   9.6654\n",
      "time/backward_zf2 (s)                   9.39765\n",
      "time/data sampling (s)                  1.75366\n",
      "time/data storing (s)                   0.0854395\n",
      "time/evaluation sampling (s)            4.25926\n",
      "time/exploration sampling (s)           2.45577\n",
      "time/logging (s)                        0.0110607\n",
      "time/preback_alpha (s)                  0.00612882\n",
      "time/preback_policy (s)                15.2959\n",
      "time/preback_start (s)                  0.969456\n",
      "time/preback_zf (s)                    36.3911\n",
      "time/saving (s)                         2.848e-06\n",
      "time/training (s)                      11.0554\n",
      "time/epoch (s)                         99.7206\n",
      "time/total (s)                       3997.64\n",
      "Epoch                                  39\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:13:02.709982 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 40 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 215000\n",
      "trainer/ZF1 Loss                      273.607\n",
      "trainer/ZF2 Loss                      235.011\n",
      "trainer/ZF Expert Reward               22.4862\n",
      "trainer/ZF Policy Reward               10.1063\n",
      "trainer/ZF CHI2 Term                  261.226\n",
      "trainer/Policy Loss                  -997.251\n",
      "trainer/expert_lambda Loss             40.6837\n",
      "trainer/expert_lambda Value            22.6925\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              295.331\n",
      "trainer/Policy Param Norm              36.5775\n",
      "trainer/Zf1 Grad Norm               16045\n",
      "trainer/Zf1 Param Norm                133.245\n",
      "trainer/Zf2 Grad Norm               13243\n",
      "trainer/Zf2 Param Norm                130.589\n",
      "trainer/Z Expert Predictions Mean    1566.97\n",
      "trainer/Z Expert Predictions Std       78.1619\n",
      "trainer/Z Expert Predictions Max     1705.04\n",
      "trainer/Z Expert Predictions Min     1196.72\n",
      "trainer/Z Policy Predictions Mean     987.386\n",
      "trainer/Z Policy Predictions Std      542.658\n",
      "trainer/Z Policy Predictions Max     1597.15\n",
      "trainer/Z Policy Predictions Min      -11.2187\n",
      "trainer/Z Expert Targets Mean        1544.48\n",
      "trainer/Z Expert Targets Std           78.1726\n",
      "trainer/Z Expert Targets Max         1677\n",
      "trainer/Z Expert Targets Min         1170.33\n",
      "trainer/Z Policy Targets Mean         977.28\n",
      "trainer/Z Policy Targets Std          541.402\n",
      "trainer/Z Policy Targets Max         1547.16\n",
      "trainer/Z Policy Targets Min          -13.0394\n",
      "trainer/Log Pis Mean                   27.6903\n",
      "trainer/Log Pis Std                     8.68767\n",
      "trainer/Policy mu Mean                  0.200358\n",
      "trainer/Policy mu Std                   1.38232\n",
      "trainer/Policy log std Mean            -1.95887\n",
      "trainer/Policy log std Std              0.930698\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        212003\n",
      "exploration/num paths total          1504\n",
      "evaluation/num steps total         206477\n",
      "evaluation/num paths total            417\n",
      "evaluation/path length Mean           485.2\n",
      "evaluation/path length Std            272.437\n",
      "evaluation/path length Max            985\n",
      "evaluation/path length Min            111\n",
      "evaluation/Rewards Mean                 5.17321\n",
      "evaluation/Rewards Std                  0.336935\n",
      "evaluation/Rewards Max                  6.41135\n",
      "evaluation/Rewards Min                  3.35825\n",
      "evaluation/Returns Mean              2510.04\n",
      "evaluation/Returns Std               1443.35\n",
      "evaluation/Returns Max               5138.62\n",
      "evaluation/Returns Min                504.146\n",
      "evaluation/Estimation Bias Mean      1075.49\n",
      "evaluation/Estimation Bias Std        531.523\n",
      "evaluation/EB/Q_True Mean              95.413\n",
      "evaluation/EB/Q_True Std              195.956\n",
      "evaluation/EB/Q_Pred Mean            1170.9\n",
      "evaluation/EB/Q_Pred Std              461.216\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           2510.04\n",
      "evaluation/Actions Mean                 0.101726\n",
      "evaluation/Actions Std                  0.552285\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -1\n",
      "time/backward_policy (s)                7.72466\n",
      "time/backward_zf1 (s)                   9.03298\n",
      "time/backward_zf2 (s)                   8.65736\n",
      "time/data sampling (s)                  1.82284\n",
      "time/data storing (s)                   0.0843764\n",
      "time/evaluation sampling (s)            4.72275\n",
      "time/exploration sampling (s)           2.47562\n",
      "time/logging (s)                        0.00798834\n",
      "time/preback_alpha (s)                  0.00606101\n",
      "time/preback_policy (s)                16.3199\n",
      "time/preback_start (s)                  0.964863\n",
      "time/preback_zf (s)                    36.3058\n",
      "time/saving (s)                         2.93e-06\n",
      "time/training (s)                      11.5976\n",
      "time/epoch (s)                         99.7228\n",
      "time/total (s)                       4097.37\n",
      "Epoch                                  40\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:14:42.317447 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 41 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 220000\n",
      "trainer/ZF1 Loss                      238.915\n",
      "trainer/ZF2 Loss                      221.478\n",
      "trainer/ZF Expert Reward               25.1206\n",
      "trainer/ZF Policy Reward                4.40638\n",
      "trainer/ZF CHI2 Term                  239.991\n",
      "trainer/Policy Loss                  -936.355\n",
      "trainer/expert_lambda Loss             40.0813\n",
      "trainer/expert_lambda Value            22.7493\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              328.243\n",
      "trainer/Policy Param Norm              36.8398\n",
      "trainer/Zf1 Grad Norm               14982.6\n",
      "trainer/Zf1 Param Norm                135.022\n",
      "trainer/Zf2 Grad Norm               14810.6\n",
      "trainer/Zf2 Param Norm                132.374\n",
      "trainer/Z Expert Predictions Mean    1541.91\n",
      "trainer/Z Expert Predictions Std       72.9711\n",
      "trainer/Z Expert Predictions Max     1669.77\n",
      "trainer/Z Expert Predictions Min     1174.77\n",
      "trainer/Z Policy Predictions Mean     927.417\n",
      "trainer/Z Policy Predictions Std      549.381\n",
      "trainer/Z Policy Predictions Max     1517.83\n",
      "trainer/Z Policy Predictions Min      -45.8256\n",
      "trainer/Z Expert Targets Mean        1516.79\n",
      "trainer/Z Expert Targets Std           74.7678\n",
      "trainer/Z Expert Targets Max         1647.88\n",
      "trainer/Z Expert Targets Min         1161.79\n",
      "trainer/Z Policy Targets Mean         923.011\n",
      "trainer/Z Policy Targets Std          544.215\n",
      "trainer/Z Policy Targets Max         1517.97\n",
      "trainer/Z Policy Targets Min          -49.6348\n",
      "trainer/Log Pis Mean                   27.6293\n",
      "trainer/Log Pis Std                     9.69511\n",
      "trainer/Policy mu Mean                  0.164362\n",
      "trainer/Policy mu Std                   1.36253\n",
      "trainer/Policy log std Mean            -1.92644\n",
      "trainer/Policy log std Std              0.93629\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        215350\n",
      "exploration/num paths total          1508\n",
      "evaluation/num steps total         215599\n",
      "evaluation/num paths total            427\n",
      "evaluation/path length Mean           912.2\n",
      "evaluation/path length Std            263.4\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            122\n",
      "evaluation/Rewards Mean                 5.30064\n",
      "evaluation/Rewards Std                  0.112038\n",
      "evaluation/Rewards Max                  5.50252\n",
      "evaluation/Rewards Min                  3.6216\n",
      "evaluation/Returns Mean              4835.25\n",
      "evaluation/Returns Std               1420.2\n",
      "evaluation/Returns Max               5317.66\n",
      "evaluation/Returns Min                574.68\n",
      "evaluation/Estimation Bias Mean      1359.92\n",
      "evaluation/Estimation Bias Std        249.956\n",
      "evaluation/EB/Q_True Mean              52.5553\n",
      "evaluation/EB/Q_True Std              153.803\n",
      "evaluation/EB/Q_Pred Mean            1412.47\n",
      "evaluation/EB/Q_Pred Std              145.529\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4835.25\n",
      "evaluation/Actions Mean                 0.0764441\n",
      "evaluation/Actions Std                  0.503606\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999929\n",
      "time/backward_policy (s)                7.71944\n",
      "time/backward_zf1 (s)                   9.01084\n",
      "time/backward_zf2 (s)                   8.66613\n",
      "time/data sampling (s)                  1.71824\n",
      "time/data storing (s)                   0.0838547\n",
      "time/evaluation sampling (s)            4.76019\n",
      "time/exploration sampling (s)           2.47164\n",
      "time/logging (s)                        0.0115363\n",
      "time/preback_alpha (s)                  0.00605802\n",
      "time/preback_policy (s)                16.2301\n",
      "time/preback_start (s)                  0.947351\n",
      "time/preback_zf (s)                    36.2567\n",
      "time/saving (s)                         2.807e-06\n",
      "time/training (s)                      11.5204\n",
      "time/epoch (s)                         99.4025\n",
      "time/total (s)                       4196.78\n",
      "Epoch                                  41\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:16:22.392131 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 42 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 225000\n",
      "trainer/ZF1 Loss                     1049.01\n",
      "trainer/ZF2 Loss                     1141.12\n",
      "trainer/ZF Expert Reward               21.7786\n",
      "trainer/ZF Policy Reward                8.81575\n",
      "trainer/ZF CHI2 Term                 1101.9\n",
      "trainer/Policy Loss                  -918.277\n",
      "trainer/expert_lambda Loss             45.9295\n",
      "trainer/expert_lambda Value            22.807\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              283.882\n",
      "trainer/Policy Param Norm              37.0671\n",
      "trainer/Zf1 Grad Norm               28434.2\n",
      "trainer/Zf1 Param Norm                136.868\n",
      "trainer/Zf2 Grad Norm               35760.4\n",
      "trainer/Zf2 Param Norm                134.226\n",
      "trainer/Z Expert Predictions Mean    1500.89\n",
      "trainer/Z Expert Predictions Std      117.725\n",
      "trainer/Z Expert Predictions Max     1645.94\n",
      "trainer/Z Expert Predictions Min      354.802\n",
      "trainer/Z Policy Predictions Mean     910.409\n",
      "trainer/Z Policy Predictions Std      547.002\n",
      "trainer/Z Policy Predictions Max     1572.54\n",
      "trainer/Z Policy Predictions Min      -22.2087\n",
      "trainer/Z Expert Targets Mean        1479.11\n",
      "trainer/Z Expert Targets Std          119.602\n",
      "trainer/Z Expert Targets Max         1617.54\n",
      "trainer/Z Expert Targets Min          292.617\n",
      "trainer/Z Policy Targets Mean         901.593\n",
      "trainer/Z Policy Targets Std          550.276\n",
      "trainer/Z Policy Targets Max         1516.84\n",
      "trainer/Z Policy Targets Min          -31.7327\n",
      "trainer/Log Pis Mean                   28.0142\n",
      "trainer/Log Pis Std                     8.82444\n",
      "trainer/Policy mu Mean                  0.141197\n",
      "trainer/Policy mu Std                   1.33016\n",
      "trainer/Policy log std Mean            -1.95445\n",
      "trainer/Policy log std Std              0.963001\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        221350\n",
      "exploration/num paths total          1514\n",
      "evaluation/num steps total         225599\n",
      "evaluation/num paths total            437\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.31954\n",
      "evaluation/Rewards Std                  0.0816454\n",
      "evaluation/Rewards Max                  5.52115\n",
      "evaluation/Rewards Min                  4.74894\n",
      "evaluation/Returns Mean              5319.54\n",
      "evaluation/Returns Std                  4.78999\n",
      "evaluation/Returns Max               5330.35\n",
      "evaluation/Returns Min               5311.12\n",
      "evaluation/Estimation Bias Mean      1316.02\n",
      "evaluation/Estimation Bias Std        165.589\n",
      "evaluation/EB/Q_True Mean              48.027\n",
      "evaluation/EB/Q_True Std              147.915\n",
      "evaluation/EB/Q_Pred Mean            1364.04\n",
      "evaluation/EB/Q_Pred Std               69.0031\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5319.54\n",
      "evaluation/Actions Mean                 0.0725027\n",
      "evaluation/Actions Std                  0.506042\n",
      "evaluation/Actions Max                  0.999611\n",
      "evaluation/Actions Min                 -0.995296\n",
      "time/backward_policy (s)                7.83837\n",
      "time/backward_zf1 (s)                   9.1786\n",
      "time/backward_zf2 (s)                   8.80314\n",
      "time/data sampling (s)                  1.83712\n",
      "time/data storing (s)                   0.0840726\n",
      "time/evaluation sampling (s)            4.53941\n",
      "time/exploration sampling (s)           2.48467\n",
      "time/logging (s)                        0.0128823\n",
      "time/preback_alpha (s)                  0.00605526\n",
      "time/preback_policy (s)                16.2024\n",
      "time/preback_start (s)                  0.965018\n",
      "time/preback_zf (s)                    36.3313\n",
      "time/saving (s)                         3.134e-06\n",
      "time/training (s)                      11.5881\n",
      "time/epoch (s)                         99.8711\n",
      "time/total (s)                       4296.66\n",
      "Epoch                                  42\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:18:03.556807 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 43 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 230000\n",
      "trainer/ZF1 Loss                      186.535\n",
      "trainer/ZF2 Loss                      196.676\n",
      "trainer/ZF Expert Reward               23.5198\n",
      "trainer/ZF Policy Reward                7.20337\n",
      "trainer/ZF CHI2 Term                  200.357\n",
      "trainer/Policy Loss                  -901.074\n",
      "trainer/expert_lambda Loss             37.8953\n",
      "trainer/expert_lambda Value            22.8718\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              362.618\n",
      "trainer/Policy Param Norm              37.3422\n",
      "trainer/Zf1 Grad Norm               10481.1\n",
      "trainer/Zf1 Param Norm                138.545\n",
      "trainer/Zf2 Grad Norm               12458.1\n",
      "trainer/Zf2 Param Norm                135.979\n",
      "trainer/Z Expert Predictions Mean    1489.33\n",
      "trainer/Z Expert Predictions Std       66.4966\n",
      "trainer/Z Expert Predictions Max     1596.94\n",
      "trainer/Z Expert Predictions Min     1185.27\n",
      "trainer/Z Policy Predictions Mean     893.604\n",
      "trainer/Z Policy Predictions Std      527.066\n",
      "trainer/Z Policy Predictions Max     1476.13\n",
      "trainer/Z Policy Predictions Min      -16.0433\n",
      "trainer/Z Expert Targets Mean        1465.81\n",
      "trainer/Z Expert Targets Std           67.8039\n",
      "trainer/Z Expert Targets Max         1578.39\n",
      "trainer/Z Expert Targets Min         1154.72\n",
      "trainer/Z Policy Targets Mean         886.4\n",
      "trainer/Z Policy Targets Std          526.243\n",
      "trainer/Z Policy Targets Max         1473.63\n",
      "trainer/Z Policy Targets Min          -28.8658\n",
      "trainer/Log Pis Mean                   28.3995\n",
      "trainer/Log Pis Std                     9.48241\n",
      "trainer/Policy mu Mean                  0.174228\n",
      "trainer/Policy mu Std                   1.36874\n",
      "trainer/Policy log std Mean            -1.94276\n",
      "trainer/Policy log std Std              0.97779\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        225350\n",
      "exploration/num paths total          1518\n",
      "evaluation/num steps total         235599\n",
      "evaluation/num paths total            447\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.32942\n",
      "evaluation/Rewards Std                  0.0827093\n",
      "evaluation/Rewards Max                  5.49201\n",
      "evaluation/Rewards Min                  4.76568\n",
      "evaluation/Returns Mean              5329.42\n",
      "evaluation/Returns Std                  6.14337\n",
      "evaluation/Returns Max               5339.02\n",
      "evaluation/Returns Min               5317.73\n",
      "evaluation/Estimation Bias Mean      1321.81\n",
      "evaluation/Estimation Bias Std        163.672\n",
      "evaluation/EB/Q_True Mean              48.2022\n",
      "evaluation/EB/Q_True Std              148.482\n",
      "evaluation/EB/Q_Pred Mean            1370.02\n",
      "evaluation/EB/Q_Pred Std               75.8824\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5329.42\n",
      "evaluation/Actions Mean                 0.0703835\n",
      "evaluation/Actions Std                  0.517395\n",
      "evaluation/Actions Max                  0.999332\n",
      "evaluation/Actions Min                 -0.996407\n",
      "time/backward_policy (s)                8.34329\n",
      "time/backward_zf1 (s)                   9.69219\n",
      "time/backward_zf2 (s)                   9.40322\n",
      "time/data sampling (s)                  1.80029\n",
      "time/data storing (s)                   0.0879025\n",
      "time/evaluation sampling (s)            4.53689\n",
      "time/exploration sampling (s)           2.48579\n",
      "time/logging (s)                        0.0127292\n",
      "time/preback_alpha (s)                  0.00615972\n",
      "time/preback_policy (s)                15.6452\n",
      "time/preback_start (s)                  0.98283\n",
      "time/preback_zf (s)                    36.5283\n",
      "time/saving (s)                         3.072e-06\n",
      "time/training (s)                      11.4295\n",
      "time/epoch (s)                        100.954\n",
      "time/total (s)                       4397.62\n",
      "Epoch                                  43\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:19:44.108072 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 44 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 235000\n",
      "trainer/ZF1 Loss                      165.233\n",
      "trainer/ZF2 Loss                      181.742\n",
      "trainer/ZF Expert Reward               22.9885\n",
      "trainer/ZF Policy Reward                2.56995\n",
      "trainer/ZF CHI2 Term                  182.228\n",
      "trainer/Policy Loss                  -925.911\n",
      "trainer/expert_lambda Loss             24.0031\n",
      "trainer/expert_lambda Value            22.9322\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              319.046\n",
      "trainer/Policy Param Norm              37.6087\n",
      "trainer/Zf1 Grad Norm                8160.15\n",
      "trainer/Zf1 Param Norm                140.188\n",
      "trainer/Zf2 Grad Norm               11510.8\n",
      "trainer/Zf2 Param Norm                137.688\n",
      "trainer/Z Expert Predictions Mean    1435.34\n",
      "trainer/Z Expert Predictions Std       68.6015\n",
      "trainer/Z Expert Predictions Max     1574.68\n",
      "trainer/Z Expert Predictions Min      882.298\n",
      "trainer/Z Policy Predictions Mean     919.089\n",
      "trainer/Z Policy Predictions Std      501.884\n",
      "trainer/Z Policy Predictions Max     1414.15\n",
      "trainer/Z Policy Predictions Min      -52.3786\n",
      "trainer/Z Expert Targets Mean        1412.35\n",
      "trainer/Z Expert Targets Std           68.5107\n",
      "trainer/Z Expert Targets Max         1542.5\n",
      "trainer/Z Expert Targets Min          864.028\n",
      "trainer/Z Policy Targets Mean         916.519\n",
      "trainer/Z Policy Targets Std          500.164\n",
      "trainer/Z Policy Targets Max         1423.43\n",
      "trainer/Z Policy Targets Min          -52.8132\n",
      "trainer/Log Pis Mean                   27.1051\n",
      "trainer/Log Pis Std                     8.53451\n",
      "trainer/Policy mu Mean                  0.125575\n",
      "trainer/Policy mu Std                   1.25603\n",
      "trainer/Policy log std Mean            -2.0119\n",
      "trainer/Policy log std Std              0.967327\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        231460\n",
      "exploration/num paths total          1525\n",
      "evaluation/num steps total         245599\n",
      "evaluation/num paths total            457\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.36009\n",
      "evaluation/Rewards Std                  0.0835653\n",
      "evaluation/Rewards Max                  5.53748\n",
      "evaluation/Rewards Min                  4.79378\n",
      "evaluation/Returns Mean              5360.09\n",
      "evaluation/Returns Std                  4.68063\n",
      "evaluation/Returns Max               5365.57\n",
      "evaluation/Returns Min               5352.07\n",
      "evaluation/Estimation Bias Mean      1252.75\n",
      "evaluation/Estimation Bias Std        162.943\n",
      "evaluation/EB/Q_True Mean              48.4431\n",
      "evaluation/EB/Q_True Std              149.215\n",
      "evaluation/EB/Q_Pred Mean            1301.2\n",
      "evaluation/EB/Q_Pred Std               66.1527\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5360.09\n",
      "evaluation/Actions Mean                 0.0569655\n",
      "evaluation/Actions Std                  0.509745\n",
      "evaluation/Actions Max                  0.999163\n",
      "evaluation/Actions Min                 -0.996765\n",
      "time/backward_policy (s)                8.45284\n",
      "time/backward_zf1 (s)                   9.75516\n",
      "time/backward_zf2 (s)                   9.48386\n",
      "time/data sampling (s)                  1.75467\n",
      "time/data storing (s)                   0.0844927\n",
      "time/evaluation sampling (s)            4.56417\n",
      "time/exploration sampling (s)           2.47184\n",
      "time/logging (s)                        0.0120599\n",
      "time/preback_alpha (s)                  0.00611954\n",
      "time/preback_policy (s)                15.282\n",
      "time/preback_start (s)                  0.969495\n",
      "time/preback_zf (s)                    36.4434\n",
      "time/saving (s)                         2.905e-06\n",
      "time/training (s)                      11.0612\n",
      "time/epoch (s)                        100.341\n",
      "time/total (s)                       4497.97\n",
      "Epoch                                  44\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:21:23.863473 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 45 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 240000\n",
      "trainer/ZF1 Loss                      185.972\n",
      "trainer/ZF2 Loss                      199.678\n",
      "trainer/ZF Expert Reward               26.3439\n",
      "trainer/ZF Policy Reward                5.77677\n",
      "trainer/ZF CHI2 Term                  205.512\n",
      "trainer/Policy Loss                  -909.848\n",
      "trainer/expert_lambda Loss             33.0423\n",
      "trainer/expert_lambda Value            23.0012\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              326.433\n",
      "trainer/Policy Param Norm              37.8621\n",
      "trainer/Zf1 Grad Norm                9905.42\n",
      "trainer/Zf1 Param Norm                141.687\n",
      "trainer/Zf2 Grad Norm               11472.8\n",
      "trainer/Zf2 Param Norm                139.24\n",
      "trainer/Z Expert Predictions Mean    1377.52\n",
      "trainer/Z Expert Predictions Std       84.9968\n",
      "trainer/Z Expert Predictions Max     1511.22\n",
      "trainer/Z Expert Predictions Min      631.766\n",
      "trainer/Z Policy Predictions Mean     904.609\n",
      "trainer/Z Policy Predictions Std      459.735\n",
      "trainer/Z Policy Predictions Max     1371.27\n",
      "trainer/Z Policy Predictions Min       -2.83227\n",
      "trainer/Z Expert Targets Mean        1351.17\n",
      "trainer/Z Expert Targets Std           87.4721\n",
      "trainer/Z Expert Targets Max         1494.53\n",
      "trainer/Z Expert Targets Min          558.243\n",
      "trainer/Z Policy Targets Mean         898.832\n",
      "trainer/Z Policy Targets Std          459.348\n",
      "trainer/Z Policy Targets Max         1401.9\n",
      "trainer/Z Policy Targets Min          -20.0886\n",
      "trainer/Log Pis Mean                   27.1765\n",
      "trainer/Log Pis Std                     8.3918\n",
      "trainer/Policy mu Mean                  0.142148\n",
      "trainer/Policy mu Std                   1.23762\n",
      "trainer/Policy log std Mean            -2.03104\n",
      "trainer/Policy log std Std              0.953092\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        234460\n",
      "exploration/num paths total          1528\n",
      "evaluation/num steps total         255599\n",
      "evaluation/num paths total            467\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.33803\n",
      "evaluation/Rewards Std                  0.089581\n",
      "evaluation/Rewards Max                  5.52644\n",
      "evaluation/Rewards Min                  4.73531\n",
      "evaluation/Returns Mean              5338.03\n",
      "evaluation/Returns Std                  5.48225\n",
      "evaluation/Returns Max               5345.73\n",
      "evaluation/Returns Min               5325.3\n",
      "evaluation/Estimation Bias Mean      1174.7\n",
      "evaluation/Estimation Bias Std        168.288\n",
      "evaluation/EB/Q_True Mean              48.2369\n",
      "evaluation/EB/Q_True Std              148.56\n",
      "evaluation/EB/Q_Pred Mean            1222.94\n",
      "evaluation/EB/Q_Pred Std               68.2397\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5338.03\n",
      "evaluation/Actions Mean                 0.0634856\n",
      "evaluation/Actions Std                  0.504378\n",
      "evaluation/Actions Max                  0.999405\n",
      "evaluation/Actions Min                 -0.995554\n",
      "time/backward_policy (s)                8.26521\n",
      "time/backward_zf1 (s)                   9.56201\n",
      "time/backward_zf2 (s)                   9.27514\n",
      "time/data sampling (s)                  1.75451\n",
      "time/data storing (s)                   0.0856031\n",
      "time/evaluation sampling (s)            4.25485\n",
      "time/exploration sampling (s)           2.48706\n",
      "time/logging (s)                        0.0115352\n",
      "time/preback_alpha (s)                  0.00610313\n",
      "time/preback_policy (s)                15.446\n",
      "time/preback_start (s)                  0.973453\n",
      "time/preback_zf (s)                    36.3332\n",
      "time/saving (s)                         2.919e-06\n",
      "time/training (s)                      11.092\n",
      "time/epoch (s)                         99.5467\n",
      "time/total (s)                       4597.52\n",
      "Epoch                                  45\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:23:04.459846 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 46 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 245000\n",
      "trainer/ZF1 Loss                      196.864\n",
      "trainer/ZF2 Loss                      181.635\n",
      "trainer/ZF Expert Reward               24.7125\n",
      "trainer/ZF Policy Reward                6.99561\n",
      "trainer/ZF CHI2 Term                  201.12\n",
      "trainer/Policy Loss                  -885.68\n",
      "trainer/expert_lambda Loss             28.8444\n",
      "trainer/expert_lambda Value            23.0811\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              262.814\n",
      "trainer/Policy Param Norm              38.1111\n",
      "trainer/Zf1 Grad Norm               16822\n",
      "trainer/Zf1 Param Norm                143.193\n",
      "trainer/Zf2 Grad Norm               15966.2\n",
      "trainer/Zf2 Param Norm                140.804\n",
      "trainer/Z Expert Predictions Mean    1294.83\n",
      "trainer/Z Expert Predictions Std      149.379\n",
      "trainer/Z Expert Predictions Max     1461.44\n",
      "trainer/Z Expert Predictions Min      -50.1492\n",
      "trainer/Z Policy Predictions Mean     879.24\n",
      "trainer/Z Policy Predictions Std      407.664\n",
      "trainer/Z Policy Predictions Max     1278.5\n",
      "trainer/Z Policy Predictions Min       -0.786916\n",
      "trainer/Z Expert Targets Mean        1270.12\n",
      "trainer/Z Expert Targets Std          150.657\n",
      "trainer/Z Expert Targets Max         1428.65\n",
      "trainer/Z Expert Targets Min          -77.404\n",
      "trainer/Z Policy Targets Mean         872.245\n",
      "trainer/Z Policy Targets Std          406.237\n",
      "trainer/Z Policy Targets Max         1257.94\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   28.231\n",
      "trainer/Log Pis Std                     7.77435\n",
      "trainer/Policy mu Mean                  0.131747\n",
      "trainer/Policy mu Std                   1.27791\n",
      "trainer/Policy log std Mean            -2.07361\n",
      "trainer/Policy log std Std              0.904816\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        241460\n",
      "exploration/num paths total          1535\n",
      "evaluation/num steps total         264708\n",
      "evaluation/num paths total            477\n",
      "evaluation/path length Mean           910.9\n",
      "evaluation/path length Std            267.3\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            109\n",
      "evaluation/Rewards Mean                 5.27799\n",
      "evaluation/Rewards Std                  0.11229\n",
      "evaluation/Rewards Max                  5.47207\n",
      "evaluation/Rewards Min                  3.49757\n",
      "evaluation/Returns Mean              4807.72\n",
      "evaluation/Returns Std               1434.29\n",
      "evaluation/Returns Max               5293.63\n",
      "evaluation/Returns Min                504.868\n",
      "evaluation/Estimation Bias Mean      1085.19\n",
      "evaluation/Estimation Bias Std        213.592\n",
      "evaluation/EB/Q_True Mean              52.4219\n",
      "evaluation/EB/Q_True Std              153.273\n",
      "evaluation/EB/Q_Pred Mean            1137.61\n",
      "evaluation/EB/Q_Pred Std              110.981\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4807.72\n",
      "evaluation/Actions Mean                 0.0859955\n",
      "evaluation/Actions Std                  0.496777\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999994\n",
      "time/backward_policy (s)                8.62475\n",
      "time/backward_zf1 (s)                   9.8941\n",
      "time/backward_zf2 (s)                   9.67782\n",
      "time/data sampling (s)                  1.76051\n",
      "time/data storing (s)                   0.0850497\n",
      "time/evaluation sampling (s)            4.57643\n",
      "time/exploration sampling (s)           2.51535\n",
      "time/logging (s)                        0.0114137\n",
      "time/preback_alpha (s)                  0.00613907\n",
      "time/preback_policy (s)                14.9337\n",
      "time/preback_start (s)                  0.979105\n",
      "time/preback_zf (s)                    36.4402\n",
      "time/saving (s)                         2.87e-06\n",
      "time/training (s)                      10.883\n",
      "time/epoch (s)                        100.388\n",
      "time/total (s)                       4697.91\n",
      "Epoch                                  46\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:24:44.651205 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 47 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 250000\n",
      "trainer/ZF1 Loss                      127.761\n",
      "trainer/ZF2 Loss                      156.319\n",
      "trainer/ZF Expert Reward               24.5403\n",
      "trainer/ZF Policy Reward                8.67761\n",
      "trainer/ZF CHI2 Term                  154.148\n",
      "trainer/Policy Loss                  -791.928\n",
      "trainer/expert_lambda Loss             30.5571\n",
      "trainer/expert_lambda Value            23.1555\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              222.755\n",
      "trainer/Policy Param Norm              38.35\n",
      "trainer/Zf1 Grad Norm                9117.86\n",
      "trainer/Zf1 Param Norm                144.635\n",
      "trainer/Zf2 Grad Norm               11272.8\n",
      "trainer/Zf2 Param Norm                142.336\n",
      "trainer/Z Expert Predictions Mean    1249.94\n",
      "trainer/Z Expert Predictions Std      135.167\n",
      "trainer/Z Expert Predictions Max     1445.98\n",
      "trainer/Z Expert Predictions Min     -126.526\n",
      "trainer/Z Policy Predictions Mean     784.476\n",
      "trainer/Z Policy Predictions Std      412.091\n",
      "trainer/Z Policy Predictions Max     1220.25\n",
      "trainer/Z Policy Predictions Min       -4.34409\n",
      "trainer/Z Expert Targets Mean        1225.4\n",
      "trainer/Z Expert Targets Std          135.321\n",
      "trainer/Z Expert Targets Max         1421.92\n",
      "trainer/Z Expert Targets Min         -148.287\n",
      "trainer/Z Policy Targets Mean         775.798\n",
      "trainer/Z Policy Targets Std          411.698\n",
      "trainer/Z Policy Targets Max         1224.12\n",
      "trainer/Z Policy Targets Min          -18.1432\n",
      "trainer/Log Pis Mean                   25.8669\n",
      "trainer/Log Pis Std                     9.26573\n",
      "trainer/Policy mu Mean                  0.110322\n",
      "trainer/Policy mu Std                   1.24018\n",
      "trainer/Policy log std Mean            -1.94792\n",
      "trainer/Policy log std Std              0.85412\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        244792\n",
      "exploration/num paths total          1540\n",
      "evaluation/num steps total         274708\n",
      "evaluation/num paths total            487\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.30801\n",
      "evaluation/Rewards Std                  0.0889303\n",
      "evaluation/Rewards Max                  5.51325\n",
      "evaluation/Rewards Min                  4.72574\n",
      "evaluation/Returns Mean              5308.01\n",
      "evaluation/Returns Std                 18.5918\n",
      "evaluation/Returns Max               5328.31\n",
      "evaluation/Returns Min               5259.47\n",
      "evaluation/Estimation Bias Mean      1075.93\n",
      "evaluation/Estimation Bias Std        175.018\n",
      "evaluation/EB/Q_True Mean              48.0125\n",
      "evaluation/EB/Q_True Std              147.836\n",
      "evaluation/EB/Q_Pred Mean            1123.95\n",
      "evaluation/EB/Q_Pred Std               68.5402\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5308.01\n",
      "evaluation/Actions Mean                 0.0700835\n",
      "evaluation/Actions Std                  0.507823\n",
      "evaluation/Actions Max                  0.999071\n",
      "evaluation/Actions Min                 -0.996491\n",
      "time/backward_policy (s)                8.03835\n",
      "time/backward_zf1 (s)                   9.34334\n",
      "time/backward_zf2 (s)                   9.00837\n",
      "time/data sampling (s)                  1.82243\n",
      "time/data storing (s)                   0.0850628\n",
      "time/evaluation sampling (s)            4.4699\n",
      "time/exploration sampling (s)           2.46437\n",
      "time/logging (s)                        0.0123144\n",
      "time/preback_alpha (s)                  0.00607468\n",
      "time/preback_policy (s)                15.9079\n",
      "time/preback_start (s)                  0.970506\n",
      "time/preback_zf (s)                    36.4021\n",
      "time/saving (s)                         2.758e-06\n",
      "time/training (s)                      11.4544\n",
      "time/epoch (s)                         99.9852\n",
      "time/total (s)                       4797.9\n",
      "Epoch                                  47\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:26:25.993018 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 48 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 255000\n",
      "trainer/ZF1 Loss                      149.457\n",
      "trainer/ZF2 Loss                      164.583\n",
      "trainer/ZF Expert Reward               27.8992\n",
      "trainer/ZF Policy Reward                8.01968\n",
      "trainer/ZF CHI2 Term                  173.021\n",
      "trainer/Policy Loss                  -788.248\n",
      "trainer/expert_lambda Loss             32.4913\n",
      "trainer/expert_lambda Value            23.2484\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              253.248\n",
      "trainer/Policy Param Norm              38.5676\n",
      "trainer/Zf1 Grad Norm                7057.58\n",
      "trainer/Zf1 Param Norm                146.011\n",
      "trainer/Zf2 Grad Norm                8375.17\n",
      "trainer/Zf2 Param Norm                143.764\n",
      "trainer/Z Expert Predictions Mean    1201.98\n",
      "trainer/Z Expert Predictions Std       99.6779\n",
      "trainer/Z Expert Predictions Max     1386.5\n",
      "trainer/Z Expert Predictions Min      -40.7284\n",
      "trainer/Z Policy Predictions Mean     786.972\n",
      "trainer/Z Policy Predictions Std      391.584\n",
      "trainer/Z Policy Predictions Max     1231.55\n",
      "trainer/Z Policy Predictions Min      -11.757\n",
      "trainer/Z Expert Targets Mean        1174.08\n",
      "trainer/Z Expert Targets Std           99.6747\n",
      "trainer/Z Expert Targets Max         1364.42\n",
      "trainer/Z Expert Targets Min          -68.4006\n",
      "trainer/Z Policy Targets Mean         778.952\n",
      "trainer/Z Policy Targets Std          392.766\n",
      "trainer/Z Policy Targets Max         1195.77\n",
      "trainer/Z Policy Targets Min          -24.7579\n",
      "trainer/Log Pis Mean                   27.2856\n",
      "trainer/Log Pis Std                     7.83649\n",
      "trainer/Policy mu Mean                  0.110322\n",
      "trainer/Policy mu Std                   1.25865\n",
      "trainer/Policy log std Mean            -2.00985\n",
      "trainer/Policy log std Std              0.936811\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        250792\n",
      "exploration/num paths total          1546\n",
      "evaluation/num steps total         283845\n",
      "evaluation/num paths total            497\n",
      "evaluation/path length Mean           913.7\n",
      "evaluation/path length Std            258.9\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            137\n",
      "evaluation/Rewards Mean                 5.28373\n",
      "evaluation/Rewards Std                  0.120506\n",
      "evaluation/Rewards Max                  5.52516\n",
      "evaluation/Rewards Min                  3.36866\n",
      "evaluation/Returns Mean              4827.74\n",
      "evaluation/Returns Std               1397.49\n",
      "evaluation/Returns Max               5310.3\n",
      "evaluation/Returns Min                635.34\n",
      "evaluation/Estimation Bias Mean       984.126\n",
      "evaluation/Estimation Bias Std        217.202\n",
      "evaluation/EB/Q_True Mean              52.2975\n",
      "evaluation/EB/Q_True Std              153.172\n",
      "evaluation/EB/Q_Pred Mean            1036.42\n",
      "evaluation/EB/Q_Pred Std              111.372\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4827.74\n",
      "evaluation/Actions Mean                 0.0771827\n",
      "evaluation/Actions Std                  0.492742\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999999\n",
      "time/backward_policy (s)                8.97978\n",
      "time/backward_zf1 (s)                  10.2931\n",
      "time/backward_zf2 (s)                  10.0664\n",
      "time/data sampling (s)                  1.82508\n",
      "time/data storing (s)                   0.0859986\n",
      "time/evaluation sampling (s)            4.43889\n",
      "time/exploration sampling (s)           2.47534\n",
      "time/logging (s)                        0.0150158\n",
      "time/preback_alpha (s)                  0.006275\n",
      "time/preback_policy (s)                14.634\n",
      "time/preback_start (s)                  0.989651\n",
      "time/preback_zf (s)                    36.5237\n",
      "time/saving (s)                         3.947e-06\n",
      "time/training (s)                      10.7993\n",
      "time/epoch (s)                        101.133\n",
      "time/total (s)                       4899.04\n",
      "Epoch                                  48\n",
      "---------------------------------  --------------\n",
      "2024-11-02 00:28:07.434805 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 49 finished\n",
      "---------------------------------  ----------------\n",
      "replay_buffer/size                 260000\n",
      "trainer/ZF1 Loss                      163.889\n",
      "trainer/ZF2 Loss                      195.752\n",
      "trainer/ZF Expert Reward               19.8417\n",
      "trainer/ZF Policy Reward                5.21655\n",
      "trainer/ZF CHI2 Term                  188.428\n",
      "trainer/Policy Loss                  -739.901\n",
      "trainer/expert_lambda Loss             38.7288\n",
      "trainer/expert_lambda Value            23.3527\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              314.096\n",
      "trainer/Policy Param Norm              38.7692\n",
      "trainer/Zf1 Grad Norm               10109.4\n",
      "trainer/Zf1 Param Norm                147.368\n",
      "trainer/Zf2 Grad Norm               14094\n",
      "trainer/Zf2 Param Norm                145.188\n",
      "trainer/Z Expert Predictions Mean    1134.96\n",
      "trainer/Z Expert Predictions Std      124.731\n",
      "trainer/Z Expert Predictions Max     1281.53\n",
      "trainer/Z Expert Predictions Min      -88.1071\n",
      "trainer/Z Policy Predictions Mean     734.352\n",
      "trainer/Z Policy Predictions Std      366.37\n",
      "trainer/Z Policy Predictions Max     1129.18\n",
      "trainer/Z Policy Predictions Min       -3.38604\n",
      "trainer/Z Expert Targets Mean        1115.12\n",
      "trainer/Z Expert Targets Std          125.854\n",
      "trainer/Z Expert Targets Max         1265.43\n",
      "trainer/Z Expert Targets Min         -118.306\n",
      "trainer/Z Policy Targets Mean         729.136\n",
      "trainer/Z Policy Targets Std          370.354\n",
      "trainer/Z Policy Targets Max         1111.37\n",
      "trainer/Z Policy Targets Min           -8.17657\n",
      "trainer/Log Pis Mean                   26.9335\n",
      "trainer/Log Pis Std                     7.42114\n",
      "trainer/Policy mu Mean                  0.162609\n",
      "trainer/Policy mu Std                   1.24342\n",
      "trainer/Policy log std Mean            -2.01484\n",
      "trainer/Policy log std Std              0.939702\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        254792\n",
      "exploration/num paths total          1550\n",
      "evaluation/num steps total         293080\n",
      "evaluation/num paths total            507\n",
      "evaluation/path length Mean           923.5\n",
      "evaluation/path length Std            229.5\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            235\n",
      "evaluation/Rewards Mean                 5.2724\n",
      "evaluation/Rewards Std                  0.103119\n",
      "evaluation/Rewards Max                  5.47287\n",
      "evaluation/Rewards Min                  3.92671\n",
      "evaluation/Returns Mean              4869.06\n",
      "evaluation/Returns Std               1233.23\n",
      "evaluation/Returns Max               5290.53\n",
      "evaluation/Returns Min               1169.41\n",
      "evaluation/Estimation Bias Mean       922.872\n",
      "evaluation/Estimation Bias Std        202.322\n",
      "evaluation/EB/Q_True Mean              51.6258\n",
      "evaluation/EB/Q_True Std              152.105\n",
      "evaluation/EB/Q_Pred Mean             974.498\n",
      "evaluation/EB/Q_Pred Std               95.4116\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4869.06\n",
      "evaluation/Actions Mean                 0.0649665\n",
      "evaluation/Actions Std                  0.511729\n",
      "evaluation/Actions Max                  0.999996\n",
      "evaluation/Actions Min                 -0.999932\n",
      "time/backward_policy (s)                8.83626\n",
      "time/backward_zf1 (s)                  10.1653\n",
      "time/backward_zf2 (s)                   9.94241\n",
      "time/data sampling (s)                  1.85651\n",
      "time/data storing (s)                   0.086907\n",
      "time/evaluation sampling (s)            4.85334\n",
      "time/exploration sampling (s)           2.51237\n",
      "time/logging (s)                        0.0117282\n",
      "time/preback_alpha (s)                  0.0061889\n",
      "time/preback_policy (s)                14.6481\n",
      "time/preback_start (s)                  0.98661\n",
      "time/preback_zf (s)                    36.5644\n",
      "time/saving (s)                         3.01099e-06\n",
      "time/training (s)                      10.7577\n",
      "time/epoch (s)                        101.228\n",
      "time/total (s)                       5000.28\n",
      "Epoch                                  49\n",
      "---------------------------------  ----------------\n",
      "2024-11-02 00:29:47.154034 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 50 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 265000\n",
      "trainer/ZF1 Loss                      629.058\n",
      "trainer/ZF2 Loss                      864.592\n",
      "trainer/ZF Expert Reward               23.4654\n",
      "trainer/ZF Policy Reward                7.36183\n",
      "trainer/ZF CHI2 Term                  759.722\n",
      "trainer/Policy Loss                  -687.736\n",
      "trainer/expert_lambda Loss             26.9028\n",
      "trainer/expert_lambda Value            23.4806\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              329.955\n",
      "trainer/Policy Param Norm              38.9608\n",
      "trainer/Zf1 Grad Norm               22390.2\n",
      "trainer/Zf1 Param Norm                148.699\n",
      "trainer/Zf2 Grad Norm               14858.8\n",
      "trainer/Zf2 Param Norm                146.594\n",
      "trainer/Z Expert Predictions Mean    1070.39\n",
      "trainer/Z Expert Predictions Std       81.9697\n",
      "trainer/Z Expert Predictions Max     1246.68\n",
      "trainer/Z Expert Predictions Min      292.27\n",
      "trainer/Z Policy Predictions Mean     688.696\n",
      "trainer/Z Policy Predictions Std      347.071\n",
      "trainer/Z Policy Predictions Max     1029.15\n",
      "trainer/Z Policy Predictions Min      -20.9949\n",
      "trainer/Z Expert Targets Mean        1046.93\n",
      "trainer/Z Expert Targets Std           82.2068\n",
      "trainer/Z Expert Targets Max         1219.01\n",
      "trainer/Z Expert Targets Min          265.227\n",
      "trainer/Z Policy Targets Mean         681.335\n",
      "trainer/Z Policy Targets Std          350.681\n",
      "trainer/Z Policy Targets Max         1038.07\n",
      "trainer/Z Policy Targets Min          -25.9671\n",
      "trainer/Log Pis Mean                   25.7407\n",
      "trainer/Log Pis Std                     7.31367\n",
      "trainer/Policy mu Mean                  0.144154\n",
      "trainer/Policy mu Std                   1.2344\n",
      "trainer/Policy log std Mean            -1.95571\n",
      "trainer/Policy log std Std              0.942044\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        260792\n",
      "exploration/num paths total          1556\n",
      "evaluation/num steps total         302929\n",
      "evaluation/num paths total            517\n",
      "evaluation/path length Mean           984.9\n",
      "evaluation/path length Std             45.3\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            849\n",
      "evaluation/Rewards Mean                 5.30289\n",
      "evaluation/Rewards Std                  0.0987234\n",
      "evaluation/Rewards Max                  5.5122\n",
      "evaluation/Rewards Min                  4.31614\n",
      "evaluation/Returns Mean              5222.81\n",
      "evaluation/Returns Std                257.393\n",
      "evaluation/Returns Max               5318.15\n",
      "evaluation/Returns Min               4450.89\n",
      "evaluation/Estimation Bias Mean       843.024\n",
      "evaluation/Estimation Bias Std        182.535\n",
      "evaluation/EB/Q_True Mean              48.5924\n",
      "evaluation/EB/Q_True Std              148.432\n",
      "evaluation/EB/Q_Pred Mean             891.617\n",
      "evaluation/EB/Q_Pred Std               79.06\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5222.81\n",
      "evaluation/Actions Mean                 0.0627942\n",
      "evaluation/Actions Std                  0.515755\n",
      "evaluation/Actions Max                  0.99998\n",
      "evaluation/Actions Min                 -0.999986\n",
      "time/backward_policy (s)                7.83523\n",
      "time/backward_zf1 (s)                   9.13799\n",
      "time/backward_zf2 (s)                   8.78752\n",
      "time/data sampling (s)                  1.84236\n",
      "time/data storing (s)                   0.0842019\n",
      "time/evaluation sampling (s)            4.5569\n",
      "time/exploration sampling (s)           2.47187\n",
      "time/logging (s)                        0.0127144\n",
      "time/preback_alpha (s)                  0.00604811\n",
      "time/preback_policy (s)                16.0764\n",
      "time/preback_start (s)                  0.963492\n",
      "time/preback_zf (s)                    36.2905\n",
      "time/saving (s)                         3.086e-06\n",
      "time/training (s)                      11.4484\n",
      "time/epoch (s)                         99.5136\n",
      "time/total (s)                       5099.79\n",
      "Epoch                                  50\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:31:27.492271 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 51 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 270000\n",
      "trainer/ZF1 Loss                      109.696\n",
      "trainer/ZF2 Loss                      106.643\n",
      "trainer/ZF Expert Reward               22.2411\n",
      "trainer/ZF Policy Reward                5.11646\n",
      "trainer/ZF CHI2 Term                  120.38\n",
      "trainer/Policy Loss                  -657.499\n",
      "trainer/expert_lambda Loss             27.6114\n",
      "trainer/expert_lambda Value            23.5997\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              246.38\n",
      "trainer/Policy Param Norm              39.1468\n",
      "trainer/Zf1 Grad Norm                9923.25\n",
      "trainer/Zf1 Param Norm                150.037\n",
      "trainer/Zf2 Grad Norm                9984.59\n",
      "trainer/Zf2 Param Norm                147.945\n",
      "trainer/Z Expert Predictions Mean    1018.13\n",
      "trainer/Z Expert Predictions Std       59.9632\n",
      "trainer/Z Expert Predictions Max     1181.01\n",
      "trainer/Z Expert Predictions Min      466.251\n",
      "trainer/Z Policy Predictions Mean     653.672\n",
      "trainer/Z Policy Predictions Std      312.386\n",
      "trainer/Z Policy Predictions Max      978.929\n",
      "trainer/Z Policy Predictions Min      -19.9683\n",
      "trainer/Z Expert Targets Mean         995.887\n",
      "trainer/Z Expert Targets Std           60.5611\n",
      "trainer/Z Expert Targets Max         1150.83\n",
      "trainer/Z Expert Targets Min          449.991\n",
      "trainer/Z Policy Targets Mean         648.555\n",
      "trainer/Z Policy Targets Std          312.51\n",
      "trainer/Z Policy Targets Max          949.539\n",
      "trainer/Z Policy Targets Min          -25.9377\n",
      "trainer/Log Pis Mean                   25.7335\n",
      "trainer/Log Pis Std                     8.96979\n",
      "trainer/Policy mu Mean                  0.140119\n",
      "trainer/Policy mu Std                   1.21991\n",
      "trainer/Policy log std Mean            -1.94291\n",
      "trainer/Policy log std Std              0.890217\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        264792\n",
      "exploration/num paths total          1560\n",
      "evaluation/num steps total         312795\n",
      "evaluation/num paths total            528\n",
      "evaluation/path length Mean           896.909\n",
      "evaluation/path length Std            254.874\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            126\n",
      "evaluation/Rewards Mean                 5.26186\n",
      "evaluation/Rewards Std                  0.110412\n",
      "evaluation/Rewards Max                  5.59867\n",
      "evaluation/Rewards Min                  3.72741\n",
      "evaluation/Returns Mean              4719.41\n",
      "evaluation/Returns Std               1364.22\n",
      "evaluation/Returns Max               5278.39\n",
      "evaluation/Returns Min                587.45\n",
      "evaluation/Estimation Bias Mean       788.302\n",
      "evaluation/Estimation Bias Std        198.072\n",
      "evaluation/EB/Q_True Mean              48.1397\n",
      "evaluation/EB/Q_True Std              147.177\n",
      "evaluation/EB/Q_Pred Mean             836.442\n",
      "evaluation/EB/Q_Pred Std               94.7498\n",
      "evaluation/Num Paths                   11\n",
      "evaluation/Average Returns           4719.41\n",
      "evaluation/Actions Mean                 0.0738437\n",
      "evaluation/Actions Std                  0.502535\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999993\n",
      "time/backward_policy (s)                8.35776\n",
      "time/backward_zf1 (s)                   9.68909\n",
      "time/backward_zf2 (s)                   9.40806\n",
      "time/data sampling (s)                  1.79034\n",
      "time/data storing (s)                   0.0869292\n",
      "time/evaluation sampling (s)            4.49112\n",
      "time/exploration sampling (s)           2.49302\n",
      "time/logging (s)                        0.0127975\n",
      "time/preback_alpha (s)                  0.00617231\n",
      "time/preback_policy (s)                15.3127\n",
      "time/preback_start (s)                  0.988158\n",
      "time/preback_zf (s)                    36.4402\n",
      "time/saving (s)                         2.652e-06\n",
      "time/training (s)                      11.0533\n",
      "time/epoch (s)                        100.13\n",
      "time/total (s)                       5199.93\n",
      "Epoch                                  51\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:33:08.093243 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 52 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 275000\n",
      "trainer/ZF1 Loss                       99.9443\n",
      "trainer/ZF2 Loss                       89.1697\n",
      "trainer/ZF Expert Reward               28.1267\n",
      "trainer/ZF Policy Reward                4.98509\n",
      "trainer/ZF CHI2 Term                  113.299\n",
      "trainer/Policy Loss                  -596.13\n",
      "trainer/expert_lambda Loss             34.1207\n",
      "trainer/expert_lambda Value            23.7402\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              246.965\n",
      "trainer/Policy Param Norm              39.3084\n",
      "trainer/Zf1 Grad Norm                6304.88\n",
      "trainer/Zf1 Param Norm                151.32\n",
      "trainer/Zf2 Grad Norm                5485.69\n",
      "trainer/Zf2 Param Norm                149.297\n",
      "trainer/Z Expert Predictions Mean     952.439\n",
      "trainer/Z Expert Predictions Std       84.423\n",
      "trainer/Z Expert Predictions Max     1095.04\n",
      "trainer/Z Expert Predictions Min      -17.8251\n",
      "trainer/Z Policy Predictions Mean     592.771\n",
      "trainer/Z Policy Predictions Std      293.126\n",
      "trainer/Z Policy Predictions Max      901.546\n",
      "trainer/Z Policy Predictions Min       -8.44079\n",
      "trainer/Z Expert Targets Mean         924.312\n",
      "trainer/Z Expert Targets Std           83.1094\n",
      "trainer/Z Expert Targets Max         1046.77\n",
      "trainer/Z Expert Targets Min          -40.9471\n",
      "trainer/Z Policy Targets Mean         587.786\n",
      "trainer/Z Policy Targets Std          291.869\n",
      "trainer/Z Policy Targets Max          906.745\n",
      "trainer/Z Policy Targets Min          -13.5972\n",
      "trainer/Log Pis Mean                   24.5251\n",
      "trainer/Log Pis Std                     8.13273\n",
      "trainer/Policy mu Mean                  0.161853\n",
      "trainer/Policy mu Std                   1.21471\n",
      "trainer/Policy log std Mean            -1.87011\n",
      "trainer/Policy log std Std              0.878427\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        270792\n",
      "exploration/num paths total          1566\n",
      "evaluation/num steps total         322476\n",
      "evaluation/num paths total            538\n",
      "evaluation/path length Mean           968.1\n",
      "evaluation/path length Std             95.7\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            681\n",
      "evaluation/Rewards Mean                 5.28857\n",
      "evaluation/Rewards Std                  0.120269\n",
      "evaluation/Rewards Max                  5.78154\n",
      "evaluation/Rewards Min                  3.73311\n",
      "evaluation/Returns Mean              5119.87\n",
      "evaluation/Returns Std                523.185\n",
      "evaluation/Returns Max               5306.69\n",
      "evaluation/Returns Min               3550.68\n",
      "evaluation/Estimation Bias Mean       724.884\n",
      "evaluation/Estimation Bias Std        189.425\n",
      "evaluation/EB/Q_True Mean              49.3139\n",
      "evaluation/EB/Q_True Std              149.142\n",
      "evaluation/EB/Q_Pred Mean             774.198\n",
      "evaluation/EB/Q_Pred Std               98.2382\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5119.87\n",
      "evaluation/Actions Mean                 0.0867403\n",
      "evaluation/Actions Std                  0.502896\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.998911\n",
      "time/backward_policy (s)                8.41265\n",
      "time/backward_zf1 (s)                   9.724\n",
      "time/backward_zf2 (s)                   9.44883\n",
      "time/data sampling (s)                  1.81341\n",
      "time/data storing (s)                   0.0842601\n",
      "time/evaluation sampling (s)            4.60746\n",
      "time/exploration sampling (s)           2.47279\n",
      "time/logging (s)                        0.0122227\n",
      "time/preback_alpha (s)                  0.00614053\n",
      "time/preback_policy (s)                15.2936\n",
      "time/preback_start (s)                  0.973049\n",
      "time/preback_zf (s)                    36.4895\n",
      "time/saving (s)                         3.437e-06\n",
      "time/training (s)                      11.0526\n",
      "time/epoch (s)                        100.391\n",
      "time/total (s)                       5300.33\n",
      "Epoch                                  52\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:34:49.968361 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 53 finished\n",
      "---------------------------------  ----------------\n",
      "replay_buffer/size                 280000\n",
      "trainer/ZF1 Loss                      133.936\n",
      "trainer/ZF2 Loss                      107.425\n",
      "trainer/ZF Expert Reward               28.4347\n",
      "trainer/ZF Policy Reward                4.43964\n",
      "trainer/ZF CHI2 Term                  140.284\n",
      "trainer/Policy Loss                  -603.266\n",
      "trainer/expert_lambda Loss             44.5166\n",
      "trainer/expert_lambda Value            23.8985\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              277.876\n",
      "trainer/Policy Param Norm              39.4664\n",
      "trainer/Zf1 Grad Norm                9925.62\n",
      "trainer/Zf1 Param Norm                152.605\n",
      "trainer/Zf2 Grad Norm                8316.22\n",
      "trainer/Zf2 Param Norm                150.611\n",
      "trainer/Z Expert Predictions Mean     899.128\n",
      "trainer/Z Expert Predictions Std       81.4065\n",
      "trainer/Z Expert Predictions Max      982.74\n",
      "trainer/Z Expert Predictions Min       -6.97498\n",
      "trainer/Z Policy Predictions Mean     600.89\n",
      "trainer/Z Policy Predictions Std      257.202\n",
      "trainer/Z Policy Predictions Max      841.874\n",
      "trainer/Z Policy Predictions Min      -22.2004\n",
      "trainer/Z Expert Targets Mean         870.693\n",
      "trainer/Z Expert Targets Std           83.459\n",
      "trainer/Z Expert Targets Max          956.211\n",
      "trainer/Z Expert Targets Min          -36.7671\n",
      "trainer/Z Policy Targets Mean         596.45\n",
      "trainer/Z Policy Targets Std          256.258\n",
      "trainer/Z Policy Targets Max          854.504\n",
      "trainer/Z Policy Targets Min          -15.3293\n",
      "trainer/Log Pis Mean                   24.4659\n",
      "trainer/Log Pis Std                     8.04165\n",
      "trainer/Policy mu Mean                  0.12237\n",
      "trainer/Policy mu Std                   1.11575\n",
      "trainer/Policy log std Mean            -1.99718\n",
      "trainer/Policy log std Std              0.842313\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        274792\n",
      "exploration/num paths total          1570\n",
      "evaluation/num steps total         332476\n",
      "evaluation/num paths total            548\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.3505\n",
      "evaluation/Rewards Std                  0.0825375\n",
      "evaluation/Rewards Max                  5.54475\n",
      "evaluation/Rewards Min                  4.82145\n",
      "evaluation/Returns Mean              5350.5\n",
      "evaluation/Returns Std                  2.70353\n",
      "evaluation/Returns Max               5356.12\n",
      "evaluation/Returns Min               5347.71\n",
      "evaluation/Estimation Bias Mean       691.407\n",
      "evaluation/Estimation Bias Std        153.206\n",
      "evaluation/EB/Q_True Mean              48.3014\n",
      "evaluation/EB/Q_True Std              148.756\n",
      "evaluation/EB/Q_Pred Mean             739.708\n",
      "evaluation/EB/Q_Pred Std               34.9658\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5350.5\n",
      "evaluation/Actions Mean                 0.10159\n",
      "evaluation/Actions Std                  0.53147\n",
      "evaluation/Actions Max                  0.998879\n",
      "evaluation/Actions Min                 -0.99854\n",
      "time/backward_policy (s)                8.91793\n",
      "time/backward_zf1 (s)                  10.2692\n",
      "time/backward_zf2 (s)                  10.0201\n",
      "time/data sampling (s)                  1.83804\n",
      "time/data storing (s)                   0.0864123\n",
      "time/evaluation sampling (s)            4.7778\n",
      "time/exploration sampling (s)           2.48453\n",
      "time/logging (s)                        0.0174334\n",
      "time/preback_alpha (s)                  0.00631481\n",
      "time/preback_policy (s)                14.7404\n",
      "time/preback_start (s)                  0.989474\n",
      "time/preback_zf (s)                    36.5886\n",
      "time/saving (s)                         5.69101e-06\n",
      "time/training (s)                      10.9335\n",
      "time/epoch (s)                        101.67\n",
      "time/total (s)                       5402\n",
      "Epoch                                  53\n",
      "---------------------------------  ----------------\n",
      "2024-11-02 00:36:29.994422 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 54 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 285000\n",
      "trainer/ZF1 Loss                       85.6835\n",
      "trainer/ZF2 Loss                       81.986\n",
      "trainer/ZF Expert Reward               25.7678\n",
      "trainer/ZF Policy Reward                3.48692\n",
      "trainer/ZF CHI2 Term                  101.325\n",
      "trainer/Policy Loss                  -524.069\n",
      "trainer/expert_lambda Loss             25.6171\n",
      "trainer/expert_lambda Value            24.0804\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              234.223\n",
      "trainer/Policy Param Norm              39.6205\n",
      "trainer/Zf1 Grad Norm                4802.67\n",
      "trainer/Zf1 Param Norm                153.952\n",
      "trainer/Zf2 Grad Norm                4602.79\n",
      "trainer/Zf2 Param Norm                151.999\n",
      "trainer/Z Expert Predictions Mean     844.037\n",
      "trainer/Z Expert Predictions Std       67.9435\n",
      "trainer/Z Expert Predictions Max      976.814\n",
      "trainer/Z Expert Predictions Min       -4.82527\n",
      "trainer/Z Policy Predictions Mean     523.332\n",
      "trainer/Z Policy Predictions Std      266.887\n",
      "trainer/Z Policy Predictions Max      811.983\n",
      "trainer/Z Policy Predictions Min       -2.89711\n",
      "trainer/Z Expert Targets Mean         818.269\n",
      "trainer/Z Expert Targets Std           67.8864\n",
      "trainer/Z Expert Targets Max          947.988\n",
      "trainer/Z Expert Targets Min          -34.5352\n",
      "trainer/Z Policy Targets Mean         519.845\n",
      "trainer/Z Policy Targets Std          265.9\n",
      "trainer/Z Policy Targets Max          802.918\n",
      "trainer/Z Policy Targets Min           -7.0864\n",
      "trainer/Log Pis Mean                   24.0806\n",
      "trainer/Log Pis Std                     7.84677\n",
      "trainer/Policy mu Mean                  0.225486\n",
      "trainer/Policy mu Std                   1.23047\n",
      "trainer/Policy log std Mean            -1.81752\n",
      "trainer/Policy log std Std              0.903639\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        280792\n",
      "exploration/num paths total          1576\n",
      "evaluation/num steps total         342476\n",
      "evaluation/num paths total            558\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.31352\n",
      "evaluation/Rewards Std                  0.0828407\n",
      "evaluation/Rewards Max                  5.61819\n",
      "evaluation/Rewards Min                  4.76594\n",
      "evaluation/Returns Mean              5313.52\n",
      "evaluation/Returns Std                  4.17371\n",
      "evaluation/Returns Max               5319.56\n",
      "evaluation/Returns Min               5305.79\n",
      "evaluation/Estimation Bias Mean       654.228\n",
      "evaluation/Estimation Bias Std        153.586\n",
      "evaluation/EB/Q_True Mean              47.9306\n",
      "evaluation/EB/Q_True Std              147.644\n",
      "evaluation/EB/Q_Pred Mean             702.158\n",
      "evaluation/EB/Q_Pred Std               34.3391\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5313.52\n",
      "evaluation/Actions Mean                 0.0747466\n",
      "evaluation/Actions Std                  0.516437\n",
      "evaluation/Actions Max                  0.999296\n",
      "evaluation/Actions Min                 -0.996053\n",
      "time/backward_policy (s)                7.94762\n",
      "time/backward_zf1 (s)                   9.27153\n",
      "time/backward_zf2 (s)                   8.92117\n",
      "time/data sampling (s)                  1.79956\n",
      "time/data storing (s)                   0.0839843\n",
      "time/evaluation sampling (s)            4.62167\n",
      "time/exploration sampling (s)           2.42409\n",
      "time/logging (s)                        0.0126662\n",
      "time/preback_alpha (s)                  0.00608266\n",
      "time/preback_policy (s)                15.9648\n",
      "time/preback_start (s)                  0.973903\n",
      "time/preback_zf (s)                    36.3497\n",
      "time/saving (s)                         2.83e-06\n",
      "time/training (s)                      11.4373\n",
      "time/epoch (s)                         99.8141\n",
      "time/total (s)                       5501.82\n",
      "Epoch                                  54\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:38:14.044733 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 55 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 290000\n",
      "trainer/ZF1 Loss                       83.7258\n",
      "trainer/ZF2 Loss                       80.7114\n",
      "trainer/ZF Expert Reward               24.0677\n",
      "trainer/ZF Policy Reward                1.92361\n",
      "trainer/ZF CHI2 Term                   98.3491\n",
      "trainer/Policy Loss                  -523.232\n",
      "trainer/expert_lambda Loss             25.7052\n",
      "trainer/expert_lambda Value            24.2693\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              264.821\n",
      "trainer/Policy Param Norm              39.7727\n",
      "trainer/Zf1 Grad Norm                5280.97\n",
      "trainer/Zf1 Param Norm                155.292\n",
      "trainer/Zf2 Grad Norm                7259.13\n",
      "trainer/Zf2 Param Norm                153.349\n",
      "trainer/Z Expert Predictions Mean     809.705\n",
      "trainer/Z Expert Predictions Std       81.4062\n",
      "trainer/Z Expert Predictions Max      942.613\n",
      "trainer/Z Expert Predictions Min       -7.05002\n",
      "trainer/Z Policy Predictions Mean     519.522\n",
      "trainer/Z Policy Predictions Std      242.255\n",
      "trainer/Z Policy Predictions Max      747.28\n",
      "trainer/Z Policy Predictions Min      -15.2256\n",
      "trainer/Z Expert Targets Mean         785.637\n",
      "trainer/Z Expert Targets Std           81.6856\n",
      "trainer/Z Expert Targets Max          904.597\n",
      "trainer/Z Expert Targets Min          -35.4946\n",
      "trainer/Z Policy Targets Mean         517.598\n",
      "trainer/Z Policy Targets Std          242.764\n",
      "trainer/Z Policy Targets Max          751.534\n",
      "trainer/Z Policy Targets Min          -15.6238\n",
      "trainer/Log Pis Mean                   24.1304\n",
      "trainer/Log Pis Std                     7.35313\n",
      "trainer/Policy mu Mean                  0.160599\n",
      "trainer/Policy mu Std                   1.19747\n",
      "trainer/Policy log std Mean            -1.91162\n",
      "trainer/Policy log std Std              0.903219\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        284792\n",
      "exploration/num paths total          1580\n",
      "evaluation/num steps total         351776\n",
      "evaluation/num paths total            568\n",
      "evaluation/path length Mean           930\n",
      "evaluation/path length Std            210\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            300\n",
      "evaluation/Rewards Mean                 5.32401\n",
      "evaluation/Rewards Std                  0.087003\n",
      "evaluation/Rewards Max                  5.78658\n",
      "evaluation/Rewards Min                  4.80156\n",
      "evaluation/Returns Mean              4951.33\n",
      "evaluation/Returns Std               1119.93\n",
      "evaluation/Returns Max               5331.25\n",
      "evaluation/Returns Min               1591.54\n",
      "evaluation/Estimation Bias Mean       605.726\n",
      "evaluation/Estimation Bias Std        170.269\n",
      "evaluation/EB/Q_True Mean              51.7083\n",
      "evaluation/EB/Q_True Std              152.949\n",
      "evaluation/EB/Q_Pred Mean             657.435\n",
      "evaluation/EB/Q_Pred Std               49.9176\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4951.33\n",
      "evaluation/Actions Mean                 0.0591696\n",
      "evaluation/Actions Std                  0.501928\n",
      "evaluation/Actions Max                  0.999275\n",
      "evaluation/Actions Min                 -0.998038\n",
      "time/backward_policy (s)                8.93754\n",
      "time/backward_zf1 (s)                  10.7336\n",
      "time/backward_zf2 (s)                  10.1846\n",
      "time/data sampling (s)                  2.01863\n",
      "time/data storing (s)                   0.0960668\n",
      "time/evaluation sampling (s)            3.45297\n",
      "time/exploration sampling (s)           2.55848\n",
      "time/logging (s)                        0.0126298\n",
      "time/preback_alpha (s)                  0.00655556\n",
      "time/preback_policy (s)                15.4124\n",
      "time/preback_start (s)                  1.09548\n",
      "time/preback_zf (s)                    37.7782\n",
      "time/saving (s)                         6.568e-06\n",
      "time/training (s)                      11.5322\n",
      "time/epoch (s)                        103.819\n",
      "time/total (s)                       5605.65\n",
      "Epoch                                  55\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:39:58.335466 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 56 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 295000\n",
      "trainer/ZF1 Loss                       59.866\n",
      "trainer/ZF2 Loss                       63.5266\n",
      "trainer/ZF Expert Reward               26.2368\n",
      "trainer/ZF Policy Reward                6.25864\n",
      "trainer/ZF CHI2 Term                   80.4593\n",
      "trainer/Policy Loss                  -474.434\n",
      "trainer/expert_lambda Loss             22.0324\n",
      "trainer/expert_lambda Value            24.4688\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              224.904\n",
      "trainer/Policy Param Norm              39.9136\n",
      "trainer/Zf1 Grad Norm                3644.39\n",
      "trainer/Zf1 Param Norm                156.584\n",
      "trainer/Zf2 Grad Norm                4714.88\n",
      "trainer/Zf2 Param Norm                154.646\n",
      "trainer/Z Expert Predictions Mean     763.355\n",
      "trainer/Z Expert Predictions Std       60.7464\n",
      "trainer/Z Expert Predictions Max      856.732\n",
      "trainer/Z Expert Predictions Min       -5.41378\n",
      "trainer/Z Policy Predictions Mean     472.401\n",
      "trainer/Z Policy Predictions Std      229.247\n",
      "trainer/Z Policy Predictions Max      725.999\n",
      "trainer/Z Policy Predictions Min       -1.09236\n",
      "trainer/Z Expert Targets Mean         737.119\n",
      "trainer/Z Expert Targets Std           60.8016\n",
      "trainer/Z Expert Targets Max          826.048\n",
      "trainer/Z Expert Targets Min          -33.8217\n",
      "trainer/Z Policy Targets Mean         466.142\n",
      "trainer/Z Policy Targets Std          227.272\n",
      "trainer/Z Policy Targets Max          708.665\n",
      "trainer/Z Policy Targets Min           -5.2336\n",
      "trainer/Log Pis Mean                   23.7032\n",
      "trainer/Log Pis Std                     8.12876\n",
      "trainer/Policy mu Mean                  0.151399\n",
      "trainer/Policy mu Std                   1.17663\n",
      "trainer/Policy log std Mean            -1.85223\n",
      "trainer/Policy log std Std              0.838762\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        290792\n",
      "exploration/num paths total          1586\n",
      "evaluation/num steps total         360861\n",
      "evaluation/num paths total            578\n",
      "evaluation/path length Mean           908.5\n",
      "evaluation/path length Std            274.5\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             85\n",
      "evaluation/Rewards Mean                 5.30771\n",
      "evaluation/Rewards Std                  0.0866454\n",
      "evaluation/Rewards Max                  5.88538\n",
      "evaluation/Rewards Min                  4.74562\n",
      "evaluation/Returns Mean              4822.06\n",
      "evaluation/Returns Std               1458.7\n",
      "evaluation/Returns Max               5315.64\n",
      "evaluation/Returns Min                445.969\n",
      "evaluation/Estimation Bias Mean       566.428\n",
      "evaluation/Estimation Bias Std        166.824\n",
      "evaluation/EB/Q_True Mean              52.7385\n",
      "evaluation/EB/Q_True Std              153.968\n",
      "evaluation/EB/Q_Pred Mean             619.167\n",
      "evaluation/EB/Q_Pred Std               47.0649\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4822.06\n",
      "evaluation/Actions Mean                 0.0710652\n",
      "evaluation/Actions Std                  0.509184\n",
      "evaluation/Actions Max                  0.999525\n",
      "evaluation/Actions Min                 -0.998973\n",
      "time/backward_policy (s)                9.12232\n",
      "time/backward_zf1 (s)                  10.8152\n",
      "time/backward_zf2 (s)                  10.3338\n",
      "time/data sampling (s)                  2.08052\n",
      "time/data storing (s)                   0.090398\n",
      "time/evaluation sampling (s)            3.69782\n",
      "time/exploration sampling (s)           2.62002\n",
      "time/logging (s)                        0.0113321\n",
      "time/preback_alpha (s)                  0.00658783\n",
      "time/preback_policy (s)                15.0965\n",
      "time/preback_start (s)                  1.06068\n",
      "time/preback_zf (s)                    37.6678\n",
      "time/saving (s)                         2.775e-06\n",
      "time/training (s)                      11.4573\n",
      "time/epoch (s)                        104.06\n",
      "time/total (s)                       5709.71\n",
      "Epoch                                  56\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:41:42.466684 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 57 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 300000\n",
      "trainer/ZF1 Loss                      100.021\n",
      "trainer/ZF2 Loss                       82.9477\n",
      "trainer/ZF Expert Reward               23.95\n",
      "trainer/ZF Policy Reward                2.7082\n",
      "trainer/ZF CHI2 Term                  108.253\n",
      "trainer/Policy Loss                  -466.09\n",
      "trainer/expert_lambda Loss             26.1489\n",
      "trainer/expert_lambda Value            24.6696\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              224.265\n",
      "trainer/Policy Param Norm              40.0456\n",
      "trainer/Zf1 Grad Norm                7217.39\n",
      "trainer/Zf1 Param Norm                157.839\n",
      "trainer/Zf2 Grad Norm                6064.25\n",
      "trainer/Zf2 Param Norm                155.952\n",
      "trainer/Z Expert Predictions Mean     735.72\n",
      "trainer/Z Expert Predictions Std       37.1643\n",
      "trainer/Z Expert Predictions Max      853.851\n",
      "trainer/Z Expert Predictions Min      548.657\n",
      "trainer/Z Policy Predictions Mean     463.311\n",
      "trainer/Z Policy Predictions Std      195.915\n",
      "trainer/Z Policy Predictions Max      662.178\n",
      "trainer/Z Policy Predictions Min        2.56156\n",
      "trainer/Z Expert Targets Mean         711.77\n",
      "trainer/Z Expert Targets Std           37.3682\n",
      "trainer/Z Expert Targets Max          819.662\n",
      "trainer/Z Expert Targets Min          522.662\n",
      "trainer/Z Policy Targets Mean         460.603\n",
      "trainer/Z Policy Targets Std          195.861\n",
      "trainer/Z Policy Targets Max          674.487\n",
      "trainer/Z Policy Targets Min           -1.02749\n",
      "trainer/Log Pis Mean                   23.4465\n",
      "trainer/Log Pis Std                     6.58976\n",
      "trainer/Policy mu Mean                  0.148777\n",
      "trainer/Policy mu Std                   1.12328\n",
      "trainer/Policy log std Mean            -1.92276\n",
      "trainer/Policy log std Std              0.816255\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        294792\n",
      "exploration/num paths total          1590\n",
      "evaluation/num steps total         370861\n",
      "evaluation/num paths total            588\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.33225\n",
      "evaluation/Rewards Std                  0.0833559\n",
      "evaluation/Rewards Max                  5.50519\n",
      "evaluation/Rewards Min                  4.74141\n",
      "evaluation/Returns Mean              5332.25\n",
      "evaluation/Returns Std                  3.36172\n",
      "evaluation/Returns Max               5335.52\n",
      "evaluation/Returns Min               5323.17\n",
      "evaluation/Estimation Bias Mean       535.383\n",
      "evaluation/Estimation Bias Std        151.526\n",
      "evaluation/EB/Q_True Mean              48.158\n",
      "evaluation/EB/Q_True Std              148.319\n",
      "evaluation/EB/Q_Pred Mean             583.541\n",
      "evaluation/EB/Q_Pred Std               40.3148\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5332.25\n",
      "evaluation/Actions Mean                 0.0613882\n",
      "evaluation/Actions Std                  0.528252\n",
      "evaluation/Actions Max                  0.999684\n",
      "evaluation/Actions Min                 -0.996776\n",
      "time/backward_policy (s)                8.84014\n",
      "time/backward_zf1 (s)                  10.4576\n",
      "time/backward_zf2 (s)                   9.98581\n",
      "time/data sampling (s)                  1.98043\n",
      "time/data storing (s)                   0.0906821\n",
      "time/evaluation sampling (s)            5.00769\n",
      "time/exploration sampling (s)           2.57345\n",
      "time/logging (s)                        0.0127722\n",
      "time/preback_alpha (s)                  0.00641994\n",
      "time/preback_policy (s)                15.2857\n",
      "time/preback_start (s)                  1.03234\n",
      "time/preback_zf (s)                    37.1908\n",
      "time/saving (s)                         2.7e-06\n",
      "time/training (s)                      11.45\n",
      "time/epoch (s)                        103.914\n",
      "time/total (s)                       5813.63\n",
      "Epoch                                  57\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:43:28.463871 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 58 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 305000\n",
      "trainer/ZF1 Loss                       96.8846\n",
      "trainer/ZF2 Loss                      151.809\n",
      "trainer/ZF Expert Reward               25.5603\n",
      "trainer/ZF Policy Reward                4.74047\n",
      "trainer/ZF CHI2 Term                  143.2\n",
      "trainer/Policy Loss                  -447.427\n",
      "trainer/expert_lambda Loss             26.2075\n",
      "trainer/expert_lambda Value            24.8923\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              197.168\n",
      "trainer/Policy Param Norm              40.1864\n",
      "trainer/Zf1 Grad Norm                5135.22\n",
      "trainer/Zf1 Param Norm                159.082\n",
      "trainer/Zf2 Grad Norm                5726.17\n",
      "trainer/Zf2 Param Norm                157.269\n",
      "trainer/Z Expert Predictions Mean     688.039\n",
      "trainer/Z Expert Predictions Std       41.5736\n",
      "trainer/Z Expert Predictions Max      808.254\n",
      "trainer/Z Expert Predictions Min      303.24\n",
      "trainer/Z Policy Predictions Mean     446.225\n",
      "trainer/Z Policy Predictions Std      185.099\n",
      "trainer/Z Policy Predictions Max      627.068\n",
      "trainer/Z Policy Predictions Min       -4.74075\n",
      "trainer/Z Expert Targets Mean         662.479\n",
      "trainer/Z Expert Targets Std           42.2882\n",
      "trainer/Z Expert Targets Max          778.521\n",
      "trainer/Z Expert Targets Min          299.516\n",
      "trainer/Z Policy Targets Mean         441.484\n",
      "trainer/Z Policy Targets Std          185.784\n",
      "trainer/Z Policy Targets Max          654.449\n",
      "trainer/Z Policy Targets Min           -3.12313\n",
      "trainer/Log Pis Mean                   23.2761\n",
      "trainer/Log Pis Std                     8.03052\n",
      "trainer/Policy mu Mean                  0.194232\n",
      "trainer/Policy mu Std                   1.12279\n",
      "trainer/Policy log std Mean            -1.89926\n",
      "trainer/Policy log std Std              0.790446\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        300888\n",
      "exploration/num paths total          1597\n",
      "evaluation/num steps total         379282\n",
      "evaluation/num paths total            598\n",
      "evaluation/path length Mean           842.1\n",
      "evaluation/path length Std            318.735\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            114\n",
      "evaluation/Rewards Mean                 5.2929\n",
      "evaluation/Rewards Std                  0.113911\n",
      "evaluation/Rewards Max                  6.29297\n",
      "evaluation/Rewards Min                  4.42772\n",
      "evaluation/Returns Mean              4457.16\n",
      "evaluation/Returns Std               1709.14\n",
      "evaluation/Returns Max               5312.05\n",
      "evaluation/Returns Min                598.613\n",
      "evaluation/Estimation Bias Mean       450.044\n",
      "evaluation/Estimation Bias Std        203.212\n",
      "evaluation/EB/Q_True Mean              56.8679\n",
      "evaluation/EB/Q_True Std              159.138\n",
      "evaluation/EB/Q_Pred Mean             506.912\n",
      "evaluation/EB/Q_Pred Std               70.1983\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4457.16\n",
      "evaluation/Actions Mean                 0.0859807\n",
      "evaluation/Actions Std                  0.526794\n",
      "evaluation/Actions Max                  0.999999\n",
      "evaluation/Actions Min                 -0.9993\n",
      "time/backward_policy (s)                9.00172\n",
      "time/backward_zf1 (s)                  10.91\n",
      "time/backward_zf2 (s)                  10.2662\n",
      "time/data sampling (s)                  2.06757\n",
      "time/data storing (s)                   0.0948223\n",
      "time/evaluation sampling (s)            5.16693\n",
      "time/exploration sampling (s)           2.63665\n",
      "time/logging (s)                        0.0112404\n",
      "time/preback_alpha (s)                  0.00655036\n",
      "time/preback_policy (s)                15.1461\n",
      "time/preback_start (s)                  1.0893\n",
      "time/preback_zf (s)                    37.8543\n",
      "time/saving (s)                         2.854e-06\n",
      "time/training (s)                      11.5129\n",
      "time/epoch (s)                        105.764\n",
      "time/total (s)                       5919.4\n",
      "Epoch                                  58\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:45:12.302043 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 59 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 310000\n",
      "trainer/ZF1 Loss                       51.0451\n",
      "trainer/ZF2 Loss                       53.1162\n",
      "trainer/ZF Expert Reward               25.2198\n",
      "trainer/ZF Policy Reward                3.55382\n",
      "trainer/ZF CHI2 Term                   71.0939\n",
      "trainer/Policy Loss                  -399.061\n",
      "trainer/expert_lambda Loss             22.4152\n",
      "trainer/expert_lambda Value            25.1591\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              198.629\n",
      "trainer/Policy Param Norm              40.3208\n",
      "trainer/Zf1 Grad Norm                5353.08\n",
      "trainer/Zf1 Param Norm                160.251\n",
      "trainer/Zf2 Grad Norm                4030.15\n",
      "trainer/Zf2 Param Norm                158.419\n",
      "trainer/Z Expert Predictions Mean     637.501\n",
      "trainer/Z Expert Predictions Std       66.1736\n",
      "trainer/Z Expert Predictions Max      712.167\n",
      "trainer/Z Expert Predictions Min        4.31416\n",
      "trainer/Z Policy Predictions Mean     399.669\n",
      "trainer/Z Policy Predictions Std      180.526\n",
      "trainer/Z Policy Predictions Max      603.896\n",
      "trainer/Z Policy Predictions Min        4.18195\n",
      "trainer/Z Expert Targets Mean         612.282\n",
      "trainer/Z Expert Targets Std           67.0523\n",
      "trainer/Z Expert Targets Max          691.785\n",
      "trainer/Z Expert Targets Min          -24.4951\n",
      "trainer/Z Policy Targets Mean         396.115\n",
      "trainer/Z Policy Targets Std          181.97\n",
      "trainer/Z Policy Targets Max          615.186\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   21.3459\n",
      "trainer/Log Pis Std                     8.44887\n",
      "trainer/Policy mu Mean                  0.212858\n",
      "trainer/Policy mu Std                   1.16378\n",
      "trainer/Policy log std Mean            -1.74162\n",
      "trainer/Policy log std Std              0.806111\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        305099\n",
      "exploration/num paths total          1603\n",
      "evaluation/num steps total         389282\n",
      "evaluation/num paths total            608\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.29287\n",
      "evaluation/Rewards Std                  0.0738122\n",
      "evaluation/Rewards Max                  5.45918\n",
      "evaluation/Rewards Min                  4.80202\n",
      "evaluation/Returns Mean              5292.87\n",
      "evaluation/Returns Std                  4.11237\n",
      "evaluation/Returns Max               5298.57\n",
      "evaluation/Returns Min               5287.01\n",
      "evaluation/Estimation Bias Mean       459.146\n",
      "evaluation/Estimation Bias Std        151.591\n",
      "evaluation/EB/Q_True Mean              47.8087\n",
      "evaluation/EB/Q_True Std              147.235\n",
      "evaluation/EB/Q_Pred Mean             506.954\n",
      "evaluation/EB/Q_Pred Std               27.7727\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5292.87\n",
      "evaluation/Actions Mean                 0.0724654\n",
      "evaluation/Actions Std                  0.503537\n",
      "evaluation/Actions Max                  0.999608\n",
      "evaluation/Actions Min                 -0.99846\n",
      "time/backward_policy (s)                8.82438\n",
      "time/backward_zf1 (s)                  10.5792\n",
      "time/backward_zf2 (s)                  10.0636\n",
      "time/data sampling (s)                  2.05179\n",
      "time/data storing (s)                   0.0903142\n",
      "time/evaluation sampling (s)            3.54342\n",
      "time/exploration sampling (s)           2.57484\n",
      "time/logging (s)                        0.0144064\n",
      "time/preback_alpha (s)                  0.00646677\n",
      "time/preback_policy (s)                15.5246\n",
      "time/preback_start (s)                  1.05133\n",
      "time/preback_zf (s)                    37.6718\n",
      "time/saving (s)                         3.568e-06\n",
      "time/training (s)                      11.6193\n",
      "time/epoch (s)                        103.615\n",
      "time/total (s)                       6023.02\n",
      "Epoch                                  59\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:46:59.786348 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 60 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 315000\n",
      "trainer/ZF1 Loss                       61.2554\n",
      "trainer/ZF2 Loss                       49.422\n",
      "trainer/ZF Expert Reward               29.6228\n",
      "trainer/ZF Policy Reward                5.40224\n",
      "trainer/ZF CHI2 Term                   78.9552\n",
      "trainer/Policy Loss                  -389.98\n",
      "trainer/expert_lambda Loss             39.6939\n",
      "trainer/expert_lambda Value            25.4127\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              227.557\n",
      "trainer/Policy Param Norm              40.4535\n",
      "trainer/Zf1 Grad Norm                4367.07\n",
      "trainer/Zf1 Param Norm                161.481\n",
      "trainer/Zf2 Grad Norm                3824.83\n",
      "trainer/Zf2 Param Norm                159.65\n",
      "trainer/Z Expert Predictions Mean     617.399\n",
      "trainer/Z Expert Predictions Std       64.8526\n",
      "trainer/Z Expert Predictions Max      694.011\n",
      "trainer/Z Expert Predictions Min        4.13659\n",
      "trainer/Z Policy Predictions Mean     390.639\n",
      "trainer/Z Policy Predictions Std      171.524\n",
      "trainer/Z Policy Predictions Max      550.111\n",
      "trainer/Z Policy Predictions Min        3.66275\n",
      "trainer/Z Expert Targets Mean         587.776\n",
      "trainer/Z Expert Targets Std           66.4751\n",
      "trainer/Z Expert Targets Max          668.043\n",
      "trainer/Z Expert Targets Min          -24.3514\n",
      "trainer/Z Policy Targets Mean         385.237\n",
      "trainer/Z Policy Targets Std          171.082\n",
      "trainer/Z Policy Targets Max          557.179\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   21.7801\n",
      "trainer/Log Pis Std                     8.44415\n",
      "trainer/Policy mu Mean                  0.239088\n",
      "trainer/Policy mu Std                   1.12968\n",
      "trainer/Policy log std Mean            -1.83158\n",
      "trainer/Policy log std Std              0.830155\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        309099\n",
      "exploration/num paths total          1607\n",
      "evaluation/num steps total         397538\n",
      "evaluation/num paths total            618\n",
      "evaluation/path length Mean           825.6\n",
      "evaluation/path length Std            349.171\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             92\n",
      "evaluation/Rewards Mean                 5.28531\n",
      "evaluation/Rewards Std                  0.105407\n",
      "evaluation/Rewards Max                  6.46004\n",
      "evaluation/Rewards Min                  4.67426\n",
      "evaluation/Returns Mean              4363.55\n",
      "evaluation/Returns Std               1850.44\n",
      "evaluation/Returns Max               5298.18\n",
      "evaluation/Returns Min                478.228\n",
      "evaluation/Estimation Bias Mean       408.601\n",
      "evaluation/Estimation Bias Std        189.117\n",
      "evaluation/EB/Q_True Mean              57.8214\n",
      "evaluation/EB/Q_True Std              159.971\n",
      "evaluation/EB/Q_Pred Mean             466.423\n",
      "evaluation/EB/Q_Pred Std               59.1674\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4363.55\n",
      "evaluation/Actions Mean                 0.0574985\n",
      "evaluation/Actions Std                  0.514399\n",
      "evaluation/Actions Max                  0.999919\n",
      "evaluation/Actions Min                 -0.999362\n",
      "time/backward_policy (s)                9.19664\n",
      "time/backward_zf1 (s)                  11.2738\n",
      "time/backward_zf2 (s)                  10.5147\n",
      "time/data sampling (s)                  2.16113\n",
      "time/data storing (s)                   0.0995418\n",
      "time/evaluation sampling (s)            4.26004\n",
      "time/exploration sampling (s)           2.70746\n",
      "time/logging (s)                        0.0107119\n",
      "time/preback_alpha (s)                  0.00667542\n",
      "time/preback_policy (s)                15.4402\n",
      "time/preback_start (s)                  1.10555\n",
      "time/preback_zf (s)                    38.6563\n",
      "time/saving (s)                         2.784e-06\n",
      "time/training (s)                      11.8075\n",
      "time/epoch (s)                        107.24\n",
      "time/total (s)                       6130.27\n",
      "Epoch                                  60\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:48:44.796917 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 61 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 320000\n",
      "trainer/ZF1 Loss                       39.5852\n",
      "trainer/ZF2 Loss                       44.1984\n",
      "trainer/ZF Expert Reward               25.4368\n",
      "trainer/ZF Policy Reward                4.39243\n",
      "trainer/ZF CHI2 Term                   61.5239\n",
      "trainer/Policy Loss                  -383.475\n",
      "trainer/expert_lambda Loss             20.7272\n",
      "trainer/expert_lambda Value            25.6551\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              191.234\n",
      "trainer/Policy Param Norm              40.5927\n",
      "trainer/Zf1 Grad Norm                3466.32\n",
      "trainer/Zf1 Param Norm                162.774\n",
      "trainer/Zf2 Grad Norm                4712.54\n",
      "trainer/Zf2 Param Norm                160.998\n",
      "trainer/Z Expert Predictions Mean     598.106\n",
      "trainer/Z Expert Predictions Std       62.9938\n",
      "trainer/Z Expert Predictions Max      669.601\n",
      "trainer/Z Expert Predictions Min       26.4092\n",
      "trainer/Z Policy Predictions Mean     383.994\n",
      "trainer/Z Policy Predictions Std      138.371\n",
      "trainer/Z Policy Predictions Max      538.176\n",
      "trainer/Z Policy Predictions Min       10.3049\n",
      "trainer/Z Expert Targets Mean         572.67\n",
      "trainer/Z Expert Targets Std           63.3903\n",
      "trainer/Z Expert Targets Max          640.663\n",
      "trainer/Z Expert Targets Min           -0.0902226\n",
      "trainer/Z Policy Targets Mean         379.601\n",
      "trainer/Z Policy Targets Std          138.725\n",
      "trainer/Z Policy Targets Max          516.221\n",
      "trainer/Z Policy Targets Min            6.59926\n",
      "trainer/Log Pis Mean                   22.6914\n",
      "trainer/Log Pis Std                     7.36443\n",
      "trainer/Policy mu Mean                  0.204712\n",
      "trainer/Policy mu Std                   1.1238\n",
      "trainer/Policy log std Mean            -1.88194\n",
      "trainer/Policy log std Std              0.777338\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        315099\n",
      "exploration/num paths total          1613\n",
      "evaluation/num steps total         407538\n",
      "evaluation/num paths total            628\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.29289\n",
      "evaluation/Rewards Std                  0.0757226\n",
      "evaluation/Rewards Max                  5.49927\n",
      "evaluation/Rewards Min                  4.79215\n",
      "evaluation/Returns Mean              5292.89\n",
      "evaluation/Returns Std                  5.94825\n",
      "evaluation/Returns Max               5301.73\n",
      "evaluation/Returns Min               5282.15\n",
      "evaluation/Estimation Bias Mean       422.246\n",
      "evaluation/Estimation Bias Std        150.263\n",
      "evaluation/EB/Q_True Mean              47.8297\n",
      "evaluation/EB/Q_True Std              147.298\n",
      "evaluation/EB/Q_Pred Mean             470.075\n",
      "evaluation/EB/Q_Pred Std               32.241\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5292.89\n",
      "evaluation/Actions Mean                 0.0567678\n",
      "evaluation/Actions Std                  0.514764\n",
      "evaluation/Actions Max                  0.997555\n",
      "evaluation/Actions Min                 -0.999172\n",
      "time/backward_policy (s)                9.10398\n",
      "time/backward_zf1 (s)                  10.7784\n",
      "time/backward_zf2 (s)                  10.3086\n",
      "time/data sampling (s)                  2.08354\n",
      "time/data storing (s)                   0.0898677\n",
      "time/evaluation sampling (s)            3.91923\n",
      "time/exploration sampling (s)           2.57138\n",
      "time/logging (s)                        0.0151862\n",
      "time/preback_alpha (s)                  0.00657069\n",
      "time/preback_policy (s)                15.3705\n",
      "time/preback_start (s)                  1.05761\n",
      "time/preback_zf (s)                    37.8762\n",
      "time/saving (s)                         4.499e-06\n",
      "time/training (s)                      11.6064\n",
      "time/epoch (s)                        104.787\n",
      "time/total (s)                       6235.06\n",
      "Epoch                                  61\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:50:27.807136 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 62 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 325000\n",
      "trainer/ZF1 Loss                       69.6736\n",
      "trainer/ZF2 Loss                       64.8667\n",
      "trainer/ZF Expert Reward               28.7721\n",
      "trainer/ZF Policy Reward                5.62213\n",
      "trainer/ZF CHI2 Term                   90.3476\n",
      "trainer/Policy Loss                  -367.143\n",
      "trainer/expert_lambda Loss             32.6729\n",
      "trainer/expert_lambda Value            25.8986\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              217.355\n",
      "trainer/Policy Param Norm              40.7072\n",
      "trainer/Zf1 Grad Norm                4191.95\n",
      "trainer/Zf1 Param Norm                164.062\n",
      "trainer/Zf2 Grad Norm                4808.85\n",
      "trainer/Zf2 Param Norm                162.288\n",
      "trainer/Z Expert Predictions Mean     587.1\n",
      "trainer/Z Expert Predictions Std       48.0374\n",
      "trainer/Z Expert Predictions Max      693.152\n",
      "trainer/Z Expert Predictions Min       20.4013\n",
      "trainer/Z Policy Predictions Mean     369.11\n",
      "trainer/Z Policy Predictions Std      134.303\n",
      "trainer/Z Policy Predictions Max      544.371\n",
      "trainer/Z Policy Predictions Min       -6.96253\n",
      "trainer/Z Expert Targets Mean         558.328\n",
      "trainer/Z Expert Targets Std           47.9059\n",
      "trainer/Z Expert Targets Max          659.481\n",
      "trainer/Z Expert Targets Min           -2.81177\n",
      "trainer/Z Policy Targets Mean         363.488\n",
      "trainer/Z Policy Targets Std          133.976\n",
      "trainer/Z Policy Targets Max          499.912\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   22.0318\n",
      "trainer/Log Pis Std                     7.34797\n",
      "trainer/Policy mu Mean                  0.131615\n",
      "trainer/Policy mu Std                   1.02839\n",
      "trainer/Policy log std Mean            -1.95834\n",
      "trainer/Policy log std Std              0.7674\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        319099\n",
      "exploration/num paths total          1617\n",
      "evaluation/num steps total         417538\n",
      "evaluation/num paths total            638\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.28732\n",
      "evaluation/Rewards Std                  0.074417\n",
      "evaluation/Rewards Max                  5.4915\n",
      "evaluation/Rewards Min                  4.78658\n",
      "evaluation/Returns Mean              5287.32\n",
      "evaluation/Returns Std                  5.02166\n",
      "evaluation/Returns Max               5297.08\n",
      "evaluation/Returns Min               5280.53\n",
      "evaluation/Estimation Bias Mean       397.878\n",
      "evaluation/Estimation Bias Std        150.738\n",
      "evaluation/EB/Q_True Mean              47.736\n",
      "evaluation/EB/Q_True Std              146.98\n",
      "evaluation/EB/Q_Pred Mean             445.614\n",
      "evaluation/EB/Q_Pred Std               31.1155\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5287.32\n",
      "evaluation/Actions Mean                 0.0804985\n",
      "evaluation/Actions Std                  0.518775\n",
      "evaluation/Actions Max                  0.999049\n",
      "evaluation/Actions Min                 -0.997914\n",
      "time/backward_policy (s)                8.88567\n",
      "time/backward_zf1 (s)                  10.5016\n",
      "time/backward_zf2 (s)                   9.99363\n",
      "time/data sampling (s)                  2.06266\n",
      "time/data storing (s)                   0.0925531\n",
      "time/evaluation sampling (s)            3.07521\n",
      "time/exploration sampling (s)           2.60254\n",
      "time/logging (s)                        0.0138314\n",
      "time/preback_alpha (s)                  0.00662068\n",
      "time/preback_policy (s)                15.3613\n",
      "time/preback_start (s)                  1.11949\n",
      "time/preback_zf (s)                    37.5416\n",
      "time/saving (s)                         3.302e-06\n",
      "time/training (s)                      11.524\n",
      "time/epoch (s)                        102.781\n",
      "time/total (s)                       6337.84\n",
      "Epoch                                  62\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:52:10.843851 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 63 finished\n",
      "---------------------------------  ----------------\n",
      "replay_buffer/size                 330000\n",
      "trainer/ZF1 Loss                       55.1206\n",
      "trainer/ZF2 Loss                       44.5326\n",
      "trainer/ZF Expert Reward               30.225\n",
      "trainer/ZF Policy Reward                6.02504\n",
      "trainer/ZF CHI2 Term                   74.5064\n",
      "trainer/Policy Loss                  -335.667\n",
      "trainer/expert_lambda Loss             35.7188\n",
      "trainer/expert_lambda Value            26.148\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              242.479\n",
      "trainer/Policy Param Norm              40.8006\n",
      "trainer/Zf1 Grad Norm                5903.44\n",
      "trainer/Zf1 Param Norm                165.387\n",
      "trainer/Zf2 Grad Norm                4054.38\n",
      "trainer/Zf2 Param Norm                163.636\n",
      "trainer/Z Expert Predictions Mean     573.079\n",
      "trainer/Z Expert Predictions Std       48.7733\n",
      "trainer/Z Expert Predictions Max      667.116\n",
      "trainer/Z Expert Predictions Min       30.1609\n",
      "trainer/Z Policy Predictions Mean     336.189\n",
      "trainer/Z Policy Predictions Std      141.055\n",
      "trainer/Z Policy Predictions Max      490.648\n",
      "trainer/Z Policy Predictions Min        0.244139\n",
      "trainer/Z Expert Targets Mean         542.854\n",
      "trainer/Z Expert Targets Std           49.5331\n",
      "trainer/Z Expert Targets Max          640.178\n",
      "trainer/Z Expert Targets Min           -5.5365\n",
      "trainer/Z Policy Targets Mean         330.163\n",
      "trainer/Z Policy Targets Std          140.352\n",
      "trainer/Z Policy Targets Max          482.269\n",
      "trainer/Z Policy Targets Min           -2.77519\n",
      "trainer/Log Pis Mean                   21.0939\n",
      "trainer/Log Pis Std                     7.95102\n",
      "trainer/Policy mu Mean                  0.230504\n",
      "trainer/Policy mu Std                   1.05949\n",
      "trainer/Policy log std Mean            -1.82359\n",
      "trainer/Policy log std Std              0.828345\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        325099\n",
      "exploration/num paths total          1623\n",
      "evaluation/num steps total         427538\n",
      "evaluation/num paths total            648\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.32766\n",
      "evaluation/Rewards Std                  0.0751407\n",
      "evaluation/Rewards Max                  5.53099\n",
      "evaluation/Rewards Min                  4.79635\n",
      "evaluation/Returns Mean              5327.66\n",
      "evaluation/Returns Std                  3.75675\n",
      "evaluation/Returns Max               5332.37\n",
      "evaluation/Returns Min               5319.64\n",
      "evaluation/Estimation Bias Mean       386.155\n",
      "evaluation/Estimation Bias Std        151.98\n",
      "evaluation/EB/Q_True Mean              48.127\n",
      "evaluation/EB/Q_True Std              148.215\n",
      "evaluation/EB/Q_Pred Mean             434.282\n",
      "evaluation/EB/Q_Pred Std               30.7153\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5327.66\n",
      "evaluation/Actions Mean                 0.0799971\n",
      "evaluation/Actions Std                  0.520039\n",
      "evaluation/Actions Max                  0.998604\n",
      "evaluation/Actions Min                 -0.996556\n",
      "time/backward_policy (s)                8.39751\n",
      "time/backward_zf1 (s)                   9.93304\n",
      "time/backward_zf2 (s)                   9.47811\n",
      "time/data sampling (s)                  1.89667\n",
      "time/data storing (s)                   0.0923963\n",
      "time/evaluation sampling (s)            4.77196\n",
      "time/exploration sampling (s)           2.61314\n",
      "time/logging (s)                        0.0132146\n",
      "time/preback_alpha (s)                  0.00634281\n",
      "time/preback_policy (s)                15.8241\n",
      "time/preback_start (s)                  1.02792\n",
      "time/preback_zf (s)                    37.1596\n",
      "time/saving (s)                         3.59101e-06\n",
      "time/training (s)                      11.602\n",
      "time/epoch (s)                        102.816\n",
      "time/total (s)                       6440.67\n",
      "Epoch                                  63\n",
      "---------------------------------  ----------------\n",
      "2024-11-02 00:53:51.419861 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 64 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 335000\n",
      "trainer/ZF1 Loss                      186.43\n",
      "trainer/ZF2 Loss                      194.202\n",
      "trainer/ZF Expert Reward               27.3343\n",
      "trainer/ZF Policy Reward                4.9773\n",
      "trainer/ZF CHI2 Term                  212.331\n",
      "trainer/Policy Loss                  -320.544\n",
      "trainer/expert_lambda Loss             29.519\n",
      "trainer/expert_lambda Value            26.4059\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              195.323\n",
      "trainer/Policy Param Norm              40.876\n",
      "trainer/Zf1 Grad Norm                4989.89\n",
      "trainer/Zf1 Param Norm                166.655\n",
      "trainer/Zf2 Grad Norm                5356.29\n",
      "trainer/Zf2 Param Norm                164.959\n",
      "trainer/Z Expert Predictions Mean     554.976\n",
      "trainer/Z Expert Predictions Std       33.3361\n",
      "trainer/Z Expert Predictions Max      619.203\n",
      "trainer/Z Expert Predictions Min      454.464\n",
      "trainer/Z Policy Predictions Mean     321.485\n",
      "trainer/Z Policy Predictions Std      132.195\n",
      "trainer/Z Policy Predictions Max      499.374\n",
      "trainer/Z Policy Predictions Min        9.00507\n",
      "trainer/Z Expert Targets Mean         527.642\n",
      "trainer/Z Expert Targets Std           33.2935\n",
      "trainer/Z Expert Targets Max          590.666\n",
      "trainer/Z Expert Targets Min          430.568\n",
      "trainer/Z Policy Targets Mean         316.508\n",
      "trainer/Z Policy Targets Std          133.029\n",
      "trainer/Z Policy Targets Max          480.391\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   21.0799\n",
      "trainer/Log Pis Std                     7.62166\n",
      "trainer/Policy mu Mean                  0.221679\n",
      "trainer/Policy mu Std                   1.05954\n",
      "trainer/Policy log std Mean            -1.81514\n",
      "trainer/Policy log std Std              0.814681\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        329099\n",
      "exploration/num paths total          1627\n",
      "evaluation/num steps total         437538\n",
      "evaluation/num paths total            658\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.3288\n",
      "evaluation/Rewards Std                  0.0779308\n",
      "evaluation/Rewards Max                  5.50807\n",
      "evaluation/Rewards Min                  4.81319\n",
      "evaluation/Returns Mean              5328.8\n",
      "evaluation/Returns Std                  6.36458\n",
      "evaluation/Returns Max               5341.08\n",
      "evaluation/Returns Min               5316.13\n",
      "evaluation/Estimation Bias Mean       366.505\n",
      "evaluation/Estimation Bias Std        151.079\n",
      "evaluation/EB/Q_True Mean              48.1372\n",
      "evaluation/EB/Q_True Std              148.228\n",
      "evaluation/EB/Q_Pred Mean             414.642\n",
      "evaluation/EB/Q_Pred Std               29.242\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5328.8\n",
      "evaluation/Actions Mean                 0.0736901\n",
      "evaluation/Actions Std                  0.515077\n",
      "evaluation/Actions Max                  0.998624\n",
      "evaluation/Actions Min                 -0.99734\n",
      "time/backward_policy (s)                8.4754\n",
      "time/backward_zf1 (s)                   9.83912\n",
      "time/backward_zf2 (s)                   9.56128\n",
      "time/data sampling (s)                  1.77445\n",
      "time/data storing (s)                   0.0864051\n",
      "time/evaluation sampling (s)            4.55483\n",
      "time/exploration sampling (s)           2.4859\n",
      "time/logging (s)                        0.0125391\n",
      "time/preback_alpha (s)                  0.00611873\n",
      "time/preback_policy (s)                15.1567\n",
      "time/preback_start (s)                  0.967714\n",
      "time/preback_zf (s)                    36.4508\n",
      "time/saving (s)                         3.004e-06\n",
      "time/training (s)                      10.993\n",
      "time/epoch (s)                        100.364\n",
      "time/total (s)                       6541.04\n",
      "Epoch                                  64\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:55:32.576349 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 65 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 340000\n",
      "trainer/ZF1 Loss                       43.2197\n",
      "trainer/ZF2 Loss                       33.8964\n",
      "trainer/ZF Expert Reward               27.0953\n",
      "trainer/ZF Policy Reward                3.99408\n",
      "trainer/ZF CHI2 Term                   60.6199\n",
      "trainer/Policy Loss                  -306.312\n",
      "trainer/expert_lambda Loss             20.5896\n",
      "trainer/expert_lambda Value            26.6784\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              244.751\n",
      "trainer/Policy Param Norm              40.9506\n",
      "trainer/Zf1 Grad Norm                3856.97\n",
      "trainer/Zf1 Param Norm                167.918\n",
      "trainer/Zf2 Grad Norm                2789.4\n",
      "trainer/Zf2 Param Norm                166.237\n",
      "trainer/Z Expert Predictions Mean     522.666\n",
      "trainer/Z Expert Predictions Std       53.8847\n",
      "trainer/Z Expert Predictions Max      592.119\n",
      "trainer/Z Expert Predictions Min       29.7278\n",
      "trainer/Z Policy Predictions Mean     307.747\n",
      "trainer/Z Policy Predictions Std      123.448\n",
      "trainer/Z Policy Predictions Max      456.325\n",
      "trainer/Z Policy Predictions Min        6.9033\n",
      "trainer/Z Expert Targets Mean         495.571\n",
      "trainer/Z Expert Targets Std           54.4532\n",
      "trainer/Z Expert Targets Max          574.557\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean         303.753\n",
      "trainer/Z Policy Targets Std          123.543\n",
      "trainer/Z Policy Targets Max          439.948\n",
      "trainer/Z Policy Targets Min            2.52739\n",
      "trainer/Log Pis Mean                   20.9277\n",
      "trainer/Log Pis Std                     7.66865\n",
      "trainer/Policy mu Mean                  0.181867\n",
      "trainer/Policy mu Std                   1.02414\n",
      "trainer/Policy log std Mean            -1.88779\n",
      "trainer/Policy log std Std              0.832126\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        335099\n",
      "exploration/num paths total          1633\n",
      "evaluation/num steps total         447538\n",
      "evaluation/num paths total            668\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 5.34206\n",
      "evaluation/Rewards Std                  0.0830336\n",
      "evaluation/Rewards Max                  5.62215\n",
      "evaluation/Rewards Min                  4.80691\n",
      "evaluation/Returns Mean              5342.06\n",
      "evaluation/Returns Std                  6.98506\n",
      "evaluation/Returns Max               5353.97\n",
      "evaluation/Returns Min               5331.55\n",
      "evaluation/Estimation Bias Mean       331.096\n",
      "evaluation/Estimation Bias Std        150.487\n",
      "evaluation/EB/Q_True Mean              48.2212\n",
      "evaluation/EB/Q_True Std              148.52\n",
      "evaluation/EB/Q_Pred Mean             379.317\n",
      "evaluation/EB/Q_Pred Std               30.4558\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5342.06\n",
      "evaluation/Actions Mean                 0.0591843\n",
      "evaluation/Actions Std                  0.511943\n",
      "evaluation/Actions Max                  0.998915\n",
      "evaluation/Actions Min                 -0.997683\n",
      "time/backward_policy (s)                8.69056\n",
      "time/backward_zf1 (s)                  10.0028\n",
      "time/backward_zf2 (s)                   9.74135\n",
      "time/data sampling (s)                  1.79667\n",
      "time/data storing (s)                   0.0862873\n",
      "time/evaluation sampling (s)            4.65278\n",
      "time/exploration sampling (s)           2.48662\n",
      "time/logging (s)                        0.0123985\n",
      "time/preback_alpha (s)                  0.00621011\n",
      "time/preback_policy (s)                14.9935\n",
      "time/preback_start (s)                  0.985194\n",
      "time/preback_zf (s)                    36.5083\n",
      "time/saving (s)                         2.972e-06\n",
      "time/training (s)                      10.9841\n",
      "time/epoch (s)                        100.947\n",
      "time/total (s)                       6641.99\n",
      "Epoch                                  65\n",
      "---------------------------------  ---------------\n",
      "2024-11-02 00:57:15.849377 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 66 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 345000\n",
      "trainer/ZF1 Loss                      142.585\n",
      "trainer/ZF2 Loss                      136.659\n",
      "trainer/ZF Expert Reward               28.8825\n",
      "trainer/ZF Policy Reward                6.23949\n",
      "trainer/ZF CHI2 Term                  163.683\n",
      "trainer/Policy Loss                  -293.368\n",
      "trainer/expert_lambda Loss             28.5185\n",
      "trainer/expert_lambda Value            26.9621\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              197.215\n",
      "trainer/Policy Param Norm              41.0293\n",
      "trainer/Zf1 Grad Norm                4246.75\n",
      "trainer/Zf1 Param Norm                169.174\n",
      "trainer/Zf2 Grad Norm                4610.43\n",
      "trainer/Zf2 Param Norm                167.543\n",
      "trainer/Z Expert Predictions Mean     504.747\n",
      "trainer/Z Expert Predictions Std       50.9987\n",
      "trainer/Z Expert Predictions Max      624.772\n",
      "trainer/Z Expert Predictions Min       31.6066\n",
      "trainer/Z Policy Predictions Mean     295.583\n",
      "trainer/Z Policy Predictions Std      118.475\n",
      "trainer/Z Policy Predictions Max      436.851\n",
      "trainer/Z Policy Predictions Min        9.15636\n",
      "trainer/Z Expert Targets Mean         475.864\n",
      "trainer/Z Expert Targets Std           51.3328\n",
      "trainer/Z Expert Targets Max          593.849\n",
      "trainer/Z Expert Targets Min            3.73811\n",
      "trainer/Z Policy Targets Mean         289.344\n",
      "trainer/Z Policy Targets Std          120.111\n",
      "trainer/Z Policy Targets Max          424.6\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   20.8335\n",
      "trainer/Log Pis Std                     8.57026\n",
      "trainer/Policy mu Mean                  0.218288\n",
      "trainer/Policy mu Std                   0.994005\n",
      "trainer/Policy log std Mean            -1.8856\n",
      "trainer/Policy log std Std              0.830463\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        339099\n",
      "exploration/num paths total          1637\n",
      "evaluation/num steps total         456715\n",
      "evaluation/num paths total            678\n",
      "evaluation/path length Mean           917.7\n",
      "evaluation/path length Std            246.9\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            177\n",
      "evaluation/Rewards Mean                 5.31234\n",
      "evaluation/Rewards Std                  0.098075\n",
      "evaluation/Rewards Max                  5.951\n",
      "evaluation/Rewards Min                  4.82398\n",
      "evaluation/Returns Mean              4875.14\n",
      "evaluation/Returns Std               1317.22\n",
      "evaluation/Returns Max               5326.47\n",
      "evaluation/Returns Min                923.553\n",
      "evaluation/Estimation Bias Mean       309.444\n",
      "evaluation/Estimation Bias Std        167.123\n",
      "evaluation/EB/Q_True Mean              52.3814\n",
      "evaluation/EB/Q_True Std              153.827\n",
      "evaluation/EB/Q_Pred Mean             361.825\n",
      "evaluation/EB/Q_Pred Std               36.8056\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4875.14\n",
      "evaluation/Actions Mean                 0.0738661\n",
      "evaluation/Actions Std                  0.505088\n",
      "evaluation/Actions Max                  0.999248\n",
      "evaluation/Actions Min                 -0.99809\n",
      "time/backward_policy (s)                8.85436\n",
      "time/backward_zf1 (s)                  10.522\n",
      "time/backward_zf2 (s)                  10.098\n",
      "time/data sampling (s)                  1.90193\n",
      "time/data storing (s)                   0.0874713\n",
      "time/evaluation sampling (s)            4.65324\n",
      "time/exploration sampling (s)           2.5263\n",
      "time/logging (s)                        0.0126116\n",
      "time/preback_alpha (s)                  0.0063475\n",
      "time/preback_policy (s)                14.9661\n",
      "time/preback_start (s)                  1.02465\n",
      "time/preback_zf (s)                    37.1927\n",
      "time/saving (s)                         2.9e-06\n",
      "time/training (s)                      11.2042\n",
      "time/epoch (s)                        103.05\n",
      "time/total (s)                       6745.05\n",
      "Epoch                                  66\n",
      "---------------------------------  --------------\n",
      "2024-11-02 00:59:03.799524 +0330 | [humanoid_2024_11_01_23_04_38_0000--s-0] Epoch 67 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 350000\n",
      "trainer/ZF1 Loss                       31.5285\n",
      "trainer/ZF2 Loss                       31.681\n",
      "trainer/ZF Expert Reward               27.7728\n",
      "trainer/ZF Policy Reward                4.66249\n",
      "trainer/ZF CHI2 Term                   54.6998\n",
      "trainer/Policy Loss                  -273.874\n",
      "trainer/expert_lambda Loss             27.08\n",
      "trainer/expert_lambda Value            27.2514\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              192.644\n",
      "trainer/Policy Param Norm              41.1173\n",
      "trainer/Zf1 Grad Norm                2916.92\n",
      "trainer/Zf1 Param Norm                170.374\n",
      "trainer/Zf2 Grad Norm                4812.41\n",
      "trainer/Zf2 Param Norm                168.822\n",
      "trainer/Z Expert Predictions Mean     490.651\n",
      "trainer/Z Expert Predictions Std       29.7384\n",
      "trainer/Z Expert Predictions Max      558.332\n",
      "trainer/Z Expert Predictions Min      355.935\n",
      "trainer/Z Policy Predictions Mean     275.198\n",
      "trainer/Z Policy Predictions Std      113.581\n",
      "trainer/Z Policy Predictions Max      393.814\n",
      "trainer/Z Policy Predictions Min       12.9372\n",
      "trainer/Z Expert Targets Mean         462.879\n",
      "trainer/Z Expert Targets Std           30.1583\n",
      "trainer/Z Expert Targets Max          538.681\n",
      "trainer/Z Expert Targets Min          330.262\n",
      "trainer/Z Policy Targets Mean         270.536\n",
      "trainer/Z Policy Targets Std          113.98\n",
      "trainer/Z Policy Targets Max          385.176\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   19.7777\n",
      "trainer/Log Pis Std                     7.92088\n",
      "trainer/Policy mu Mean                  0.216805\n",
      "trainer/Policy mu Std                   0.993647\n",
      "trainer/Policy log std Mean            -1.83994\n",
      "trainer/Policy log std Std              0.823532\n",
      "trainer/Alpha                           0.05\n",
      "exploration/num steps total        345099\n",
      "exploration/num paths total          1643\n",
      "evaluation/num steps total         466157\n",
      "evaluation/num paths total            688\n",
      "evaluation/path length Mean           944.2\n",
      "evaluation/path length Std            167.4\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            442\n",
      "evaluation/Rewards Mean                 5.31494\n",
      "evaluation/Rewards Std                  0.0877624\n",
      "evaluation/Rewards Max                  6.37806\n",
      "evaluation/Rewards Min                  4.82501\n",
      "evaluation/Returns Mean              5018.37\n",
      "evaluation/Returns Std                893.264\n",
      "evaluation/Returns Max               5329.4\n",
      "evaluation/Returns Min               2338.66\n",
      "evaluation/Estimation Bias Mean       278.283\n",
      "evaluation/Estimation Bias Std        160.679\n",
      "evaluation/EB/Q_True Mean              50.7801\n",
      "evaluation/EB/Q_True Std              151.503\n",
      "evaluation/EB/Q_Pred Mean             329.063\n",
      "evaluation/EB/Q_Pred Std               32.4363\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           5018.37\n",
      "evaluation/Actions Mean                 0.0703407\n",
      "evaluation/Actions Std                  0.508707\n",
      "evaluation/Actions Max                  0.998515\n",
      "evaluation/Actions Min                 -0.998086\n",
      "time/backward_policy (s)                9.05806\n",
      "time/backward_zf1 (s)                  11.1959\n",
      "time/backward_zf2 (s)                  10.656\n",
      "time/data sampling (s)                  2.08096\n",
      "time/data storing (s)                   0.0969541\n",
      "time/evaluation sampling (s)            4.71025\n",
      "time/exploration sampling (s)           2.64196\n",
      "time/logging (s)                        0.0127303\n",
      "time/preback_alpha (s)                  0.00657611\n",
      "time/preback_policy (s)                15.918\n",
      "time/preback_start (s)                  1.05665\n",
      "time/preback_zf (s)                    38.4749\n",
      "time/saving (s)                         3.793e-06\n",
      "time/training (s)                      11.8082\n",
      "time/epoch (s)                        107.717\n",
      "time/total (s)                       6852.77\n",
      "Epoch                                  67\n",
      "---------------------------------  ---------------\n"
     ]
    }
   ],
   "source": [
    "!python train.py --env humanoid --demos 1 --loss 'v0' --alpha 0.05 --seed 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0913d-3149-472a-8496-869775ffddd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
