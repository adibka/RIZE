{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ecfecf-0fda-4847-a8c9-3a04acdb5d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No personal conf_private.py found.\n",
      "doodad not detected\n",
      "2024-11-07 17:19:50.536758 +0330 | Variant:\n",
      "2024-11-07 17:19:50.537028 +0330 | {\n",
      "  \"algorithm_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"min_num_steps_before_training\": 10000,\n",
      "    \"num_epochs\": 60,\n",
      "    \"num_eval_paths_per_epoch\": 10,\n",
      "    \"num_expl_steps_per_train_loop\": 5000,\n",
      "    \"num_trains_per_train_loop\": 5000\n",
      "  },\n",
      "  \"iq_kwargs\": {\n",
      "    \"demos\": 1,\n",
      "    \"regularize\": \"TD_both\",\n",
      "    \"loss\": \"value\",\n",
      "    \"chi\": 0.5,\n",
      "    \"expert_path\": \"experts/Ant-v2_25_from_cs285.pkl\",\n",
      "    \"subsample_freq\": 1,\n",
      "    \"reward_type\": \"sparse\",\n",
      "    \"noise_std\": 1.0,\n",
      "    \"sparse_prob\": 0.5,\n",
      "    \"sparse_type\": \"empty\"\n",
      "  },\n",
      "  \"env\": \"Ant-v2\",\n",
      "  \"seed\": 0,\n",
      "  \"expectation_z\": true,\n",
      "  \"use_policy_expert_obs\": false,\n",
      "  \"eval_env_num\": 10,\n",
      "  \"expl_env_num\": 10,\n",
      "  \"layer_size\": 256,\n",
      "  \"num_quantiles\": 24,\n",
      "  \"replay_buffer_size\": 1000000,\n",
      "  \"trainer_kwargs\": {\n",
      "    \"alpha\": 0.01,\n",
      "    \"discount\": 0.99,\n",
      "    \"policy_lr\": 5e-05,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"tau_type\": \"iqn\",\n",
      "    \"use_automatic_entropy_tuning\": false,\n",
      "    \"zf_lr\": 0.0003,\n",
      "    \"expert_lambda\": 10,\n",
      "    \"expert_lambda_lr\": 0.0001,\n",
      "    \"tune_expert_lambda\": true,\n",
      "    \"policy_lambda\": 5,\n",
      "    \"policy_lambda_lr\": 0.0001,\n",
      "    \"tune_policy_lambda\": false\n",
      "  },\n",
      "  \"version\": \"normal-iqn-neutral\"\n",
      "}\n",
      "/home/eddie/venvs/LSIQ/lib/python3.10/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2024-11-07 17:21:19.682255 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 0 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 15000\n",
      "trainer/ZF1 Loss                      23.1184\n",
      "trainer/ZF2 Loss                      23.8907\n",
      "trainer/ZF Expert Reward               0.417577\n",
      "trainer/ZF Policy Reward               0.598209\n",
      "trainer/ZF CHI2 Term                  23.3606\n",
      "trainer/Policy Loss                   -0.291433\n",
      "trainer/expert_lambda Loss            45.9437\n",
      "trainer/expert_lambda Value            9.9999\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm               0.0613076\n",
      "trainer/Policy Param Norm             14.6616\n",
      "trainer/Zf1 Grad Norm                 52.1664\n",
      "trainer/Zf1 Param Norm                32.0295\n",
      "trainer/Zf2 Grad Norm                 59.3895\n",
      "trainer/Zf2 Param Norm                32.0624\n",
      "trainer/Z Expert Predictions Mean      0.0559483\n",
      "trainer/Z Expert Predictions Std       0.127652\n",
      "trainer/Z Expert Predictions Max       0.390799\n",
      "trainer/Z Expert Predictions Min      -0.28561\n",
      "trainer/Z Policy Predictions Mean      0.183209\n",
      "trainer/Z Policy Predictions Std       0.224903\n",
      "trainer/Z Policy Predictions Max       0.799769\n",
      "trainer/Z Policy Predictions Min      -0.365343\n",
      "trainer/Z Expert Targets Mean         -0.361629\n",
      "trainer/Z Expert Targets Std           0.22508\n",
      "trainer/Z Expert Targets Max           0.184326\n",
      "trainer/Z Expert Targets Min          -0.788853\n",
      "trainer/Z Policy Targets Mean         -0.415\n",
      "trainer/Z Policy Targets Std           0.223779\n",
      "trainer/Z Policy Targets Max           0.206453\n",
      "trainer/Z Policy Targets Min          -0.960183\n",
      "trainer/Log Pis Mean                  -5.33723\n",
      "trainer/Log Pis Std                    0.641007\n",
      "trainer/Policy mu Mean                -0.000856714\n",
      "trainer/Policy mu Std                  0.00117669\n",
      "trainer/Policy log std Mean           -0.000634219\n",
      "trainer/Policy log std Std             0.00145793\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        12148\n",
      "exploration/num paths total          104\n",
      "evaluation/num steps total         10000\n",
      "evaluation/num paths total            10\n",
      "evaluation/path length Mean         1000\n",
      "evaluation/path length Std             0\n",
      "evaluation/path length Max          1000\n",
      "evaluation/path length Min          1000\n",
      "evaluation/Rewards Mean                0.309956\n",
      "evaluation/Rewards Std                 0.560095\n",
      "evaluation/Rewards Max                 4.54771\n",
      "evaluation/Rewards Min                -1.89991\n",
      "evaluation/Returns Mean              309.956\n",
      "evaluation/Returns Std               408.552\n",
      "evaluation/Returns Max               662.538\n",
      "evaluation/Returns Min              -880.131\n",
      "evaluation/Estimation Bias Mean       36.1681\n",
      "evaluation/Estimation Bias Std        27.9146\n",
      "evaluation/EB/Q_True Mean              3.81468\n",
      "evaluation/EB/Q_True Std              11.8571\n",
      "evaluation/EB/Q_Pred Mean             39.9828\n",
      "evaluation/EB/Q_Pred Std              28.4474\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           309.956\n",
      "evaluation/Actions Mean                0.00697644\n",
      "evaluation/Actions Std                 0.428871\n",
      "evaluation/Actions Max                 0.994478\n",
      "evaluation/Actions Min                -0.999324\n",
      "time/backward_policy (s)               8.88102\n",
      "time/backward_zf1 (s)                 10.7318\n",
      "time/backward_zf2 (s)                 10.3482\n",
      "time/data sampling (s)                 1.21148\n",
      "time/data storing (s)                  0.0817892\n",
      "time/evaluation sampling (s)           1.89762\n",
      "time/exploration sampling (s)          1.8224\n",
      "time/logging (s)                       0.0112011\n",
      "time/preback_alpha (s)                 0.00522155\n",
      "time/preback_policy (s)                9.56299\n",
      "time/preback_start (s)                 0.681928\n",
      "time/preback_zf (s)                   29.0603\n",
      "time/saving (s)                        3.448e-06\n",
      "time/training (s)                     10.606\n",
      "time/epoch (s)                        84.902\n",
      "time/total (s)                        90.5461\n",
      "Epoch                                  0\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:22:44.630924 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 1 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 20000\n",
      "trainer/ZF1 Loss                       1.9619\n",
      "trainer/ZF2 Loss                       2.38047\n",
      "trainer/ZF Expert Reward              11.0703\n",
      "trainer/ZF Policy Reward               0.000335854\n",
      "trainer/ZF CHI2 Term                   7.9683\n",
      "trainer/Policy Loss                   -8.95872\n",
      "trainer/expert_lambda Loss             2.39762\n",
      "trainer/expert_lambda Value           10.3324\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              12.4283\n",
      "trainer/Policy Param Norm             17.6965\n",
      "trainer/Zf1 Grad Norm                257.502\n",
      "trainer/Zf1 Param Norm                38.278\n",
      "trainer/Zf2 Grad Norm                205.605\n",
      "trainer/Zf2 Param Norm                38.2686\n",
      "trainer/Z Expert Predictions Mean    197.591\n",
      "trainer/Z Expert Predictions Std      12.6565\n",
      "trainer/Z Expert Predictions Max     207.528\n",
      "trainer/Z Expert Predictions Min      83.4204\n",
      "trainer/Z Policy Predictions Mean      5.14565\n",
      "trainer/Z Policy Predictions Std      23.8833\n",
      "trainer/Z Policy Predictions Max     114.224\n",
      "trainer/Z Policy Predictions Min     -23.4806\n",
      "trainer/Z Expert Targets Mean        186.521\n",
      "trainer/Z Expert Targets Std          13.1389\n",
      "trainer/Z Expert Targets Max         197.049\n",
      "trainer/Z Expert Targets Min          71.085\n",
      "trainer/Z Policy Targets Mean          5.14532\n",
      "trainer/Z Policy Targets Std          23.8123\n",
      "trainer/Z Policy Targets Max         130.564\n",
      "trainer/Z Policy Targets Min         -23.3258\n",
      "trainer/Log Pis Mean                  15.4568\n",
      "trainer/Log Pis Std                    6.81118\n",
      "trainer/Policy mu Mean                -0.271677\n",
      "trainer/Policy mu Std                  1.1335\n",
      "trainer/Policy log std Mean           -2.26491\n",
      "trainer/Policy log std Std             1.452\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        13905\n",
      "exploration/num paths total          106\n",
      "evaluation/num steps total         20000\n",
      "evaluation/num paths total            20\n",
      "evaluation/path length Mean         1000\n",
      "evaluation/path length Std             0\n",
      "evaluation/path length Max          1000\n",
      "evaluation/path length Min          1000\n",
      "evaluation/Rewards Mean               -0.278743\n",
      "evaluation/Rewards Std                 0.0901521\n",
      "evaluation/Rewards Max                 1.39475\n",
      "evaluation/Rewards Min                -2.57394\n",
      "evaluation/Returns Mean             -278.743\n",
      "evaluation/Returns Std                40.4131\n",
      "evaluation/Returns Max              -227.973\n",
      "evaluation/Returns Min              -349.879\n",
      "evaluation/Estimation Bias Mean      174.484\n",
      "evaluation/Estimation Bias Std        11.6686\n",
      "evaluation/EB/Q_True Mean             -2.06431\n",
      "evaluation/EB/Q_True Std               6.35784\n",
      "evaluation/EB/Q_Pred Mean            172.42\n",
      "evaluation/EB/Q_Pred Std               8.47801\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns          -278.743\n",
      "evaluation/Actions Mean               -0.080412\n",
      "evaluation/Actions Std                 0.558984\n",
      "evaluation/Actions Max                 0.910446\n",
      "evaluation/Actions Min                -0.999536\n",
      "time/backward_policy (s)               8.13386\n",
      "time/backward_zf1 (s)                 10.0994\n",
      "time/backward_zf2 (s)                  9.81419\n",
      "time/data sampling (s)                 1.18989\n",
      "time/data storing (s)                  0.0786671\n",
      "time/evaluation sampling (s)           4.07909\n",
      "time/exploration sampling (s)          1.70562\n",
      "time/logging (s)                       0.0184511\n",
      "time/preback_alpha (s)                 0.00509462\n",
      "time/preback_policy (s)                9.66653\n",
      "time/preback_start (s)                 0.656201\n",
      "time/preback_zf (s)                   28.681\n",
      "time/saving (s)                        3.223e-06\n",
      "time/training (s)                     10.6219\n",
      "time/epoch (s)                        84.7499\n",
      "time/total (s)                       175.298\n",
      "Epoch                                  1\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:24:10.810092 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 2 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 25000\n",
      "trainer/ZF1 Loss                      18.1419\n",
      "trainer/ZF2 Loss                      16.2166\n",
      "trainer/ZF Expert Reward              13.3898\n",
      "trainer/ZF Policy Reward              -0.969989\n",
      "trainer/ZF CHI2 Term                  24.5281\n",
      "trainer/Policy Loss                  -29.3196\n",
      "trainer/expert_lambda Loss            10.4627\n",
      "trainer/expert_lambda Value           10.7603\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              12.2579\n",
      "trainer/Policy Param Norm             19.1513\n",
      "trainer/Zf1 Grad Norm                838.393\n",
      "trainer/Zf1 Param Norm                41.5954\n",
      "trainer/Zf2 Grad Norm                949.319\n",
      "trainer/Zf2 Param Norm                41.5565\n",
      "trainer/Z Expert Predictions Mean    224.193\n",
      "trainer/Z Expert Predictions Std      20.2022\n",
      "trainer/Z Expert Predictions Max     251.247\n",
      "trainer/Z Expert Predictions Min      13.8743\n",
      "trainer/Z Policy Predictions Mean     25.2708\n",
      "trainer/Z Policy Predictions Std      77.0045\n",
      "trainer/Z Policy Predictions Max     198.767\n",
      "trainer/Z Policy Predictions Min     -42.7028\n",
      "trainer/Z Expert Targets Mean        210.803\n",
      "trainer/Z Expert Targets Std          20.6777\n",
      "trainer/Z Expert Targets Max         240.94\n",
      "trainer/Z Expert Targets Min           0\n",
      "trainer/Z Policy Targets Mean         26.2408\n",
      "trainer/Z Policy Targets Std          76.3399\n",
      "trainer/Z Policy Targets Max         198.179\n",
      "trainer/Z Policy Targets Min         -40.9862\n",
      "trainer/Log Pis Mean                  14.767\n",
      "trainer/Log Pis Std                    8.27486\n",
      "trainer/Policy mu Mean                -0.60111\n",
      "trainer/Policy mu Std                  1.40251\n",
      "trainer/Policy log std Mean           -1.66241\n",
      "trainer/Policy log std Std             1.56431\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        21606\n",
      "exploration/num paths total          114\n",
      "evaluation/num steps total         30000\n",
      "evaluation/num paths total            30\n",
      "evaluation/path length Mean         1000\n",
      "evaluation/path length Std             0\n",
      "evaluation/path length Max          1000\n",
      "evaluation/path length Min          1000\n",
      "evaluation/Rewards Mean                0.195761\n",
      "evaluation/Rewards Std                 0.490211\n",
      "evaluation/Rewards Max                 3.11725\n",
      "evaluation/Rewards Min                -1.73986\n",
      "evaluation/Returns Mean              195.761\n",
      "evaluation/Returns Std                92.8497\n",
      "evaluation/Returns Max               342.949\n",
      "evaluation/Returns Min                55.167\n",
      "evaluation/Estimation Bias Mean      172.005\n",
      "evaluation/Estimation Bias Std        10.288\n",
      "evaluation/EB/Q_True Mean              2.73331\n",
      "evaluation/EB/Q_True Std               8.90597\n",
      "evaluation/EB/Q_Pred Mean            174.738\n",
      "evaluation/EB/Q_Pred Std               5.84783\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           195.761\n",
      "evaluation/Actions Mean                0.00745454\n",
      "evaluation/Actions Std                 0.464623\n",
      "evaluation/Actions Max                 0.993843\n",
      "evaluation/Actions Min                -0.996991\n",
      "time/backward_policy (s)               8.59848\n",
      "time/backward_zf1 (s)                 10.5315\n",
      "time/backward_zf2 (s)                 10.2174\n",
      "time/data sampling (s)                 1.28104\n",
      "time/data storing (s)                  0.0802962\n",
      "time/evaluation sampling (s)           3.82362\n",
      "time/exploration sampling (s)          1.772\n",
      "time/logging (s)                       0.0114796\n",
      "time/preback_alpha (s)                 0.00517423\n",
      "time/preback_policy (s)                9.4895\n",
      "time/preback_start (s)                 0.672594\n",
      "time/preback_zf (s)                   28.909\n",
      "time/saving (s)                        2.911e-06\n",
      "time/training (s)                     10.5702\n",
      "time/epoch (s)                        85.9623\n",
      "time/total (s)                       261.262\n",
      "Epoch                                  2\n",
      "---------------------------------  --------------\n",
      "2024-11-07 17:25:33.253739 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 3 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 30000\n",
      "trainer/ZF1 Loss                       8.12811\n",
      "trainer/ZF2 Loss                       7.79352\n",
      "trainer/ZF Expert Reward              11.931\n",
      "trainer/ZF Policy Reward              -1.61045\n",
      "trainer/ZF CHI2 Term                  14.9149\n",
      "trainer/Policy Loss                  -69.359\n",
      "trainer/expert_lambda Loss             2.67662\n",
      "trainer/expert_lambda Value           11.1367\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              14.0704\n",
      "trainer/Policy Param Norm             20.7512\n",
      "trainer/Zf1 Grad Norm                378.134\n",
      "trainer/Zf1 Param Norm                44.0843\n",
      "trainer/Zf2 Grad Norm                619.629\n",
      "trainer/Zf2 Param Norm                43.8494\n",
      "trainer/Z Expert Predictions Mean    224.375\n",
      "trainer/Z Expert Predictions Std      12.7468\n",
      "trainer/Z Expert Predictions Max     249.687\n",
      "trainer/Z Expert Predictions Min     171.917\n",
      "trainer/Z Policy Predictions Mean     66.0089\n",
      "trainer/Z Policy Predictions Std      90.4251\n",
      "trainer/Z Policy Predictions Max     194.046\n",
      "trainer/Z Policy Predictions Min     -46.6871\n",
      "trainer/Z Expert Targets Mean        212.444\n",
      "trainer/Z Expert Targets Std          12.8437\n",
      "trainer/Z Expert Targets Max         239.688\n",
      "trainer/Z Expert Targets Min         159.016\n",
      "trainer/Z Policy Targets Mean         67.6193\n",
      "trainer/Z Policy Targets Std          89.8734\n",
      "trainer/Z Policy Targets Max         183.605\n",
      "trainer/Z Policy Targets Min         -47.4824\n",
      "trainer/Log Pis Mean                  22.4364\n",
      "trainer/Log Pis Std                   10.3365\n",
      "trainer/Policy mu Mean                -0.480264\n",
      "trainer/Policy mu Std                  2.13133\n",
      "trainer/Policy log std Mean           -1.99634\n",
      "trainer/Policy log std Std             1.07303\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        23451\n",
      "exploration/num paths total          116\n",
      "evaluation/num steps total         31264\n",
      "evaluation/num paths total            40\n",
      "evaluation/path length Mean          126.4\n",
      "evaluation/path length Std           118.17\n",
      "evaluation/path length Max           399\n",
      "evaluation/path length Min            11\n",
      "evaluation/Rewards Mean                1.88216\n",
      "evaluation/Rewards Std                 1.4747\n",
      "evaluation/Rewards Max                 6.1828\n",
      "evaluation/Rewards Min                -1.79154\n",
      "evaluation/Returns Mean              237.905\n",
      "evaluation/Returns Std               236.051\n",
      "evaluation/Returns Max               701.853\n",
      "evaluation/Returns Min                 7.89485\n",
      "evaluation/Estimation Bias Mean      127.43\n",
      "evaluation/Estimation Bias Std        67.7304\n",
      "evaluation/EB/Q_True Mean             36.918\n",
      "evaluation/EB/Q_True Std              61.3815\n",
      "evaluation/EB/Q_Pred Mean            164.348\n",
      "evaluation/EB/Q_Pred Std              33.2753\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns           237.905\n",
      "evaluation/Actions Mean                0.0570265\n",
      "evaluation/Actions Std                 0.460559\n",
      "evaluation/Actions Max                 0.998589\n",
      "evaluation/Actions Min                -0.996722\n",
      "time/backward_policy (s)               8.22612\n",
      "time/backward_zf1 (s)                 10.1428\n",
      "time/backward_zf2 (s)                  9.85045\n",
      "time/data sampling (s)                 1.23279\n",
      "time/data storing (s)                  0.0787098\n",
      "time/evaluation sampling (s)           1.60737\n",
      "time/exploration sampling (s)          1.65\n",
      "time/logging (s)                       0.00463163\n",
      "time/preback_alpha (s)                 0.00518427\n",
      "time/preback_policy (s)                9.5444\n",
      "time/preback_start (s)                 0.659096\n",
      "time/preback_zf (s)                   28.7385\n",
      "time/saving (s)                        5.461e-06\n",
      "time/training (s)                     10.489\n",
      "time/epoch (s)                        82.229\n",
      "time/total (s)                       343.494\n",
      "Epoch                                  3\n",
      "---------------------------------  --------------\n",
      "2024-11-07 17:26:58.867572 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 4 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 35000\n",
      "trainer/ZF1 Loss                       5.25982\n",
      "trainer/ZF2 Loss                       3.74743\n",
      "trainer/ZF Expert Reward              13.6967\n",
      "trainer/ZF Policy Reward              -0.868644\n",
      "trainer/ZF CHI2 Term                  11.9504\n",
      "trainer/Policy Loss                  -82.3329\n",
      "trainer/expert_lambda Loss             6.28564\n",
      "trainer/expert_lambda Value           11.527\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              10.0323\n",
      "trainer/Policy Param Norm             21.8121\n",
      "trainer/Zf1 Grad Norm                503.432\n",
      "trainer/Zf1 Param Norm                46.3475\n",
      "trainer/Zf2 Grad Norm                630.924\n",
      "trainer/Zf2 Param Norm                45.8366\n",
      "trainer/Z Expert Predictions Mean    254.816\n",
      "trainer/Z Expert Predictions Std      22.7389\n",
      "trainer/Z Expert Predictions Max     281.61\n",
      "trainer/Z Expert Predictions Min      23.2296\n",
      "trainer/Z Policy Predictions Mean     79.8012\n",
      "trainer/Z Policy Predictions Std      62.9018\n",
      "trainer/Z Policy Predictions Max     230.281\n",
      "trainer/Z Policy Predictions Min     -29.3942\n",
      "trainer/Z Expert Targets Mean        241.119\n",
      "trainer/Z Expert Targets Std          23.2472\n",
      "trainer/Z Expert Targets Max         270.102\n",
      "trainer/Z Expert Targets Min           0\n",
      "trainer/Z Policy Targets Mean         80.6698\n",
      "trainer/Z Policy Targets Std          62.4879\n",
      "trainer/Z Policy Targets Max         215.511\n",
      "trainer/Z Policy Targets Min         -28.5656\n",
      "trainer/Log Pis Mean                  18.3028\n",
      "trainer/Log Pis Std                    8.12047\n",
      "trainer/Policy mu Mean                -0.102041\n",
      "trainer/Policy mu Std                  1.66449\n",
      "trainer/Policy log std Mean           -2.28062\n",
      "trainer/Policy log std Std             0.936836\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        31071\n",
      "exploration/num paths total          126\n",
      "evaluation/num steps total         40321\n",
      "evaluation/num paths total            50\n",
      "evaluation/path length Mean          905.7\n",
      "evaluation/path length Std           282.9\n",
      "evaluation/path length Max          1000\n",
      "evaluation/path length Min            57\n",
      "evaluation/Rewards Mean                2.13741\n",
      "evaluation/Rewards Std                 1.90108\n",
      "evaluation/Rewards Max                 6.683\n",
      "evaluation/Rewards Min                -3.47748\n",
      "evaluation/Returns Mean             1935.85\n",
      "evaluation/Returns Std              1213.24\n",
      "evaluation/Returns Max              2899.29\n",
      "evaluation/Returns Min              -921.25\n",
      "evaluation/Estimation Bias Mean      156.619\n",
      "evaluation/Estimation Bias Std       139.319\n",
      "evaluation/EB/Q_True Mean             26.1587\n",
      "evaluation/EB/Q_True Std              78.831\n",
      "evaluation/EB/Q_Pred Mean            182.778\n",
      "evaluation/EB/Q_Pred Std              83.816\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns          1935.85\n",
      "evaluation/Actions Mean                0.0617539\n",
      "evaluation/Actions Std                 0.472059\n",
      "evaluation/Actions Max                 0.999708\n",
      "evaluation/Actions Min                -0.999586\n",
      "time/backward_policy (s)               8.61518\n",
      "time/backward_zf1 (s)                 10.567\n",
      "time/backward_zf2 (s)                 10.2486\n",
      "time/data sampling (s)                 1.30467\n",
      "time/data storing (s)                  0.0807853\n",
      "time/evaluation sampling (s)           3.4648\n",
      "time/exploration sampling (s)          1.70416\n",
      "time/logging (s)                       0.0175381\n",
      "time/preback_alpha (s)                 0.00522264\n",
      "time/preback_policy (s)                9.40499\n",
      "time/preback_start (s)                 0.671642\n",
      "time/preback_zf (s)                   28.8949\n",
      "time/saving (s)                        4.957e-06\n",
      "time/training (s)                     10.4355\n",
      "time/epoch (s)                        85.415\n",
      "time/total (s)                       428.912\n",
      "Epoch                                  4\n",
      "---------------------------------  --------------\n",
      "2024-11-07 17:28:25.244404 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 5 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 40000\n",
      "trainer/ZF1 Loss                      14.8628\n",
      "trainer/ZF2 Loss                      14.6391\n",
      "trainer/ZF Expert Reward              10.374\n",
      "trainer/ZF Policy Reward              -0.952089\n",
      "trainer/ZF CHI2 Term                  20.5892\n",
      "trainer/Policy Loss                  -71.3617\n",
      "trainer/expert_lambda Loss             4.97672\n",
      "trainer/expert_lambda Value           11.8943\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              16.7284\n",
      "trainer/Policy Param Norm             22.7347\n",
      "trainer/Zf1 Grad Norm               1412.09\n",
      "trainer/Zf1 Param Norm                48.8015\n",
      "trainer/Zf2 Grad Norm               2102.65\n",
      "trainer/Zf2 Param Norm                48.0612\n",
      "trainer/Z Expert Predictions Mean    295.293\n",
      "trainer/Z Expert Predictions Std      24.04\n",
      "trainer/Z Expert Predictions Max     327.635\n",
      "trainer/Z Expert Predictions Min     197.834\n",
      "trainer/Z Policy Predictions Mean     68.9114\n",
      "trainer/Z Policy Predictions Std      62.1828\n",
      "trainer/Z Policy Predictions Max     263.646\n",
      "trainer/Z Policy Predictions Min     -25.3204\n",
      "trainer/Z Expert Targets Mean        284.919\n",
      "trainer/Z Expert Targets Std          23.8811\n",
      "trainer/Z Expert Targets Max         317.303\n",
      "trainer/Z Expert Targets Min         188.064\n",
      "trainer/Z Policy Targets Mean         69.8635\n",
      "trainer/Z Policy Targets Std          63.2308\n",
      "trainer/Z Policy Targets Max         265.142\n",
      "trainer/Z Policy Targets Min         -23.5505\n",
      "trainer/Log Pis Mean                  17.8402\n",
      "trainer/Log Pis Std                    7.08204\n",
      "trainer/Policy mu Mean                 0.0418564\n",
      "trainer/Policy mu Std                  1.57363\n",
      "trainer/Policy log std Mean           -2.28609\n",
      "trainer/Policy log std Std             0.973132\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        33894\n",
      "exploration/num paths total          133\n",
      "evaluation/num steps total         43842\n",
      "evaluation/num paths total            60\n",
      "evaluation/path length Mean          352.1\n",
      "evaluation/path length Std           246.737\n",
      "evaluation/path length Max           866\n",
      "evaluation/path length Min            47\n",
      "evaluation/Rewards Mean                3.45975\n",
      "evaluation/Rewards Std                 1.35446\n",
      "evaluation/Rewards Max                 6.26153\n",
      "evaluation/Rewards Min                -1.76713\n",
      "evaluation/Returns Mean             1218.18\n",
      "evaluation/Returns Std               891.031\n",
      "evaluation/Returns Max              3028.44\n",
      "evaluation/Returns Min               115.093\n",
      "evaluation/Estimation Bias Mean      231.883\n",
      "evaluation/Estimation Bias Std       180.262\n",
      "evaluation/EB/Q_True Mean             80.0713\n",
      "evaluation/EB/Q_True Std             146.191\n",
      "evaluation/EB/Q_Pred Mean            311.955\n",
      "evaluation/EB/Q_Pred Std              91.0784\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns          1218.18\n",
      "evaluation/Actions Mean                0.0497273\n",
      "evaluation/Actions Std                 0.357188\n",
      "evaluation/Actions Max                 0.9957\n",
      "evaluation/Actions Min                -0.997989\n",
      "time/backward_policy (s)               8.54307\n",
      "time/backward_zf1 (s)                 10.5878\n",
      "time/backward_zf2 (s)                 10.153\n",
      "time/data sampling (s)                 1.38944\n",
      "time/data storing (s)                  0.0804974\n",
      "time/evaluation sampling (s)           3.59818\n",
      "time/exploration sampling (s)          1.68114\n",
      "time/logging (s)                       0.00910269\n",
      "time/preback_alpha (s)                 0.00518876\n",
      "time/preback_policy (s)                9.70993\n",
      "time/preback_start (s)                 0.686855\n",
      "time/preback_zf (s)                   29.1168\n",
      "time/saving (s)                        6.051e-06\n",
      "time/training (s)                     10.5924\n",
      "time/epoch (s)                        86.1534\n",
      "time/total (s)                       515.068\n",
      "Epoch                                  5\n",
      "---------------------------------  --------------\n",
      "2024-11-07 17:29:53.534800 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 6 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 45000\n",
      "trainer/ZF1 Loss                      21.3285\n",
      "trainer/ZF2 Loss                      26.2081\n",
      "trainer/ZF Expert Reward              12.2634\n",
      "trainer/ZF Policy Reward               0.0300931\n",
      "trainer/ZF CHI2 Term                  30.0883\n",
      "trainer/Policy Loss                 -106.949\n",
      "trainer/expert_lambda Loss             6.80904\n",
      "trainer/expert_lambda Value           12.1788\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              25.9307\n",
      "trainer/Policy Param Norm             23.8191\n",
      "trainer/Zf1 Grad Norm               1203.41\n",
      "trainer/Zf1 Param Norm                51.5836\n",
      "trainer/Zf2 Grad Norm               1256.63\n",
      "trainer/Zf2 Param Norm                51.0398\n",
      "trainer/Z Expert Predictions Mean    407.425\n",
      "trainer/Z Expert Predictions Std      38.564\n",
      "trainer/Z Expert Predictions Max     439.203\n",
      "trainer/Z Expert Predictions Min       6.53367\n",
      "trainer/Z Policy Predictions Mean    104.092\n",
      "trainer/Z Policy Predictions Std     106.956\n",
      "trainer/Z Policy Predictions Max     417.629\n",
      "trainer/Z Policy Predictions Min     -30.6661\n",
      "trainer/Z Expert Targets Mean        395.162\n",
      "trainer/Z Expert Targets Std          39.2036\n",
      "trainer/Z Expert Targets Max         427.79\n",
      "trainer/Z Expert Targets Min           0\n",
      "trainer/Z Policy Targets Mean        104.062\n",
      "trainer/Z Policy Targets Std         105.698\n",
      "trainer/Z Policy Targets Max         416.31\n",
      "trainer/Z Policy Targets Min         -29.9998\n",
      "trainer/Log Pis Mean                  19.2396\n",
      "trainer/Log Pis Std                    5.87498\n",
      "trainer/Policy mu Mean                 0.0131091\n",
      "trainer/Policy mu Std                  1.54357\n",
      "trainer/Policy log std Mean           -2.45681\n",
      "trainer/Policy log std Std             1.11395\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        38894\n",
      "exploration/num paths total          138\n",
      "evaluation/num steps total         53842\n",
      "evaluation/num paths total            70\n",
      "evaluation/path length Mean         1000\n",
      "evaluation/path length Std             0\n",
      "evaluation/path length Max          1000\n",
      "evaluation/path length Min          1000\n",
      "evaluation/Rewards Mean                3.74719\n",
      "evaluation/Rewards Std                 1.53884\n",
      "evaluation/Rewards Max                 7.47116\n",
      "evaluation/Rewards Min                -2.43911\n",
      "evaluation/Returns Mean             3747.19\n",
      "evaluation/Returns Std               759.608\n",
      "evaluation/Returns Max              4166.83\n",
      "evaluation/Returns Min              1481.63\n",
      "evaluation/Estimation Bias Mean      421.502\n",
      "evaluation/Estimation Bias Std       146.706\n",
      "evaluation/EB/Q_True Mean             35.8608\n",
      "evaluation/EB/Q_True Std             110.359\n",
      "evaluation/EB/Q_Pred Mean            457.363\n",
      "evaluation/EB/Q_Pred Std             104.557\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns          3747.19\n",
      "evaluation/Actions Mean                0.0578281\n",
      "evaluation/Actions Std                 0.351709\n",
      "evaluation/Actions Max                 0.998355\n",
      "evaluation/Actions Min                -0.999485\n",
      "time/backward_policy (s)               8.69101\n",
      "time/backward_zf1 (s)                 10.9064\n",
      "time/backward_zf2 (s)                 10.4521\n",
      "time/data sampling (s)                 1.41672\n",
      "time/data storing (s)                  0.0850846\n",
      "time/evaluation sampling (s)           3.93185\n",
      "time/exploration sampling (s)          1.67934\n",
      "time/logging (s)                       0.0237461\n",
      "time/preback_alpha (s)                 0.00534719\n",
      "time/preback_policy (s)                9.92411\n",
      "time/preback_start (s)                 0.70429\n",
      "time/preback_zf (s)                   29.4158\n",
      "time/saving (s)                        5.656e-06\n",
      "time/training (s)                     10.8511\n",
      "time/epoch (s)                        88.0869\n",
      "time/total (s)                       603.157\n",
      "Epoch                                  6\n",
      "---------------------------------  --------------\n",
      "2024-11-07 17:31:22.053329 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 7 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 50000\n",
      "trainer/ZF1 Loss                      74.3067\n",
      "trainer/ZF2 Loss                      71.8587\n",
      "trainer/ZF Expert Reward              11.1788\n",
      "trainer/ZF Policy Reward               1.7462\n",
      "trainer/ZF CHI2 Term                  78.0373\n",
      "trainer/Policy Loss                 -174.089\n",
      "trainer/expert_lambda Loss             7.86633\n",
      "trainer/expert_lambda Value           12.2866\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              38.7808\n",
      "trainer/Policy Param Norm             24.9689\n",
      "trainer/Zf1 Grad Norm               2175.09\n",
      "trainer/Zf1 Param Norm                54.8633\n",
      "trainer/Zf2 Grad Norm               2607.08\n",
      "trainer/Zf2 Param Norm                54.5383\n",
      "trainer/Z Expert Predictions Mean    529.802\n",
      "trainer/Z Expert Predictions Std      51.9511\n",
      "trainer/Z Expert Predictions Max     575.924\n",
      "trainer/Z Expert Predictions Min     285.638\n",
      "trainer/Z Policy Predictions Mean    169.971\n",
      "trainer/Z Policy Predictions Std     162.649\n",
      "trainer/Z Policy Predictions Max     567.8\n",
      "trainer/Z Policy Predictions Min     -41.4031\n",
      "trainer/Z Expert Targets Mean        518.623\n",
      "trainer/Z Expert Targets Std          52.0329\n",
      "trainer/Z Expert Targets Max         562.366\n",
      "trainer/Z Expert Targets Min         272.61\n",
      "trainer/Z Policy Targets Mean        168.225\n",
      "trainer/Z Policy Targets Std         161.011\n",
      "trainer/Z Policy Targets Max         545.038\n",
      "trainer/Z Policy Targets Min         -40.1589\n",
      "trainer/Log Pis Mean                  22.5732\n",
      "trainer/Log Pis Std                    6.9188\n",
      "trainer/Policy mu Mean                -0.0485947\n",
      "trainer/Policy mu Std                  1.75119\n",
      "trainer/Policy log std Mean           -2.64949\n",
      "trainer/Policy log std Std             1.24209\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        43894\n",
      "exploration/num paths total          143\n",
      "evaluation/num steps total         62364\n",
      "evaluation/num paths total            80\n",
      "evaluation/path length Mean          852.2\n",
      "evaluation/path length Std           281.271\n",
      "evaluation/path length Max          1000\n",
      "evaluation/path length Min           150\n",
      "evaluation/Rewards Mean                3.72265\n",
      "evaluation/Rewards Std                 1.97624\n",
      "evaluation/Rewards Max                 6.9733\n",
      "evaluation/Rewards Min                -3.38291\n",
      "evaluation/Returns Mean             3172.44\n",
      "evaluation/Returns Std              1508.16\n",
      "evaluation/Returns Max              4372.66\n",
      "evaluation/Returns Min               533.952\n",
      "evaluation/Estimation Bias Mean      490.717\n",
      "evaluation/Estimation Bias Std       209.789\n",
      "evaluation/EB/Q_True Mean             46.6126\n",
      "evaluation/EB/Q_True Std             131.028\n",
      "evaluation/EB/Q_Pred Mean            537.329\n",
      "evaluation/EB/Q_Pred Std             160.831\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns          3172.44\n",
      "evaluation/Actions Mean                0.0187369\n",
      "evaluation/Actions Std                 0.397401\n",
      "evaluation/Actions Max                 1\n",
      "evaluation/Actions Min                -1\n",
      "time/backward_policy (s)               8.63817\n",
      "time/backward_zf1 (s)                 10.9995\n",
      "time/backward_zf2 (s)                 10.5119\n",
      "time/data sampling (s)                 1.4618\n",
      "time/data storing (s)                  0.0833224\n",
      "time/evaluation sampling (s)           3.55086\n",
      "time/exploration sampling (s)          1.66014\n",
      "time/logging (s)                       0.0125381\n",
      "time/preback_alpha (s)                 0.00537868\n",
      "time/preback_policy (s)               10.0999\n",
      "time/preback_start (s)                 0.715586\n",
      "time/preback_zf (s)                   29.6027\n",
      "time/saving (s)                        3.946e-06\n",
      "time/training (s)                     10.9417\n",
      "time/epoch (s)                        88.2836\n",
      "time/total (s)                       691.442\n",
      "Epoch                                  7\n",
      "---------------------------------  --------------\n",
      "2024-11-07 17:32:49.516418 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 8 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 55000\n",
      "trainer/ZF1 Loss                      79.1796\n",
      "trainer/ZF2 Loss                      57.6076\n",
      "trainer/ZF Expert Reward              12.2101\n",
      "trainer/ZF Policy Reward              -0.198204\n",
      "trainer/ZF CHI2 Term                  74.858\n",
      "trainer/Policy Loss                 -284.19\n",
      "trainer/expert_lambda Loss             8.37701\n",
      "trainer/expert_lambda Value           12.2735\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              57.0764\n",
      "trainer/Policy Param Norm             25.7607\n",
      "trainer/Zf1 Grad Norm               3977.93\n",
      "trainer/Zf1 Param Norm                57.7817\n",
      "trainer/Zf2 Grad Norm               2567.53\n",
      "trainer/Zf2 Param Norm                57.6973\n",
      "trainer/Z Expert Predictions Mean    637.289\n",
      "trainer/Z Expert Predictions Std      53.7291\n",
      "trainer/Z Expert Predictions Max     695.467\n",
      "trainer/Z Expert Predictions Min     377.768\n",
      "trainer/Z Policy Predictions Mean    279.356\n",
      "trainer/Z Policy Predictions Std     203.092\n",
      "trainer/Z Policy Predictions Max     661.162\n",
      "trainer/Z Policy Predictions Min     -50.0367\n",
      "trainer/Z Expert Targets Mean        625.079\n",
      "trainer/Z Expert Targets Std          54.4218\n",
      "trainer/Z Expert Targets Max         681.72\n",
      "trainer/Z Expert Targets Min         359.27\n",
      "trainer/Z Policy Targets Mean        279.555\n",
      "trainer/Z Policy Targets Std         202.197\n",
      "trainer/Z Policy Targets Max         653.166\n",
      "trainer/Z Policy Targets Min         -45.6177\n",
      "trainer/Log Pis Mean                  24.4217\n",
      "trainer/Log Pis Std                    6.61009\n",
      "trainer/Policy mu Mean                -0.220098\n",
      "trainer/Policy mu Std                  1.74337\n",
      "trainer/Policy log std Mean           -2.95067\n",
      "trainer/Policy log std Std             1.32819\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        48894\n",
      "exploration/num paths total          148\n",
      "evaluation/num steps total         70128\n",
      "evaluation/num paths total            91\n",
      "evaluation/path length Mean          705.818\n",
      "evaluation/path length Std           360.734\n",
      "evaluation/path length Max          1000\n",
      "evaluation/path length Min            67\n",
      "evaluation/Rewards Mean                3.05376\n",
      "evaluation/Rewards Std                 2.62983\n",
      "evaluation/Rewards Max                 7.59035\n",
      "evaluation/Rewards Min                -3.07784\n",
      "evaluation/Returns Mean             2155.4\n",
      "evaluation/Returns Std              1761.78\n",
      "evaluation/Returns Max              4364.61\n",
      "evaluation/Returns Min             -1500.8\n",
      "evaluation/Estimation Bias Mean      531.701\n",
      "evaluation/Estimation Bias Std       259.255\n",
      "evaluation/EB/Q_True Mean            -19.2562\n",
      "evaluation/EB/Q_True Std              52.2341\n",
      "evaluation/EB/Q_Pred Mean            512.444\n",
      "evaluation/EB/Q_Pred Std             245.465\n",
      "evaluation/Num Paths                  11\n",
      "evaluation/Average Returns          2155.4\n",
      "evaluation/Actions Mean                0.0308087\n",
      "evaluation/Actions Std                 0.468879\n",
      "evaluation/Actions Max                 0.999998\n",
      "evaluation/Actions Min                -0.999981\n",
      "time/backward_policy (s)               8.54092\n",
      "time/backward_zf1 (s)                 10.6645\n",
      "time/backward_zf2 (s)                 10.2595\n",
      "time/data sampling (s)                 1.43152\n",
      "time/data storing (s)                  0.0818476\n",
      "time/evaluation sampling (s)           3.94976\n",
      "time/exploration sampling (s)          1.64731\n",
      "time/logging (s)                       0.0120968\n",
      "time/preback_alpha (s)                 0.00525397\n",
      "time/preback_policy (s)                9.88414\n",
      "time/preback_start (s)                 0.69921\n",
      "time/preback_zf (s)                   29.2517\n",
      "time/saving (s)                        4.806e-06\n",
      "time/training (s)                     10.8181\n",
      "time/epoch (s)                        87.2458\n",
      "time/total (s)                       778.691\n",
      "Epoch                                  8\n",
      "---------------------------------  --------------\n",
      "2024-11-07 17:34:14.373474 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 9 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 60000\n",
      "trainer/ZF1 Loss                     111.225\n",
      "trainer/ZF2 Loss                      82.2973\n",
      "trainer/ZF Expert Reward              12.2401\n",
      "trainer/ZF Policy Reward              -0.425558\n",
      "trainer/ZF CHI2 Term                 103.355\n",
      "trainer/Policy Loss                 -343.532\n",
      "trainer/expert_lambda Loss            17.5491\n",
      "trainer/expert_lambda Value           12.1892\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              65.4516\n",
      "trainer/Policy Param Norm             26.3087\n",
      "trainer/Zf1 Grad Norm               5003.8\n",
      "trainer/Zf1 Param Norm                60.4195\n",
      "trainer/Zf2 Grad Norm               3996.12\n",
      "trainer/Zf2 Param Norm                60.4822\n",
      "trainer/Z Expert Predictions Mean    724.475\n",
      "trainer/Z Expert Predictions Std      58.884\n",
      "trainer/Z Expert Predictions Max     783.963\n",
      "trainer/Z Expert Predictions Min     427.91\n",
      "trainer/Z Policy Predictions Mean    336.766\n",
      "trainer/Z Policy Predictions Std     237.482\n",
      "trainer/Z Policy Predictions Max     774.151\n",
      "trainer/Z Policy Predictions Min     -58.0048\n",
      "trainer/Z Expert Targets Mean        712.235\n",
      "trainer/Z Expert Targets Std          57.9216\n",
      "trainer/Z Expert Targets Max         771.145\n",
      "trainer/Z Expert Targets Min         426.566\n",
      "trainer/Z Policy Targets Mean        337.192\n",
      "trainer/Z Policy Targets Std         235.272\n",
      "trainer/Z Policy Targets Max         765.14\n",
      "trainer/Z Policy Targets Min         -68.4792\n",
      "trainer/Log Pis Mean                  24.237\n",
      "trainer/Log Pis Std                    6.30693\n",
      "trainer/Policy mu Mean                 0.160423\n",
      "trainer/Policy mu Std                  1.67137\n",
      "trainer/Policy log std Mean           -3.0347\n",
      "trainer/Policy log std Std             1.37768\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        54240\n",
      "exploration/num paths total          154\n",
      "evaluation/num steps total         80128\n",
      "evaluation/num paths total           101\n",
      "evaluation/path length Mean         1000\n",
      "evaluation/path length Std             0\n",
      "evaluation/path length Max          1000\n",
      "evaluation/path length Min          1000\n",
      "evaluation/Rewards Mean                4.04356\n",
      "evaluation/Rewards Std                 1.83838\n",
      "evaluation/Rewards Max                 7.47209\n",
      "evaluation/Rewards Min                -2.63872\n",
      "evaluation/Returns Mean             4043.56\n",
      "evaluation/Returns Std               913.778\n",
      "evaluation/Returns Max              4617.73\n",
      "evaluation/Returns Min              1356.09\n",
      "evaluation/Estimation Bias Mean      645.04\n",
      "evaluation/Estimation Bias Std       212.061\n",
      "evaluation/EB/Q_True Mean             39.5558\n",
      "evaluation/EB/Q_True Std             122.232\n",
      "evaluation/EB/Q_Pred Mean            684.595\n",
      "evaluation/EB/Q_Pred Std             177.151\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns          4043.56\n",
      "evaluation/Actions Mean                0.0642347\n",
      "evaluation/Actions Std                 0.36442\n",
      "evaluation/Actions Max                 0.999981\n",
      "evaluation/Actions Min                -1\n",
      "time/backward_policy (s)               7.95436\n",
      "time/backward_zf1 (s)                  9.92873\n",
      "time/backward_zf2 (s)                  9.55172\n",
      "time/data sampling (s)                 1.34959\n",
      "time/data storing (s)                  0.0795382\n",
      "time/evaluation sampling (s)           3.95777\n",
      "time/exploration sampling (s)          1.62621\n",
      "time/logging (s)                       0.015275\n",
      "time/preback_alpha (s)                 0.00507567\n",
      "time/preback_policy (s)                9.95674\n",
      "time/preback_start (s)                 0.671866\n",
      "time/preback_zf (s)                   28.8077\n",
      "time/saving (s)                        5.041e-06\n",
      "time/training (s)                     10.7475\n",
      "time/epoch (s)                        84.6521\n",
      "time/total (s)                       863.345\n",
      "Epoch                                  9\n",
      "---------------------------------  --------------\n",
      "2024-11-07 17:35:40.579198 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 10 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 65000\n",
      "trainer/ZF1 Loss                     847.495\n",
      "trainer/ZF2 Loss                     813.203\n",
      "trainer/ZF Expert Reward              14.8503\n",
      "trainer/ZF Policy Reward               7.15\n",
      "trainer/ZF CHI2 Term                 834.464\n",
      "trainer/Policy Loss                 -444.793\n",
      "trainer/expert_lambda Loss            22.4772\n",
      "trainer/expert_lambda Value           12.0674\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              83.8281\n",
      "trainer/Policy Param Norm             26.8805\n",
      "trainer/Zf1 Grad Norm              16033.1\n",
      "trainer/Zf1 Param Norm                62.7548\n",
      "trainer/Zf2 Grad Norm              17899.7\n",
      "trainer/Zf2 Param Norm                63.0098\n",
      "trainer/Z Expert Predictions Mean    787.617\n",
      "trainer/Z Expert Predictions Std      68.8706\n",
      "trainer/Z Expert Predictions Max     869.583\n",
      "trainer/Z Expert Predictions Min     270.712\n",
      "trainer/Z Policy Predictions Mean    442.048\n",
      "trainer/Z Policy Predictions Std     247.174\n",
      "trainer/Z Policy Predictions Max     821.134\n",
      "trainer/Z Policy Predictions Min     -45.7583\n",
      "trainer/Z Expert Targets Mean        772.767\n",
      "trainer/Z Expert Targets Std          69.5567\n",
      "trainer/Z Expert Targets Max         851.639\n",
      "trainer/Z Expert Targets Min         239.304\n",
      "trainer/Z Policy Targets Mean        434.898\n",
      "trainer/Z Policy Targets Std         244.709\n",
      "trainer/Z Policy Targets Max         834.181\n",
      "trainer/Z Policy Targets Min         -46.1833\n",
      "trainer/Log Pis Mean                  24.79\n",
      "trainer/Log Pis Std                    5.82998\n",
      "trainer/Policy mu Mean                 0.186706\n",
      "trainer/Policy mu Std                  1.43792\n",
      "trainer/Policy log std Mean           -3.37691\n",
      "trainer/Policy log std Std             1.3587\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        59240\n",
      "exploration/num paths total          159\n",
      "evaluation/num steps total         89753\n",
      "evaluation/num paths total           112\n",
      "evaluation/path length Mean          875\n",
      "evaluation/path length Std           290.022\n",
      "evaluation/path length Max          1000\n",
      "evaluation/path length Min            37\n",
      "evaluation/Rewards Mean                3.82677\n",
      "evaluation/Rewards Std                 1.89233\n",
      "evaluation/Rewards Max                 7.05034\n",
      "evaluation/Rewards Min                -3.08419\n",
      "evaluation/Returns Mean             3348.42\n",
      "evaluation/Returns Std              1664.16\n",
      "evaluation/Returns Max              4438.27\n",
      "evaluation/Returns Min              -102.397\n",
      "evaluation/Estimation Bias Mean      668.948\n",
      "evaluation/Estimation Bias Std       250.886\n",
      "evaluation/EB/Q_True Mean             39.7165\n",
      "evaluation/EB/Q_True Std             119.832\n",
      "evaluation/EB/Q_Pred Mean            708.665\n",
      "evaluation/EB/Q_Pred Std             227.599\n",
      "evaluation/Num Paths                  11\n",
      "evaluation/Average Returns          3348.42\n",
      "evaluation/Actions Mean                0.0690818\n",
      "evaluation/Actions Std                 0.370607\n",
      "evaluation/Actions Max                 0.999995\n",
      "evaluation/Actions Min                -1\n",
      "time/backward_policy (s)               8.61819\n",
      "time/backward_zf1 (s)                 10.5573\n",
      "time/backward_zf2 (s)                 10.2798\n",
      "time/data sampling (s)                 1.30944\n",
      "time/data storing (s)                  0.0812338\n",
      "time/evaluation sampling (s)           4.17425\n",
      "time/exploration sampling (s)          1.62097\n",
      "time/logging (s)                       0.0116797\n",
      "time/preback_alpha (s)                 0.00511806\n",
      "time/preback_policy (s)                9.38793\n",
      "time/preback_start (s)                 0.67706\n",
      "time/preback_zf (s)                   28.8406\n",
      "time/saving (s)                        2.694e-06\n",
      "time/training (s)                     10.4289\n",
      "time/epoch (s)                        85.9925\n",
      "time/total (s)                       949.339\n",
      "Epoch                                 10\n",
      "---------------------------------  --------------\n",
      "2024-11-07 17:37:05.690758 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 11 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 70000\n",
      "trainer/ZF1 Loss                     122.613\n",
      "trainer/ZF2 Loss                     134.768\n",
      "trainer/ZF Expert Reward               9.73059\n",
      "trainer/ZF Policy Reward               1.10515\n",
      "trainer/ZF CHI2 Term                 133.281\n",
      "trainer/Policy Loss                 -475.775\n",
      "trainer/expert_lambda Loss            31.6644\n",
      "trainer/expert_lambda Value           11.921\n",
      "trainer/policy_lambda Value            5\n",
      "trainer/Policy Grad Norm              94.8849\n",
      "trainer/Policy Param Norm             27.3641\n",
      "trainer/Zf1 Grad Norm               6508.14\n",
      "trainer/Zf1 Param Norm                64.9099\n",
      "trainer/Zf2 Grad Norm              10914.4\n",
      "trainer/Zf2 Param Norm                65.2797\n",
      "trainer/Z Expert Predictions Mean    843.751\n",
      "trainer/Z Expert Predictions Std      65.136\n",
      "trainer/Z Expert Predictions Max     933.876\n",
      "trainer/Z Expert Predictions Min     580.048\n",
      "trainer/Z Policy Predictions Mean    470.42\n",
      "trainer/Z Policy Predictions Std     278.236\n",
      "trainer/Z Policy Predictions Max     904.007\n",
      "trainer/Z Policy Predictions Min     -36.1048\n",
      "trainer/Z Expert Targets Mean        834.021\n",
      "trainer/Z Expert Targets Std          65.6713\n",
      "trainer/Z Expert Targets Max         922.17\n",
      "trainer/Z Expert Targets Min         569.763\n",
      "trainer/Z Policy Targets Mean        469.314\n",
      "trainer/Z Policy Targets Std         275.832\n",
      "trainer/Z Policy Targets Max         890.071\n",
      "trainer/Z Policy Targets Min         -34.0951\n",
      "trainer/Log Pis Mean                  25.5281\n",
      "trainer/Log Pis Std                    6.52901\n",
      "trainer/Policy mu Mean                 0.16355\n",
      "trainer/Policy mu Std                  1.46814\n",
      "trainer/Policy log std Mean           -3.43339\n",
      "trainer/Policy log std Std             1.34714\n",
      "trainer/Alpha                          0.01\n",
      "exploration/num steps total        64240\n",
      "exploration/num paths total          164\n",
      "evaluation/num steps total         99753\n",
      "evaluation/num paths total           122\n",
      "evaluation/path length Mean         1000\n",
      "evaluation/path length Std             0\n",
      "evaluation/path length Max          1000\n",
      "evaluation/path length Min          1000\n",
      "evaluation/Rewards Mean                4.37262\n",
      "evaluation/Rewards Std                 1.09918\n",
      "evaluation/Rewards Max                 6.98332\n",
      "evaluation/Rewards Min                -2.69595\n",
      "evaluation/Returns Mean             4372.62\n",
      "evaluation/Returns Std               117.278\n",
      "evaluation/Returns Max              4575.27\n",
      "evaluation/Returns Min              4166.31\n",
      "evaluation/Estimation Bias Mean      772.631\n",
      "evaluation/Estimation Bias Std       157.616\n",
      "evaluation/EB/Q_True Mean             41.8288\n",
      "evaluation/EB/Q_True Std             128.897\n",
      "evaluation/EB/Q_Pred Mean            814.46\n",
      "evaluation/EB/Q_Pred Std              80.9345\n",
      "evaluation/Num Paths                  10\n",
      "evaluation/Average Returns          4372.62\n",
      "evaluation/Actions Mean                0.0457197\n",
      "evaluation/Actions Std                 0.310552\n",
      "evaluation/Actions Max                 0.996224\n",
      "evaluation/Actions Min                -0.999588\n",
      "time/backward_policy (s)               8.09687\n",
      "time/backward_zf1 (s)                 10.1027\n",
      "time/backward_zf2 (s)                  9.76582\n",
      "time/data sampling (s)                 1.32977\n",
      "time/data storing (s)                  0.0800166\n",
      "time/evaluation sampling (s)           3.81231\n",
      "time/exploration sampling (s)          1.58749\n",
      "time/logging (s)                       0.0136441\n",
      "time/preback_alpha (s)                 0.00512671\n",
      "time/preback_policy (s)                9.85074\n",
      "time/preback_start (s)                 0.672306\n",
      "time/preback_zf (s)                   28.8042\n",
      "time/saving (s)                        3.935e-06\n",
      "time/training (s)                     10.7841\n",
      "time/epoch (s)                        84.9052\n",
      "time/total (s)                      1034.25\n",
      "Epoch                                 11\n",
      "---------------------------------  --------------\n",
      "2024-11-07 17:38:32.481452 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 12 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                  75000\n",
      "trainer/ZF1 Loss                      112.425\n",
      "trainer/ZF2 Loss                      100.269\n",
      "trainer/ZF Expert Reward                8.79134\n",
      "trainer/ZF Policy Reward               -1.70551\n",
      "trainer/ZF CHI2 Term                  111.878\n",
      "trainer/Policy Loss                  -535.221\n",
      "trainer/expert_lambda Loss             29.3575\n",
      "trainer/expert_lambda Value            11.755\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               73.5558\n",
      "trainer/Policy Param Norm              27.7485\n",
      "trainer/Zf1 Grad Norm                7257.94\n",
      "trainer/Zf1 Param Norm                 66.7104\n",
      "trainer/Zf2 Grad Norm                7405.38\n",
      "trainer/Zf2 Param Norm                 67.258\n",
      "trainer/Z Expert Predictions Mean     861.262\n",
      "trainer/Z Expert Predictions Std       86.5307\n",
      "trainer/Z Expert Predictions Max      972.745\n",
      "trainer/Z Expert Predictions Min      364.642\n",
      "trainer/Z Policy Predictions Mean     529.309\n",
      "trainer/Z Policy Predictions Std      269.203\n",
      "trainer/Z Policy Predictions Max      952.654\n",
      "trainer/Z Policy Predictions Min       -3.51713\n",
      "trainer/Z Expert Targets Mean         852.471\n",
      "trainer/Z Expert Targets Std           85.9424\n",
      "trainer/Z Expert Targets Max          962.548\n",
      "trainer/Z Expert Targets Min          390.75\n",
      "trainer/Z Policy Targets Mean         531.015\n",
      "trainer/Z Policy Targets Std          266.908\n",
      "trainer/Z Policy Targets Max          925.466\n",
      "trainer/Z Policy Targets Min           -7.5617\n",
      "trainer/Log Pis Mean                   26.4165\n",
      "trainer/Log Pis Std                     6.2644\n",
      "trainer/Policy mu Mean                  0.10063\n",
      "trainer/Policy mu Std                   1.49578\n",
      "trainer/Policy log std Mean            -3.57599\n",
      "trainer/Policy log std Std              1.28808\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total         69240\n",
      "exploration/num paths total           169\n",
      "evaluation/num steps total         109753\n",
      "evaluation/num paths total            132\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 4.2098\n",
      "evaluation/Rewards Std                  1.73165\n",
      "evaluation/Rewards Max                  7.30975\n",
      "evaluation/Rewards Min                 -3.03663\n",
      "evaluation/Returns Mean              4209.8\n",
      "evaluation/Returns Std                955.064\n",
      "evaluation/Returns Max               4722.81\n",
      "evaluation/Returns Min               1374.6\n",
      "evaluation/Estimation Bias Mean       734.093\n",
      "evaluation/Estimation Bias Std        218.494\n",
      "evaluation/EB/Q_True Mean              42.8518\n",
      "evaluation/EB/Q_True Std              132.618\n",
      "evaluation/EB/Q_Pred Mean             776.945\n",
      "evaluation/EB/Q_Pred Std              182.814\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4209.8\n",
      "evaluation/Actions Mean                 0.0176058\n",
      "evaluation/Actions Std                  0.356575\n",
      "evaluation/Actions Max                  0.999993\n",
      "evaluation/Actions Min                 -1\n",
      "time/backward_policy (s)                8.63708\n",
      "time/backward_zf1 (s)                  10.7706\n",
      "time/backward_zf2 (s)                  10.3621\n",
      "time/data sampling (s)                  1.40686\n",
      "time/data storing (s)                   0.0836713\n",
      "time/evaluation sampling (s)            3.38749\n",
      "time/exploration sampling (s)           1.63234\n",
      "time/logging (s)                        0.0170171\n",
      "time/preback_alpha (s)                  0.00523661\n",
      "time/preback_policy (s)                 9.71543\n",
      "time/preback_start (s)                  0.714383\n",
      "time/preback_zf (s)                    29.1127\n",
      "time/saving (s)                         4.813e-06\n",
      "time/training (s)                      10.7337\n",
      "time/epoch (s)                         86.5787\n",
      "time/total (s)                       1120.83\n",
      "Epoch                                  12\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:40:00.663142 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 13 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                  80000\n",
      "trainer/ZF1 Loss                      134.654\n",
      "trainer/ZF2 Loss                      161.395\n",
      "trainer/ZF Expert Reward               11.3771\n",
      "trainer/ZF Policy Reward                2.32605\n",
      "trainer/ZF CHI2 Term                  152.833\n",
      "trainer/Policy Loss                  -568.915\n",
      "trainer/expert_lambda Loss             22.1189\n",
      "trainer/expert_lambda Value            11.5637\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               94.714\n",
      "trainer/Policy Param Norm              28.1684\n",
      "trainer/Zf1 Grad Norm                5610.52\n",
      "trainer/Zf1 Param Norm                 68.3155\n",
      "trainer/Zf2 Grad Norm                6596.81\n",
      "trainer/Zf2 Param Norm                 68.9642\n",
      "trainer/Z Expert Predictions Mean     899.711\n",
      "trainer/Z Expert Predictions Std       70.959\n",
      "trainer/Z Expert Predictions Max      999.401\n",
      "trainer/Z Expert Predictions Min      395.199\n",
      "trainer/Z Policy Predictions Mean     565.284\n",
      "trainer/Z Policy Predictions Std      252.227\n",
      "trainer/Z Policy Predictions Max      939.991\n",
      "trainer/Z Policy Predictions Min       12.9421\n",
      "trainer/Z Expert Targets Mean         888.333\n",
      "trainer/Z Expert Targets Std           70.4441\n",
      "trainer/Z Expert Targets Max          986.417\n",
      "trainer/Z Expert Targets Min          384.267\n",
      "trainer/Z Policy Targets Mean         562.958\n",
      "trainer/Z Policy Targets Std          249.368\n",
      "trainer/Z Policy Targets Max          913.695\n",
      "trainer/Z Policy Targets Min            0\n",
      "trainer/Log Pis Mean                   26.4845\n",
      "trainer/Log Pis Std                     7.1829\n",
      "trainer/Policy mu Mean                  0.0953993\n",
      "trainer/Policy mu Std                   1.57382\n",
      "trainer/Policy log std Mean            -3.5423\n",
      "trainer/Policy log std Std              1.36439\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total         74235\n",
      "exploration/num paths total           174\n",
      "evaluation/num steps total         118544\n",
      "evaluation/num paths total            142\n",
      "evaluation/path length Mean           879.1\n",
      "evaluation/path length Std            150.148\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            665\n",
      "evaluation/Rewards Mean                 3.56132\n",
      "evaluation/Rewards Std                  2.33447\n",
      "evaluation/Rewards Max                  7.47012\n",
      "evaluation/Rewards Min                 -2.62378\n",
      "evaluation/Returns Mean              3130.75\n",
      "evaluation/Returns Std               1474.61\n",
      "evaluation/Returns Max               4703.5\n",
      "evaluation/Returns Min                270.997\n",
      "evaluation/Estimation Bias Mean       730.594\n",
      "evaluation/Estimation Bias Std        185.674\n",
      "evaluation/EB/Q_True Mean              48.992\n",
      "evaluation/EB/Q_True Std              141.074\n",
      "evaluation/EB/Q_Pred Mean             779.586\n",
      "evaluation/EB/Q_Pred Std              130.139\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3130.75\n",
      "evaluation/Actions Mean                 0.0710184\n",
      "evaluation/Actions Std                  0.402724\n",
      "evaluation/Actions Max                  0.999962\n",
      "evaluation/Actions Min                 -0.999957\n",
      "time/backward_policy (s)                8.55952\n",
      "time/backward_zf1 (s)                  10.826\n",
      "time/backward_zf2 (s)                  10.3755\n",
      "time/data sampling (s)                  1.45492\n",
      "time/data storing (s)                   0.0887078\n",
      "time/evaluation sampling (s)            3.57795\n",
      "time/exploration sampling (s)           1.69493\n",
      "time/logging (s)                        0.0117877\n",
      "time/preback_alpha (s)                  0.00525099\n",
      "time/preback_policy (s)                10.1398\n",
      "time/preback_start (s)                  0.725393\n",
      "time/preback_zf (s)                    29.49\n",
      "time/saving (s)                         2.775e-06\n",
      "time/training (s)                      11.0043\n",
      "time/epoch (s)                         87.9541\n",
      "time/total (s)                       1208.78\n",
      "Epoch                                  13\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:41:28.856790 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 14 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                  85000\n",
      "trainer/ZF1 Loss                      146.239\n",
      "trainer/ZF2 Loss                      106.145\n",
      "trainer/ZF Expert Reward               16.2364\n",
      "trainer/ZF Policy Reward                6.09089\n",
      "trainer/ZF CHI2 Term                  131.553\n",
      "trainer/Policy Loss                  -578.783\n",
      "trainer/expert_lambda Loss             50.5027\n",
      "trainer/expert_lambda Value            11.3738\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               79.8822\n",
      "trainer/Policy Param Norm              28.5032\n",
      "trainer/Zf1 Grad Norm                9045.57\n",
      "trainer/Zf1 Param Norm                 69.753\n",
      "trainer/Zf2 Grad Norm                5871.85\n",
      "trainer/Zf2 Param Norm                 70.4635\n",
      "trainer/Z Expert Predictions Mean     917.575\n",
      "trainer/Z Expert Predictions Std       70.3895\n",
      "trainer/Z Expert Predictions Max     1023.03\n",
      "trainer/Z Expert Predictions Min      376.396\n",
      "trainer/Z Policy Predictions Mean     577.737\n",
      "trainer/Z Policy Predictions Std      286.582\n",
      "trainer/Z Policy Predictions Max      980.025\n",
      "trainer/Z Policy Predictions Min      -18.774\n",
      "trainer/Z Expert Targets Mean         901.339\n",
      "trainer/Z Expert Targets Std           71.8832\n",
      "trainer/Z Expert Targets Max         1003.82\n",
      "trainer/Z Expert Targets Min          348.986\n",
      "trainer/Z Policy Targets Mean         571.646\n",
      "trainer/Z Policy Targets Std          282.511\n",
      "trainer/Z Policy Targets Max          964.637\n",
      "trainer/Z Policy Targets Min          -25.2294\n",
      "trainer/Log Pis Mean                   26.879\n",
      "trainer/Log Pis Std                     6.64818\n",
      "trainer/Policy mu Mean                  0.202615\n",
      "trainer/Policy mu Std                   1.5483\n",
      "trainer/Policy log std Mean            -3.59991\n",
      "trainer/Policy log std Std              1.46927\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total         79235\n",
      "exploration/num paths total           179\n",
      "evaluation/num steps total         125506\n",
      "evaluation/num paths total            152\n",
      "evaluation/path length Mean           696.2\n",
      "evaluation/path length Std            348.845\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            127\n",
      "evaluation/Rewards Mean                 4.40693\n",
      "evaluation/Rewards Std                  1.27412\n",
      "evaluation/Rewards Max                  7.33197\n",
      "evaluation/Rewards Min                 -3.08218\n",
      "evaluation/Returns Mean              3068.11\n",
      "evaluation/Returns Std               1554.39\n",
      "evaluation/Returns Max               4576.26\n",
      "evaluation/Returns Min                552.815\n",
      "evaluation/Estimation Bias Mean       741.89\n",
      "evaluation/Estimation Bias Std        185.83\n",
      "evaluation/EB/Q_True Mean              59.8417\n",
      "evaluation/EB/Q_True Std              150.438\n",
      "evaluation/EB/Q_Pred Mean             801.732\n",
      "evaluation/EB/Q_Pred Std               95.9184\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3068.11\n",
      "evaluation/Actions Mean                 0.0475896\n",
      "evaluation/Actions Std                  0.323877\n",
      "evaluation/Actions Max                  0.999987\n",
      "evaluation/Actions Min                 -0.999976\n",
      "time/backward_policy (s)                8.78743\n",
      "time/backward_zf1 (s)                  11.0332\n",
      "time/backward_zf2 (s)                  10.5168\n",
      "time/data sampling (s)                  1.47897\n",
      "time/data storing (s)                   0.0832256\n",
      "time/evaluation sampling (s)            3.64292\n",
      "time/exploration sampling (s)           1.64567\n",
      "time/logging (s)                        0.0127228\n",
      "time/preback_alpha (s)                  0.00534416\n",
      "time/preback_policy (s)                 9.84985\n",
      "time/preback_start (s)                  0.715394\n",
      "time/preback_zf (s)                    29.3431\n",
      "time/saving (s)                         4.101e-06\n",
      "time/training (s)                      10.8593\n",
      "time/epoch (s)                         87.9739\n",
      "time/total (s)                       1296.76\n",
      "Epoch                                  14\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:42:54.164027 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 15 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                  90000\n",
      "trainer/ZF1 Loss                      127.52\n",
      "trainer/ZF2 Loss                      151.899\n",
      "trainer/ZF Expert Reward               12.5417\n",
      "trainer/ZF Policy Reward                2.93902\n",
      "trainer/ZF CHI2 Term                  144.803\n",
      "trainer/Policy Loss                  -601.585\n",
      "trainer/expert_lambda Loss             22.2821\n",
      "trainer/expert_lambda Value            11.1808\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               84.0749\n",
      "trainer/Policy Param Norm              28.8517\n",
      "trainer/Zf1 Grad Norm                6450.99\n",
      "trainer/Zf1 Param Norm                 71.115\n",
      "trainer/Zf2 Grad Norm                7374.37\n",
      "trainer/Zf2 Param Norm                 71.9217\n",
      "trainer/Z Expert Predictions Mean     910.987\n",
      "trainer/Z Expert Predictions Std       62.348\n",
      "trainer/Z Expert Predictions Max     1031.35\n",
      "trainer/Z Expert Predictions Min      674.829\n",
      "trainer/Z Policy Predictions Mean     599.908\n",
      "trainer/Z Policy Predictions Std      251.774\n",
      "trainer/Z Policy Predictions Max      942.885\n",
      "trainer/Z Policy Predictions Min      -33.253\n",
      "trainer/Z Expert Targets Mean         898.446\n",
      "trainer/Z Expert Targets Std           63.0218\n",
      "trainer/Z Expert Targets Max         1023.42\n",
      "trainer/Z Expert Targets Min          651.884\n",
      "trainer/Z Policy Targets Mean         596.969\n",
      "trainer/Z Policy Targets Std          250.147\n",
      "trainer/Z Policy Targets Max          923.883\n",
      "trainer/Z Policy Targets Min          -39.7967\n",
      "trainer/Log Pis Mean                   27.3736\n",
      "trainer/Log Pis Std                     6.54442\n",
      "trainer/Policy mu Mean                  0.24253\n",
      "trainer/Policy mu Std                   1.50177\n",
      "trainer/Policy log std Mean            -3.72567\n",
      "trainer/Policy log std Std              1.38069\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total         85633\n",
      "exploration/num paths total           186\n",
      "evaluation/num steps total         135213\n",
      "evaluation/num paths total            162\n",
      "evaluation/path length Mean           970.7\n",
      "evaluation/path length Std             87.9\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            707\n",
      "evaluation/Rewards Mean                 4.59679\n",
      "evaluation/Rewards Std                  1.09987\n",
      "evaluation/Rewards Max                  7.19541\n",
      "evaluation/Rewards Min                 -2.42922\n",
      "evaluation/Returns Mean              4462.1\n",
      "evaluation/Returns Std                436.18\n",
      "evaluation/Returns Max               4810.98\n",
      "evaluation/Returns Min               3204.76\n",
      "evaluation/Estimation Bias Mean       760.683\n",
      "evaluation/Estimation Bias Std        147.497\n",
      "evaluation/EB/Q_True Mean              41.3734\n",
      "evaluation/EB/Q_True Std              127.214\n",
      "evaluation/EB/Q_Pred Mean             802.056\n",
      "evaluation/EB/Q_Pred Std               76.7858\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4462.1\n",
      "evaluation/Actions Mean                 0.0439664\n",
      "evaluation/Actions Std                  0.315522\n",
      "evaluation/Actions Max                  0.99857\n",
      "evaluation/Actions Min                 -0.999818\n",
      "time/backward_policy (s)                8.49658\n",
      "time/backward_zf1 (s)                  10.4506\n",
      "time/backward_zf2 (s)                  10.1656\n",
      "time/data sampling (s)                  1.34655\n",
      "time/data storing (s)                   0.0791999\n",
      "time/evaluation sampling (s)            3.59675\n",
      "time/exploration sampling (s)           1.57846\n",
      "time/logging (s)                        0.0124255\n",
      "time/preback_alpha (s)                  0.00516435\n",
      "time/preback_policy (s)                 9.43895\n",
      "time/preback_start (s)                  0.663888\n",
      "time/preback_zf (s)                    28.8133\n",
      "time/saving (s)                         3.078e-06\n",
      "time/training (s)                      10.4507\n",
      "time/epoch (s)                         85.0981\n",
      "time/total (s)                       1381.86\n",
      "Epoch                                  15\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:44:19.079510 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 16 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                  95000\n",
      "trainer/ZF1 Loss                       95.9586\n",
      "trainer/ZF2 Loss                       91.9501\n",
      "trainer/ZF Expert Reward                8.60845\n",
      "trainer/ZF Policy Reward               -0.623537\n",
      "trainer/ZF CHI2 Term                   98.8662\n",
      "trainer/Policy Loss                  -565.917\n",
      "trainer/expert_lambda Loss             23.3174\n",
      "trainer/expert_lambda Value            10.9951\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               87.4275\n",
      "trainer/Policy Param Norm              29.1688\n",
      "trainer/Zf1 Grad Norm                6333.47\n",
      "trainer/Zf1 Param Norm                 72.2342\n",
      "trainer/Zf2 Grad Norm                7044.32\n",
      "trainer/Zf2 Param Norm                 73.0196\n",
      "trainer/Z Expert Predictions Mean     878.132\n",
      "trainer/Z Expert Predictions Std       73.4852\n",
      "trainer/Z Expert Predictions Max     1012.33\n",
      "trainer/Z Expert Predictions Min      613.18\n",
      "trainer/Z Policy Predictions Mean     562.957\n",
      "trainer/Z Policy Predictions Std      274.26\n",
      "trainer/Z Policy Predictions Max      925.866\n",
      "trainer/Z Policy Predictions Min      -29.7886\n",
      "trainer/Z Expert Targets Mean         869.524\n",
      "trainer/Z Expert Targets Std           73.5874\n",
      "trainer/Z Expert Targets Max         1007.12\n",
      "trainer/Z Expert Targets Min          590.767\n",
      "trainer/Z Policy Targets Mean         563.58\n",
      "trainer/Z Policy Targets Std          273.519\n",
      "trainer/Z Policy Targets Max          926.269\n",
      "trainer/Z Policy Targets Min          -29.3136\n",
      "trainer/Log Pis Mean                   27.6835\n",
      "trainer/Log Pis Std                     7.12348\n",
      "trainer/Policy mu Mean                  0.246606\n",
      "trainer/Policy mu Std                   1.59758\n",
      "trainer/Policy log std Mean            -3.63759\n",
      "trainer/Policy log std Std              1.54155\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total         89575\n",
      "exploration/num paths total           191\n",
      "evaluation/num steps total         144228\n",
      "evaluation/num paths total            172\n",
      "evaluation/path length Mean           901.5\n",
      "evaluation/path length Std            295.5\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             15\n",
      "evaluation/Rewards Mean                 4.40371\n",
      "evaluation/Rewards Std                  1.04166\n",
      "evaluation/Rewards Max                  7.26788\n",
      "evaluation/Rewards Min                 -1.33529\n",
      "evaluation/Returns Mean              3969.94\n",
      "evaluation/Returns Std               1326.61\n",
      "evaluation/Returns Max               4512.21\n",
      "evaluation/Returns Min                  1.85942\n",
      "evaluation/Estimation Bias Mean       734.577\n",
      "evaluation/Estimation Bias Std        148.74\n",
      "evaluation/EB/Q_True Mean              44.338\n",
      "evaluation/EB/Q_True Std              128.764\n",
      "evaluation/EB/Q_Pred Mean             778.915\n",
      "evaluation/EB/Q_Pred Std               66.356\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3969.94\n",
      "evaluation/Actions Mean                 0.040095\n",
      "evaluation/Actions Std                  0.312302\n",
      "evaluation/Actions Max                  0.985674\n",
      "evaluation/Actions Min                 -0.999887\n",
      "time/backward_policy (s)                8.01393\n",
      "time/backward_zf1 (s)                  10.011\n",
      "time/backward_zf2 (s)                   9.67548\n",
      "time/data sampling (s)                  1.35272\n",
      "time/data storing (s)                   0.0805259\n",
      "time/evaluation sampling (s)            3.90988\n",
      "time/exploration sampling (s)           1.62039\n",
      "time/logging (s)                        0.012375\n",
      "time/preback_alpha (s)                  0.00511211\n",
      "time/preback_policy (s)                 9.84171\n",
      "time/preback_start (s)                  0.668854\n",
      "time/preback_zf (s)                    28.8012\n",
      "time/saving (s)                         2.961e-06\n",
      "time/training (s)                      10.7138\n",
      "time/epoch (s)                         84.707\n",
      "time/total (s)                       1466.57\n",
      "Epoch                                  16\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:45:49.992763 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 17 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 100000\n",
      "trainer/ZF1 Loss                      149.742\n",
      "trainer/ZF2 Loss                      157.701\n",
      "trainer/ZF Expert Reward               10.0005\n",
      "trainer/ZF Policy Reward                3.36447\n",
      "trainer/ZF CHI2 Term                  157.334\n",
      "trainer/Policy Loss                  -603.538\n",
      "trainer/expert_lambda Loss             20.8138\n",
      "trainer/expert_lambda Value            10.8229\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              114.207\n",
      "trainer/Policy Param Norm              29.4412\n",
      "trainer/Zf1 Grad Norm                6503.7\n",
      "trainer/Zf1 Param Norm                 73.1973\n",
      "trainer/Zf2 Grad Norm                6286.64\n",
      "trainer/Zf2 Param Norm                 73.9232\n",
      "trainer/Z Expert Predictions Mean     860.685\n",
      "trainer/Z Expert Predictions Std       74.206\n",
      "trainer/Z Expert Predictions Max      989.491\n",
      "trainer/Z Expert Predictions Min      571.579\n",
      "trainer/Z Policy Predictions Mean     602.164\n",
      "trainer/Z Policy Predictions Std      240.855\n",
      "trainer/Z Policy Predictions Max      921.416\n",
      "trainer/Z Policy Predictions Min      -21.1117\n",
      "trainer/Z Expert Targets Mean         850.684\n",
      "trainer/Z Expert Targets Std           74.4173\n",
      "trainer/Z Expert Targets Max          975.574\n",
      "trainer/Z Expert Targets Min          558.853\n",
      "trainer/Z Policy Targets Mean         598.799\n",
      "trainer/Z Policy Targets Std          238.469\n",
      "trainer/Z Policy Targets Max          888.574\n",
      "trainer/Z Policy Targets Min          -19.4029\n",
      "trainer/Log Pis Mean                   27.2964\n",
      "trainer/Log Pis Std                     6.45134\n",
      "trainer/Policy mu Mean                  0.107274\n",
      "trainer/Policy mu Std                   1.27481\n",
      "trainer/Policy log std Mean            -3.95994\n",
      "trainer/Policy log std Std              1.47442\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total         94575\n",
      "exploration/num paths total           196\n",
      "evaluation/num steps total         153265\n",
      "evaluation/num paths total            182\n",
      "evaluation/path length Mean           903.7\n",
      "evaluation/path length Std            244.863\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            179\n",
      "evaluation/Rewards Mean                 4.27527\n",
      "evaluation/Rewards Std                  1.53354\n",
      "evaluation/Rewards Max                  7.33088\n",
      "evaluation/Rewards Min                 -2.2116\n",
      "evaluation/Returns Mean              3863.56\n",
      "evaluation/Returns Std               1173.31\n",
      "evaluation/Returns Max               4625\n",
      "evaluation/Returns Min                757.426\n",
      "evaluation/Estimation Bias Mean       698.515\n",
      "evaluation/Estimation Bias Std        132.883\n",
      "evaluation/EB/Q_True Mean              27.7993\n",
      "evaluation/EB/Q_True Std              110.167\n",
      "evaluation/EB/Q_Pred Mean             726.314\n",
      "evaluation/EB/Q_Pred Std               80.3093\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3863.56\n",
      "evaluation/Actions Mean                 0.0565365\n",
      "evaluation/Actions Std                  0.347468\n",
      "evaluation/Actions Max                  0.999999\n",
      "evaluation/Actions Min                 -0.999966\n",
      "time/backward_policy (s)                9.01111\n",
      "time/backward_zf1 (s)                  11.4266\n",
      "time/backward_zf2 (s)                  10.8675\n",
      "time/data sampling (s)                  1.59157\n",
      "time/data storing (s)                   0.0867676\n",
      "time/evaluation sampling (s)            3.9212\n",
      "time/exploration sampling (s)           1.71267\n",
      "time/logging (s)                        0.0114896\n",
      "time/preback_alpha (s)                  0.00550685\n",
      "time/preback_policy (s)                10.1624\n",
      "time/preback_start (s)                  0.750978\n",
      "time/preback_zf (s)                    29.9553\n",
      "time/saving (s)                         2.644e-06\n",
      "time/training (s)                      11.1785\n",
      "time/epoch (s)                         90.6815\n",
      "time/total (s)                       1557.25\n",
      "Epoch                                  17\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:47:19.524696 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 18 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 105000\n",
      "trainer/ZF1 Loss                      108.446\n",
      "trainer/ZF2 Loss                      101.577\n",
      "trainer/ZF Expert Reward                7.17715\n",
      "trainer/ZF Policy Reward               -1.01113\n",
      "trainer/ZF CHI2 Term                  109.399\n",
      "trainer/Policy Loss                  -567.872\n",
      "trainer/expert_lambda Loss             21.1606\n",
      "trainer/expert_lambda Value            10.6684\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              116.033\n",
      "trainer/Policy Param Norm              29.6979\n",
      "trainer/Zf1 Grad Norm                6060.68\n",
      "trainer/Zf1 Param Norm                 73.9942\n",
      "trainer/Zf2 Grad Norm                5451.07\n",
      "trainer/Zf2 Param Norm                 74.6488\n",
      "trainer/Z Expert Predictions Mean     814.726\n",
      "trainer/Z Expert Predictions Std       92.9127\n",
      "trainer/Z Expert Predictions Max      968.261\n",
      "trainer/Z Expert Predictions Min       -3.59849\n",
      "trainer/Z Policy Predictions Mean     565.779\n",
      "trainer/Z Policy Predictions Std      234.405\n",
      "trainer/Z Policy Predictions Max      846.919\n",
      "trainer/Z Policy Predictions Min      -16.6374\n",
      "trainer/Z Expert Targets Mean         807.549\n",
      "trainer/Z Expert Targets Std           93.2622\n",
      "trainer/Z Expert Targets Max          964.614\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean         566.79\n",
      "trainer/Z Policy Targets Std          233.083\n",
      "trainer/Z Policy Targets Max          851.195\n",
      "trainer/Z Policy Targets Min          -12.8729\n",
      "trainer/Log Pis Mean                   27.8353\n",
      "trainer/Log Pis Std                     6.35071\n",
      "trainer/Policy mu Mean                  0.138811\n",
      "trainer/Policy mu Std                   1.32322\n",
      "trainer/Policy log std Mean            -3.97669\n",
      "trainer/Policy log std Std              1.35635\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total         99575\n",
      "exploration/num paths total           201\n",
      "evaluation/num steps total         161210\n",
      "evaluation/num paths total            192\n",
      "evaluation/path length Mean           794.5\n",
      "evaluation/path length Std            344.63\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             13\n",
      "evaluation/Rewards Mean                 3.82337\n",
      "evaluation/Rewards Std                  2.20037\n",
      "evaluation/Rewards Max                  7.34382\n",
      "evaluation/Rewards Min                 -3.24348\n",
      "evaluation/Returns Mean              3037.67\n",
      "evaluation/Returns Std               2018.87\n",
      "evaluation/Returns Max               4768.3\n",
      "evaluation/Returns Min               -741.969\n",
      "evaluation/Estimation Bias Mean       585.86\n",
      "evaluation/Estimation Bias Std        255.951\n",
      "evaluation/EB/Q_True Mean              54.538\n",
      "evaluation/EB/Q_True Std              147.89\n",
      "evaluation/EB/Q_Pred Mean             640.398\n",
      "evaluation/EB/Q_Pred Std              221.34\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3037.67\n",
      "evaluation/Actions Mean                 0.0561011\n",
      "evaluation/Actions Std                  0.405376\n",
      "evaluation/Actions Max                  0.999997\n",
      "evaluation/Actions Min                 -1\n",
      "time/backward_policy (s)                8.88153\n",
      "time/backward_zf1 (s)                  11.2158\n",
      "time/backward_zf2 (s)                  10.6918\n",
      "time/data sampling (s)                  1.55414\n",
      "time/data storing (s)                   0.0844263\n",
      "time/evaluation sampling (s)            3.67769\n",
      "time/exploration sampling (s)           1.66217\n",
      "time/logging (s)                        0.0102269\n",
      "time/preback_alpha (s)                  0.00542348\n",
      "time/preback_policy (s)                10.019\n",
      "time/preback_start (s)                  0.733341\n",
      "time/preback_zf (s)                    29.7339\n",
      "time/saving (s)                         2.788e-06\n",
      "time/training (s)                      11.0375\n",
      "time/epoch (s)                         89.3069\n",
      "time/total (s)                       1646.56\n",
      "Epoch                                  18\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:48:49.443077 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 19 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 110000\n",
      "trainer/ZF1 Loss                      138.79\n",
      "trainer/ZF2 Loss                      124.543\n",
      "trainer/ZF Expert Reward               11.9977\n",
      "trainer/ZF Policy Reward                2.7378\n",
      "trainer/ZF CHI2 Term                  136.593\n",
      "trainer/Policy Loss                  -547.673\n",
      "trainer/expert_lambda Loss             18.9695\n",
      "trainer/expert_lambda Value            10.5205\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               93.6178\n",
      "trainer/Policy Param Norm              29.9411\n",
      "trainer/Zf1 Grad Norm                6262.22\n",
      "trainer/Zf1 Param Norm                 74.7287\n",
      "trainer/Zf2 Grad Norm                5409.04\n",
      "trainer/Zf2 Param Norm                 75.2827\n",
      "trainer/Z Expert Predictions Mean     802.512\n",
      "trainer/Z Expert Predictions Std       68.418\n",
      "trainer/Z Expert Predictions Max      927.773\n",
      "trainer/Z Expert Predictions Min      521.978\n",
      "trainer/Z Policy Predictions Mean     548.144\n",
      "trainer/Z Policy Predictions Std      248.222\n",
      "trainer/Z Policy Predictions Max      818.323\n",
      "trainer/Z Policy Predictions Min      -24.7107\n",
      "trainer/Z Expert Targets Mean         790.514\n",
      "trainer/Z Expert Targets Std           68.6104\n",
      "trainer/Z Expert Targets Max          914.415\n",
      "trainer/Z Expert Targets Min          512.092\n",
      "trainer/Z Policy Targets Mean         545.407\n",
      "trainer/Z Policy Targets Std          245.507\n",
      "trainer/Z Policy Targets Max          852.382\n",
      "trainer/Z Policy Targets Min          -14.2693\n",
      "trainer/Log Pis Mean                   28.2675\n",
      "trainer/Log Pis Std                     7.34116\n",
      "trainer/Policy mu Mean                  0.142645\n",
      "trainer/Policy mu Std                   1.35942\n",
      "trainer/Policy log std Mean            -4.07277\n",
      "trainer/Policy log std Std              1.43155\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        105003\n",
      "exploration/num paths total           207\n",
      "evaluation/num steps total         168916\n",
      "evaluation/num paths total            202\n",
      "evaluation/path length Mean           770.6\n",
      "evaluation/path length Std            366.506\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             10\n",
      "evaluation/Rewards Mean                 2.94309\n",
      "evaluation/Rewards Std                  2.89236\n",
      "evaluation/Rewards Max                  7.76183\n",
      "evaluation/Rewards Min                 -4.10785\n",
      "evaluation/Returns Mean              2267.95\n",
      "evaluation/Returns Std               2033.29\n",
      "evaluation/Returns Max               4796.68\n",
      "evaluation/Returns Min              -1010.21\n",
      "evaluation/Estimation Bias Mean       513.622\n",
      "evaluation/Estimation Bias Std        305.842\n",
      "evaluation/EB/Q_True Mean             -15.8287\n",
      "evaluation/EB/Q_True Std               60.704\n",
      "evaluation/EB/Q_Pred Mean             497.793\n",
      "evaluation/EB/Q_Pred Std              292.122\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           2267.95\n",
      "evaluation/Actions Mean                 0.0656067\n",
      "evaluation/Actions Std                  0.494249\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -1\n",
      "time/backward_policy (s)                8.81772\n",
      "time/backward_zf1 (s)                  11.1015\n",
      "time/backward_zf2 (s)                  10.6939\n",
      "time/data sampling (s)                  1.5203\n",
      "time/data storing (s)                   0.0865454\n",
      "time/evaluation sampling (s)            3.92519\n",
      "time/exploration sampling (s)           1.66671\n",
      "time/logging (s)                        0.0104664\n",
      "time/preback_alpha (s)                  0.00535932\n",
      "time/preback_policy (s)                10.2093\n",
      "time/preback_start (s)                  0.727538\n",
      "time/preback_zf (s)                    29.8352\n",
      "time/saving (s)                         2.918e-06\n",
      "time/training (s)                      11.0938\n",
      "time/epoch (s)                         89.6934\n",
      "time/total (s)                       1736.26\n",
      "Epoch                                  19\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:50:17.616360 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 20 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 115000\n",
      "trainer/ZF1 Loss                       76.0251\n",
      "trainer/ZF2 Loss                       92.3611\n",
      "trainer/ZF Expert Reward                6.6224\n",
      "trainer/ZF Policy Reward                0.642198\n",
      "trainer/ZF CHI2 Term                   87.4793\n",
      "trainer/Policy Loss                  -524.026\n",
      "trainer/expert_lambda Loss             19.5091\n",
      "trainer/expert_lambda Value            10.3844\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              101.196\n",
      "trainer/Policy Param Norm              30.1942\n",
      "trainer/Zf1 Grad Norm                4487.96\n",
      "trainer/Zf1 Param Norm                 75.2954\n",
      "trainer/Zf2 Grad Norm                6506.94\n",
      "trainer/Zf2 Param Norm                 75.7705\n",
      "trainer/Z Expert Predictions Mean     754.928\n",
      "trainer/Z Expert Predictions Std       63.2896\n",
      "trainer/Z Expert Predictions Max      872.824\n",
      "trainer/Z Expert Predictions Min      514.624\n",
      "trainer/Z Policy Predictions Mean     524.119\n",
      "trainer/Z Policy Predictions Std      217.023\n",
      "trainer/Z Policy Predictions Max      786.365\n",
      "trainer/Z Policy Predictions Min      -23.1327\n",
      "trainer/Z Expert Targets Mean         748.305\n",
      "trainer/Z Expert Targets Std           63.6646\n",
      "trainer/Z Expert Targets Max          868.795\n",
      "trainer/Z Expert Targets Min          505.469\n",
      "trainer/Z Policy Targets Mean         523.477\n",
      "trainer/Z Policy Targets Std          216.525\n",
      "trainer/Z Policy Targets Max          801.383\n",
      "trainer/Z Policy Targets Min          -19.002\n",
      "trainer/Log Pis Mean                   27.9613\n",
      "trainer/Log Pis Std                     6.47924\n",
      "trainer/Policy mu Mean                  0.133508\n",
      "trainer/Policy mu Std                   1.40167\n",
      "trainer/Policy log std Mean            -4.03349\n",
      "trainer/Policy log std Std              1.43239\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        109003\n",
      "exploration/num paths total           211\n",
      "evaluation/num steps total         178032\n",
      "evaluation/num paths total            212\n",
      "evaluation/path length Mean           911.6\n",
      "evaluation/path length Std            265.2\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            116\n",
      "evaluation/Rewards Mean                 4.28583\n",
      "evaluation/Rewards Std                  1.25899\n",
      "evaluation/Rewards Max                  7.27586\n",
      "evaluation/Rewards Min                 -2.44112\n",
      "evaluation/Returns Mean              3906.96\n",
      "evaluation/Returns Std               1224.18\n",
      "evaluation/Returns Max               4471.73\n",
      "evaluation/Returns Min                253.004\n",
      "evaluation/Estimation Bias Mean       582.883\n",
      "evaluation/Estimation Bias Std        143.663\n",
      "evaluation/EB/Q_True Mean              45.1275\n",
      "evaluation/EB/Q_True Std              132.462\n",
      "evaluation/EB/Q_Pred Mean             628.01\n",
      "evaluation/EB/Q_Pred Std               52.5704\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3906.96\n",
      "evaluation/Actions Mean                 0.0359873\n",
      "evaluation/Actions Std                  0.324581\n",
      "evaluation/Actions Max                  0.99983\n",
      "evaluation/Actions Min                 -0.999988\n",
      "time/backward_policy (s)                8.88264\n",
      "time/backward_zf1 (s)                  11.2626\n",
      "time/backward_zf2 (s)                  10.6607\n",
      "time/data sampling (s)                  1.57878\n",
      "time/data storing (s)                   0.0865985\n",
      "time/evaluation sampling (s)            2.0352\n",
      "time/exploration sampling (s)           1.6899\n",
      "time/logging (s)                        0.0123555\n",
      "time/preback_alpha (s)                  0.00557122\n",
      "time/preback_policy (s)                10.0048\n",
      "time/preback_start (s)                  0.750422\n",
      "time/preback_zf (s)                    29.8781\n",
      "time/saving (s)                         3.282e-06\n",
      "time/training (s)                      11.1019\n",
      "time/epoch (s)                         87.9495\n",
      "time/total (s)                       1824.21\n",
      "Epoch                                  20\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:51:45.356289 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 21 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 120000\n",
      "trainer/ZF1 Loss                      331.25\n",
      "trainer/ZF2 Loss                      322.229\n",
      "trainer/ZF Expert Reward                8.87414\n",
      "trainer/ZF Policy Reward                2.50755\n",
      "trainer/ZF CHI2 Term                  330.217\n",
      "trainer/Policy Loss                  -492.378\n",
      "trainer/expert_lambda Loss             15.4775\n",
      "trainer/expert_lambda Value            10.2467\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               98.49\n",
      "trainer/Policy Param Norm              30.4415\n",
      "trainer/Zf1 Grad Norm                9874.97\n",
      "trainer/Zf1 Param Norm                 75.7859\n",
      "trainer/Zf2 Grad Norm                9333.51\n",
      "trainer/Zf2 Param Norm                 76.1203\n",
      "trainer/Z Expert Predictions Mean     712.705\n",
      "trainer/Z Expert Predictions Std       55.4233\n",
      "trainer/Z Expert Predictions Max      833.79\n",
      "trainer/Z Expert Predictions Min      528.288\n",
      "trainer/Z Policy Predictions Mean     494.359\n",
      "trainer/Z Policy Predictions Std      203.208\n",
      "trainer/Z Policy Predictions Max      768.716\n",
      "trainer/Z Policy Predictions Min      -36.0407\n",
      "trainer/Z Expert Targets Mean         703.831\n",
      "trainer/Z Expert Targets Std           55.9565\n",
      "trainer/Z Expert Targets Max          829.05\n",
      "trainer/Z Expert Targets Min          516.039\n",
      "trainer/Z Policy Targets Mean         491.852\n",
      "trainer/Z Policy Targets Std          204.038\n",
      "trainer/Z Policy Targets Max          746.698\n",
      "trainer/Z Policy Targets Min          -32.4083\n",
      "trainer/Log Pis Mean                   27.6349\n",
      "trainer/Log Pis Std                     7.22098\n",
      "trainer/Policy mu Mean                  0.0839393\n",
      "trainer/Policy mu Std                   1.32325\n",
      "trainer/Policy log std Mean            -4.05696\n",
      "trainer/Policy log std Std              1.42274\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        115366\n",
      "exploration/num paths total           218\n",
      "evaluation/num steps total         186078\n",
      "evaluation/num paths total            222\n",
      "evaluation/path length Mean           804.6\n",
      "evaluation/path length Std            390.801\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             21\n",
      "evaluation/Rewards Mean                 3.00977\n",
      "evaluation/Rewards Std                  2.53835\n",
      "evaluation/Rewards Max                  7.22009\n",
      "evaluation/Rewards Min                 -2.99216\n",
      "evaluation/Returns Mean              2421.66\n",
      "evaluation/Returns Std               2219.94\n",
      "evaluation/Returns Max               4634.69\n",
      "evaluation/Returns Min               -418.095\n",
      "evaluation/Estimation Bias Mean       490.221\n",
      "evaluation/Estimation Bias Std        176.029\n",
      "evaluation/EB/Q_True Mean              50.5312\n",
      "evaluation/EB/Q_True Std              138.887\n",
      "evaluation/EB/Q_Pred Mean             540.752\n",
      "evaluation/EB/Q_Pred Std              121.68\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           2421.66\n",
      "evaluation/Actions Mean                 0.0366445\n",
      "evaluation/Actions Std                  0.433654\n",
      "evaluation/Actions Max                  0.999714\n",
      "evaluation/Actions Min                 -0.999993\n",
      "time/backward_policy (s)                8.95617\n",
      "time/backward_zf1 (s)                  11.1373\n",
      "time/backward_zf2 (s)                  10.6875\n",
      "time/data sampling (s)                  1.50278\n",
      "time/data storing (s)                   0.0856913\n",
      "time/evaluation sampling (s)            2.36749\n",
      "time/exploration sampling (s)           1.66099\n",
      "time/logging (s)                        0.0144718\n",
      "time/preback_alpha (s)                  0.00543526\n",
      "time/preback_policy (s)                 9.94379\n",
      "time/preback_start (s)                  0.717407\n",
      "time/preback_zf (s)                    29.4966\n",
      "time/saving (s)                         4.95e-06\n",
      "time/training (s)                      10.9445\n",
      "time/epoch (s)                         87.5202\n",
      "time/total (s)                       1911.73\n",
      "Epoch                                  21\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:53:10.535683 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 22 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 125000\n",
      "trainer/ZF1 Loss                       61.027\n",
      "trainer/ZF2 Loss                       77.5972\n",
      "trainer/ZF Expert Reward                7.61689\n",
      "trainer/ZF Policy Reward                0.318276\n",
      "trainer/ZF CHI2 Term                   73.2486\n",
      "trainer/Policy Loss                  -489.607\n",
      "trainer/expert_lambda Loss             13.7236\n",
      "trainer/expert_lambda Value            10.1152\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              130.758\n",
      "trainer/Policy Param Norm              30.6815\n",
      "trainer/Zf1 Grad Norm                3060.03\n",
      "trainer/Zf1 Param Norm                 76.2005\n",
      "trainer/Zf2 Grad Norm                7318.73\n",
      "trainer/Zf2 Param Norm                 76.4897\n",
      "trainer/Z Expert Predictions Mean     672.722\n",
      "trainer/Z Expert Predictions Std       59.9504\n",
      "trainer/Z Expert Predictions Max      805.353\n",
      "trainer/Z Expert Predictions Min      494.312\n",
      "trainer/Z Policy Predictions Mean     488.163\n",
      "trainer/Z Policy Predictions Std      172.272\n",
      "trainer/Z Policy Predictions Max      675.129\n",
      "trainer/Z Policy Predictions Min      -40.8501\n",
      "trainer/Z Expert Targets Mean         665.105\n",
      "trainer/Z Expert Targets Std           60.9671\n",
      "trainer/Z Expert Targets Max          802.245\n",
      "trainer/Z Expert Targets Min          481.431\n",
      "trainer/Z Policy Targets Mean         487.844\n",
      "trainer/Z Policy Targets Std          171.623\n",
      "trainer/Z Policy Targets Max          677.959\n",
      "trainer/Z Policy Targets Min          -46.545\n",
      "trainer/Log Pis Mean                   27.5988\n",
      "trainer/Log Pis Std                     6.17768\n",
      "trainer/Policy mu Mean                 -0.0156548\n",
      "trainer/Policy mu Std                   1.1608\n",
      "trainer/Policy log std Mean            -4.17687\n",
      "trainer/Policy log std Std              1.25792\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        119366\n",
      "exploration/num paths total           223\n",
      "evaluation/num steps total         196078\n",
      "evaluation/num paths total            232\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 4.54069\n",
      "evaluation/Rewards Std                  1.16256\n",
      "evaluation/Rewards Max                  7.37155\n",
      "evaluation/Rewards Min                 -2.00478\n",
      "evaluation/Returns Mean              4540.69\n",
      "evaluation/Returns Std                143.267\n",
      "evaluation/Returns Max               4782.93\n",
      "evaluation/Returns Min               4385.8\n",
      "evaluation/Estimation Bias Mean       509.396\n",
      "evaluation/Estimation Bias Std        140.567\n",
      "evaluation/EB/Q_True Mean              42.7302\n",
      "evaluation/EB/Q_True Std              132.533\n",
      "evaluation/EB/Q_Pred Mean             552.126\n",
      "evaluation/EB/Q_Pred Std               50.1428\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4540.69\n",
      "evaluation/Actions Mean                 0.0461466\n",
      "evaluation/Actions Std                  0.318431\n",
      "evaluation/Actions Max                  0.994325\n",
      "evaluation/Actions Min                 -0.999847\n",
      "time/backward_policy (s)                8.09909\n",
      "time/backward_zf1 (s)                  10.1267\n",
      "time/backward_zf2 (s)                   9.7285\n",
      "time/data sampling (s)                  1.42799\n",
      "time/data storing (s)                   0.0810906\n",
      "time/evaluation sampling (s)            3.49389\n",
      "time/exploration sampling (s)           1.59782\n",
      "time/logging (s)                        0.0126265\n",
      "time/preback_alpha (s)                  0.00511363\n",
      "time/preback_policy (s)                 9.96094\n",
      "time/preback_start (s)                  0.692675\n",
      "time/preback_zf (s)                    28.9196\n",
      "time/saving (s)                         2.956e-06\n",
      "time/training (s)                      10.8183\n",
      "time/epoch (s)                         84.9643\n",
      "time/total (s)                       1996.7\n",
      "Epoch                                  22\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:54:38.091577 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 23 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 130000\n",
      "trainer/ZF1 Loss                       74.7104\n",
      "trainer/ZF2 Loss                       70.0846\n",
      "trainer/ZF Expert Reward               13.8775\n",
      "trainer/ZF Policy Reward                2.64305\n",
      "trainer/ZF CHI2 Term                   78.3029\n",
      "trainer/Policy Loss                  -423.341\n",
      "trainer/expert_lambda Loss             15.7677\n",
      "trainer/expert_lambda Value             9.99294\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               81.7927\n",
      "trainer/Policy Param Norm              30.921\n",
      "trainer/Zf1 Grad Norm                4682.96\n",
      "trainer/Zf1 Param Norm                 76.5924\n",
      "trainer/Zf2 Grad Norm                3778.58\n",
      "trainer/Zf2 Param Norm                 76.8153\n",
      "trainer/Z Expert Predictions Mean     637.144\n",
      "trainer/Z Expert Predictions Std       68.201\n",
      "trainer/Z Expert Predictions Max      760.084\n",
      "trainer/Z Expert Predictions Min       14.0396\n",
      "trainer/Z Policy Predictions Mean     425.741\n",
      "trainer/Z Policy Predictions Std      184.433\n",
      "trainer/Z Policy Predictions Max      653.291\n",
      "trainer/Z Policy Predictions Min      -42.6121\n",
      "trainer/Z Expert Targets Mean         623.267\n",
      "trainer/Z Expert Targets Std           67.2894\n",
      "trainer/Z Expert Targets Max          752.363\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean         423.098\n",
      "trainer/Z Policy Targets Std          182.441\n",
      "trainer/Z Policy Targets Max          633.854\n",
      "trainer/Z Policy Targets Min          -44.2838\n",
      "trainer/Log Pis Mean                   26.7955\n",
      "trainer/Log Pis Std                     6.43271\n",
      "trainer/Policy mu Mean                 -0.0147816\n",
      "trainer/Policy mu Std                   1.31999\n",
      "trainer/Policy log std Mean            -3.94328\n",
      "trainer/Policy log std Std              1.37586\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        124366\n",
      "exploration/num paths total           228\n",
      "evaluation/num steps total         204116\n",
      "evaluation/num paths total            243\n",
      "evaluation/path length Mean           730.727\n",
      "evaluation/path length Std            352.437\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             88\n",
      "evaluation/Rewards Mean                 3.01265\n",
      "evaluation/Rewards Std                  2.78546\n",
      "evaluation/Rewards Max                  7.46853\n",
      "evaluation/Rewards Min                 -3.28615\n",
      "evaluation/Returns Mean              2201.43\n",
      "evaluation/Returns Std               1811.12\n",
      "evaluation/Returns Max               4528.34\n",
      "evaluation/Returns Min               -913.316\n",
      "evaluation/Estimation Bias Mean       354.675\n",
      "evaluation/Estimation Bias Std        224.561\n",
      "evaluation/EB/Q_True Mean              32.141\n",
      "evaluation/EB/Q_True Std              119.975\n",
      "evaluation/EB/Q_Pred Mean             386.816\n",
      "evaluation/EB/Q_Pred Std              203.267\n",
      "evaluation/Num Paths                   11\n",
      "evaluation/Average Returns           2201.43\n",
      "evaluation/Actions Mean                -0.00583026\n",
      "evaluation/Actions Std                  0.488249\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -1\n",
      "time/backward_policy (s)                8.80128\n",
      "time/backward_zf1 (s)                  10.9573\n",
      "time/backward_zf2 (s)                  10.5533\n",
      "time/data sampling (s)                  1.43748\n",
      "time/data storing (s)                   0.0826592\n",
      "time/evaluation sampling (s)            3.80631\n",
      "time/exploration sampling (s)           1.6153\n",
      "time/logging (s)                        0.0173104\n",
      "time/preback_alpha (s)                  0.00523184\n",
      "time/preback_policy (s)                 9.56768\n",
      "time/preback_start (s)                  0.731479\n",
      "time/preback_zf (s)                    29.1625\n",
      "time/saving (s)                         4.382e-06\n",
      "time/training (s)                      10.6069\n",
      "time/epoch (s)                         87.3447\n",
      "time/total (s)                       2084.05\n",
      "Epoch                                  23\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:56:04.966526 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 24 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 135000\n",
      "trainer/ZF1 Loss                      245.01\n",
      "trainer/ZF2 Loss                      227.75\n",
      "trainer/ZF Expert Reward                8.21486\n",
      "trainer/ZF Policy Reward                0.0994511\n",
      "trainer/ZF CHI2 Term                  240.721\n",
      "trainer/Policy Loss                  -400.932\n",
      "trainer/expert_lambda Loss             11.388\n",
      "trainer/expert_lambda Value             9.89337\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               88.9875\n",
      "trainer/Policy Param Norm              31.1596\n",
      "trainer/Zf1 Grad Norm                4824.71\n",
      "trainer/Zf1 Param Norm                 76.9043\n",
      "trainer/Zf2 Grad Norm                5940\n",
      "trainer/Zf2 Param Norm                 77.0677\n",
      "trainer/Z Expert Predictions Mean     584.936\n",
      "trainer/Z Expert Predictions Std       60.7932\n",
      "trainer/Z Expert Predictions Max      712.85\n",
      "trainer/Z Expert Predictions Min      188.314\n",
      "trainer/Z Policy Predictions Mean     398.971\n",
      "trainer/Z Policy Predictions Std      158.602\n",
      "trainer/Z Policy Predictions Max      642.718\n",
      "trainer/Z Policy Predictions Min      -55.7351\n",
      "trainer/Z Expert Targets Mean         576.721\n",
      "trainer/Z Expert Targets Std           60.4851\n",
      "trainer/Z Expert Targets Max          707.023\n",
      "trainer/Z Expert Targets Min          182.606\n",
      "trainer/Z Policy Targets Mean         398.872\n",
      "trainer/Z Policy Targets Std          159.777\n",
      "trainer/Z Policy Targets Max          642.699\n",
      "trainer/Z Policy Targets Min          -58.3217\n",
      "trainer/Log Pis Mean                   26.7101\n",
      "trainer/Log Pis Std                     6.07344\n",
      "trainer/Policy mu Mean                 -0.0515681\n",
      "trainer/Policy mu Std                   1.30795\n",
      "trainer/Policy log std Mean            -3.93458\n",
      "trainer/Policy log std Std              1.39164\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        130161\n",
      "exploration/num paths total           235\n",
      "evaluation/num steps total         214116\n",
      "evaluation/num paths total            253\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 4.19958\n",
      "evaluation/Rewards Std                  1.69205\n",
      "evaluation/Rewards Max                  7.13984\n",
      "evaluation/Rewards Min                 -3.52622\n",
      "evaluation/Returns Mean              4199.58\n",
      "evaluation/Returns Std               1014.67\n",
      "evaluation/Returns Max               4693.1\n",
      "evaluation/Returns Min               1179.47\n",
      "evaluation/Estimation Bias Mean       405.851\n",
      "evaluation/Estimation Bias Std        165.885\n",
      "evaluation/EB/Q_True Mean              41.6803\n",
      "evaluation/EB/Q_True Std              128.465\n",
      "evaluation/EB/Q_Pred Mean             447.531\n",
      "evaluation/EB/Q_Pred Std              117.286\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4199.58\n",
      "evaluation/Actions Mean                 0.0512333\n",
      "evaluation/Actions Std                  0.354981\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999998\n",
      "time/backward_policy (s)                8.29481\n",
      "time/backward_zf1 (s)                  10.383\n",
      "time/backward_zf2 (s)                   9.97285\n",
      "time/data sampling (s)                  1.44443\n",
      "time/data storing (s)                   0.0838225\n",
      "time/evaluation sampling (s)            4.25071\n",
      "time/exploration sampling (s)           1.64343\n",
      "time/logging (s)                        0.0159952\n",
      "time/preback_alpha (s)                  0.0051576\n",
      "time/preback_policy (s)                 9.93169\n",
      "time/preback_start (s)                  0.696532\n",
      "time/preback_zf (s)                    29.0246\n",
      "time/saving (s)                         3.246e-06\n",
      "time/training (s)                      10.9126\n",
      "time/epoch (s)                         86.6597\n",
      "time/total (s)                       2170.71\n",
      "Epoch                                  24\n",
      "---------------------------------  --------------\n",
      "2024-11-07 17:57:32.129480 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 25 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 140000\n",
      "trainer/ZF1 Loss                       77.6986\n",
      "trainer/ZF2 Loss                       64.7453\n",
      "trainer/ZF Expert Reward               15.031\n",
      "trainer/ZF Policy Reward                4.03043\n",
      "trainer/ZF CHI2 Term                   77.008\n",
      "trainer/Policy Loss                  -376.408\n",
      "trainer/expert_lambda Loss             22.5568\n",
      "trainer/expert_lambda Value             9.8109\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               87.216\n",
      "trainer/Policy Param Norm              31.3696\n",
      "trainer/Zf1 Grad Norm                6269.33\n",
      "trainer/Zf1 Param Norm                 77.1683\n",
      "trainer/Zf2 Grad Norm                3588.43\n",
      "trainer/Zf2 Param Norm                 77.2985\n",
      "trainer/Z Expert Predictions Mean     545.166\n",
      "trainer/Z Expert Predictions Std       57.2615\n",
      "trainer/Z Expert Predictions Max      659.907\n",
      "trainer/Z Expert Predictions Min      387.058\n",
      "trainer/Z Policy Predictions Mean     378.337\n",
      "trainer/Z Policy Predictions Std      149.895\n",
      "trainer/Z Policy Predictions Max      580.651\n",
      "trainer/Z Policy Predictions Min      -56.2208\n",
      "trainer/Z Expert Targets Mean         530.135\n",
      "trainer/Z Expert Targets Std           56.3436\n",
      "trainer/Z Expert Targets Max          645.038\n",
      "trainer/Z Expert Targets Min          369.698\n",
      "trainer/Z Policy Targets Mean         374.307\n",
      "trainer/Z Policy Targets Std          147.224\n",
      "trainer/Z Policy Targets Max          570.586\n",
      "trainer/Z Policy Targets Min          -56.5527\n",
      "trainer/Log Pis Mean                   26.9123\n",
      "trainer/Log Pis Std                     6.10976\n",
      "trainer/Policy mu Mean                 -0.051167\n",
      "trainer/Policy mu Std                   1.17925\n",
      "trainer/Policy log std Mean            -4.09027\n",
      "trainer/Policy log std Std              1.35619\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        134161\n",
      "exploration/num paths total           239\n",
      "evaluation/num steps total         224116\n",
      "evaluation/num paths total            263\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 4.53131\n",
      "evaluation/Rewards Std                  1.00998\n",
      "evaluation/Rewards Max                  7.04159\n",
      "evaluation/Rewards Min                 -1.75838\n",
      "evaluation/Returns Mean              4531.31\n",
      "evaluation/Returns Std                 68.2049\n",
      "evaluation/Returns Max               4674.52\n",
      "evaluation/Returns Min               4407.34\n",
      "evaluation/Estimation Bias Mean       392.582\n",
      "evaluation/Estimation Bias Std        133.865\n",
      "evaluation/EB/Q_True Mean              42.0639\n",
      "evaluation/EB/Q_True Std              130.107\n",
      "evaluation/EB/Q_Pred Mean             434.646\n",
      "evaluation/EB/Q_Pred Std               37.7607\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4531.31\n",
      "evaluation/Actions Mean                 0.0399277\n",
      "evaluation/Actions Std                  0.314627\n",
      "evaluation/Actions Max                  0.996026\n",
      "evaluation/Actions Min                 -0.999834\n",
      "time/backward_policy (s)                8.44765\n",
      "time/backward_zf1 (s)                  10.5753\n",
      "time/backward_zf2 (s)                  10.1908\n",
      "time/data sampling (s)                  1.42118\n",
      "time/data storing (s)                   0.0817098\n",
      "time/evaluation sampling (s)            4.11318\n",
      "time/exploration sampling (s)           1.63683\n",
      "time/logging (s)                        0.0274162\n",
      "time/preback_alpha (s)                  0.00541355\n",
      "time/preback_policy (s)                 9.86699\n",
      "time/preback_start (s)                  0.715717\n",
      "time/preback_zf (s)                    29.1114\n",
      "time/saving (s)                         6.49e-06\n",
      "time/training (s)                      10.7647\n",
      "time/epoch (s)                         86.9583\n",
      "time/total (s)                       2257.67\n",
      "Epoch                                  25\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 17:58:59.505954 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 26 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 145000\n",
      "trainer/ZF1 Loss                       79.9505\n",
      "trainer/ZF2 Loss                      124.471\n",
      "trainer/ZF Expert Reward               13.6896\n",
      "trainer/ZF Policy Reward                4.26604\n",
      "trainer/ZF CHI2 Term                  107.199\n",
      "trainer/Policy Loss                  -345.055\n",
      "trainer/expert_lambda Loss             14.5801\n",
      "trainer/expert_lambda Value             9.74432\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               84.289\n",
      "trainer/Policy Param Norm              31.5482\n",
      "trainer/Zf1 Grad Norm                3869.34\n",
      "trainer/Zf1 Param Norm                 77.4151\n",
      "trainer/Zf2 Grad Norm                4991.74\n",
      "trainer/Zf2 Param Norm                 77.5144\n",
      "trainer/Z Expert Predictions Mean     502.534\n",
      "trainer/Z Expert Predictions Std       53.1515\n",
      "trainer/Z Expert Predictions Max      617.087\n",
      "trainer/Z Expert Predictions Min      341.162\n",
      "trainer/Z Policy Predictions Mean     345.788\n",
      "trainer/Z Policy Predictions Std      134.25\n",
      "trainer/Z Policy Predictions Max      624.226\n",
      "trainer/Z Policy Predictions Min      -31.2152\n",
      "trainer/Z Expert Targets Mean         488.844\n",
      "trainer/Z Expert Targets Std           52.5754\n",
      "trainer/Z Expert Targets Max          593.824\n",
      "trainer/Z Expert Targets Min          330.577\n",
      "trainer/Z Policy Targets Mean         341.522\n",
      "trainer/Z Policy Targets Std          133.83\n",
      "trainer/Z Policy Targets Max          543.964\n",
      "trainer/Z Policy Targets Min          -28.6447\n",
      "trainer/Log Pis Mean                   26.0015\n",
      "trainer/Log Pis Std                     6.03645\n",
      "trainer/Policy mu Mean                 -0.0866502\n",
      "trainer/Policy mu Std                   1.11176\n",
      "trainer/Policy log std Mean            -4.02586\n",
      "trainer/Policy log std Std              1.29394\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        140380\n",
      "exploration/num paths total           248\n",
      "evaluation/num steps total         232628\n",
      "evaluation/num paths total            273\n",
      "evaluation/path length Mean           851.2\n",
      "evaluation/path length Std            308.767\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             72\n",
      "evaluation/Rewards Mean                 4.44253\n",
      "evaluation/Rewards Std                  1.24918\n",
      "evaluation/Rewards Max                  7.49583\n",
      "evaluation/Rewards Min                 -2.27803\n",
      "evaluation/Returns Mean              3781.48\n",
      "evaluation/Returns Std               1430.04\n",
      "evaluation/Returns Max               4539.28\n",
      "evaluation/Returns Min                138.1\n",
      "evaluation/Estimation Bias Mean       334.905\n",
      "evaluation/Estimation Bias Std        145.934\n",
      "evaluation/EB/Q_True Mean              48.8906\n",
      "evaluation/EB/Q_True Std              138.309\n",
      "evaluation/EB/Q_Pred Mean             383.795\n",
      "evaluation/EB/Q_Pred Std               39.3321\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3781.48\n",
      "evaluation/Actions Mean                 0.0484468\n",
      "evaluation/Actions Std                  0.316482\n",
      "evaluation/Actions Max                  0.994919\n",
      "evaluation/Actions Min                 -0.999862\n",
      "time/backward_policy (s)                8.5362\n",
      "time/backward_zf1 (s)                  10.7246\n",
      "time/backward_zf2 (s)                  10.3098\n",
      "time/data sampling (s)                  1.4452\n",
      "time/data storing (s)                   0.0879594\n",
      "time/evaluation sampling (s)            3.73252\n",
      "time/exploration sampling (s)           1.66179\n",
      "time/logging (s)                        0.01125\n",
      "time/preback_alpha (s)                  0.00523803\n",
      "time/preback_policy (s)                 9.85746\n",
      "time/preback_start (s)                  0.706501\n",
      "time/preback_zf (s)                    29.2437\n",
      "time/saving (s)                         3.736e-06\n",
      "time/training (s)                      10.8194\n",
      "time/epoch (s)                         87.1415\n",
      "time/total (s)                       2344.81\n",
      "Epoch                                  26\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:00:25.536112 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 27 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 150000\n",
      "trainer/ZF1 Loss                       46.6943\n",
      "trainer/ZF2 Loss                       67.5468\n",
      "trainer/ZF Expert Reward                8.8731\n",
      "trainer/ZF Policy Reward                0.859373\n",
      "trainer/ZF CHI2 Term                   61.4029\n",
      "trainer/Policy Loss                  -310.863\n",
      "trainer/expert_lambda Loss              9.01246\n",
      "trainer/expert_lambda Value             9.68466\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               93.3353\n",
      "trainer/Policy Param Norm              31.7355\n",
      "trainer/Zf1 Grad Norm                2101.75\n",
      "trainer/Zf1 Param Norm                 77.6175\n",
      "trainer/Zf2 Grad Norm                4017.66\n",
      "trainer/Zf2 Param Norm                 77.731\n",
      "trainer/Z Expert Predictions Mean     469.587\n",
      "trainer/Z Expert Predictions Std       54.2876\n",
      "trainer/Z Expert Predictions Max      583.595\n",
      "trainer/Z Expert Predictions Min        8.54415\n",
      "trainer/Z Policy Predictions Mean     310.952\n",
      "trainer/Z Policy Predictions Std      126.454\n",
      "trainer/Z Policy Predictions Max      499.545\n",
      "trainer/Z Policy Predictions Min      -42.3498\n",
      "trainer/Z Expert Targets Mean         460.714\n",
      "trainer/Z Expert Targets Std           54.8929\n",
      "trainer/Z Expert Targets Max          568.167\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean         310.092\n",
      "trainer/Z Policy Targets Std          125.109\n",
      "trainer/Z Policy Targets Max          490.88\n",
      "trainer/Z Policy Targets Min          -39.6576\n",
      "trainer/Log Pis Mean                   25.7516\n",
      "trainer/Log Pis Std                     5.84905\n",
      "trainer/Policy mu Mean                 -0.00516663\n",
      "trainer/Policy mu Std                   0.976712\n",
      "trainer/Policy log std Mean            -4.01814\n",
      "trainer/Policy log std Std              1.27993\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        144380\n",
      "exploration/num paths total           252\n",
      "evaluation/num steps total         242308\n",
      "evaluation/num paths total            283\n",
      "evaluation/path length Mean           968\n",
      "evaluation/path length Std             96\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            680\n",
      "evaluation/Rewards Mean                 4.5301\n",
      "evaluation/Rewards Std                  1.02561\n",
      "evaluation/Rewards Max                  7.04901\n",
      "evaluation/Rewards Min                 -1.7255\n",
      "evaluation/Returns Mean              4385.13\n",
      "evaluation/Returns Std                467.418\n",
      "evaluation/Returns Max               4715.19\n",
      "evaluation/Returns Min               3057.14\n",
      "evaluation/Estimation Bias Mean       314.493\n",
      "evaluation/Estimation Bias Std        140.413\n",
      "evaluation/EB/Q_True Mean              44.288\n",
      "evaluation/EB/Q_True Std              133.47\n",
      "evaluation/EB/Q_Pred Mean             358.781\n",
      "evaluation/EB/Q_Pred Std               33.6534\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4385.13\n",
      "evaluation/Actions Mean                 0.0397591\n",
      "evaluation/Actions Std                  0.31019\n",
      "evaluation/Actions Max                  0.997115\n",
      "evaluation/Actions Min                 -0.999835\n",
      "time/backward_policy (s)                8.44172\n",
      "time/backward_zf1 (s)                  10.4374\n",
      "time/backward_zf2 (s)                  10.1118\n",
      "time/data sampling (s)                  1.38702\n",
      "time/data storing (s)                   0.0795044\n",
      "time/evaluation sampling (s)            3.78268\n",
      "time/exploration sampling (s)           1.60087\n",
      "time/logging (s)                        0.0154226\n",
      "time/preback_alpha (s)                  0.0051616\n",
      "time/preback_policy (s)                 9.66568\n",
      "time/preback_start (s)                  0.67562\n",
      "time/preback_zf (s)                    28.926\n",
      "time/saving (s)                         2.942e-06\n",
      "time/training (s)                      10.6942\n",
      "time/epoch (s)                         85.8231\n",
      "time/total (s)                       2430.64\n",
      "Epoch                                  27\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:01:50.646346 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 28 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 155000\n",
      "trainer/ZF1 Loss                       71.3673\n",
      "trainer/ZF2 Loss                       59.3487\n",
      "trainer/ZF Expert Reward                9.10269\n",
      "trainer/ZF Policy Reward                0.673746\n",
      "trainer/ZF CHI2 Term                   69.8458\n",
      "trainer/Policy Loss                  -296.779\n",
      "trainer/expert_lambda Loss             11.647\n",
      "trainer/expert_lambda Value             9.63358\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               85.9055\n",
      "trainer/Policy Param Norm              31.9182\n",
      "trainer/Zf1 Grad Norm                7164.19\n",
      "trainer/Zf1 Param Norm                 77.8476\n",
      "trainer/Zf2 Grad Norm                4921.77\n",
      "trainer/Zf2 Param Norm                 77.9238\n",
      "trainer/Z Expert Predictions Mean     437.079\n",
      "trainer/Z Expert Predictions Std       48.7191\n",
      "trainer/Z Expert Predictions Max      523.715\n",
      "trainer/Z Expert Predictions Min       89.0865\n",
      "trainer/Z Policy Predictions Mean     296.477\n",
      "trainer/Z Policy Predictions Std      111.159\n",
      "trainer/Z Policy Predictions Max      459.202\n",
      "trainer/Z Policy Predictions Min      -43.8838\n",
      "trainer/Z Expert Targets Mean         427.977\n",
      "trainer/Z Expert Targets Std           48.2133\n",
      "trainer/Z Expert Targets Max          525.437\n",
      "trainer/Z Expert Targets Min           91.8072\n",
      "trainer/Z Policy Targets Mean         295.804\n",
      "trainer/Z Policy Targets Std          109.458\n",
      "trainer/Z Policy Targets Max          453.546\n",
      "trainer/Z Policy Targets Min          -41.663\n",
      "trainer/Log Pis Mean                   25.4498\n",
      "trainer/Log Pis Std                     5.84486\n",
      "trainer/Policy mu Mean                 -0.0268371\n",
      "trainer/Policy mu Std                   0.944251\n",
      "trainer/Policy log std Mean            -4.06908\n",
      "trainer/Policy log std Std              1.23846\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        150482\n",
      "exploration/num paths total           259\n",
      "evaluation/num steps total         252308\n",
      "evaluation/num paths total            293\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 4.57138\n",
      "evaluation/Rewards Std                  1.16909\n",
      "evaluation/Rewards Max                  7.58732\n",
      "evaluation/Rewards Min                 -2.01489\n",
      "evaluation/Returns Mean              4571.38\n",
      "evaluation/Returns Std                152.582\n",
      "evaluation/Returns Max               4840.64\n",
      "evaluation/Returns Min               4285.3\n",
      "evaluation/Estimation Bias Mean       290.264\n",
      "evaluation/Estimation Bias Std        135.228\n",
      "evaluation/EB/Q_True Mean              42.989\n",
      "evaluation/EB/Q_True Std              132.252\n",
      "evaluation/EB/Q_Pred Mean             333.253\n",
      "evaluation/EB/Q_Pred Std               33.3462\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4571.38\n",
      "evaluation/Actions Mean                 0.0442402\n",
      "evaluation/Actions Std                  0.312791\n",
      "evaluation/Actions Max                  0.998169\n",
      "evaluation/Actions Min                 -0.999743\n",
      "time/backward_policy (s)                8.17693\n",
      "time/backward_zf1 (s)                  10.1562\n",
      "time/backward_zf2 (s)                   9.83349\n",
      "time/data sampling (s)                  1.39128\n",
      "time/data storing (s)                   0.0797945\n",
      "time/evaluation sampling (s)            3.81812\n",
      "time/exploration sampling (s)           1.59878\n",
      "time/logging (s)                        0.0129817\n",
      "time/preback_alpha (s)                  0.00507269\n",
      "time/preback_policy (s)                 9.71281\n",
      "time/preback_start (s)                  0.671022\n",
      "time/preback_zf (s)                    28.8046\n",
      "time/saving (s)                         3.416e-06\n",
      "time/training (s)                      10.6384\n",
      "time/epoch (s)                         84.8995\n",
      "time/total (s)                       2515.54\n",
      "Epoch                                  28\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:03:15.551408 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 29 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 160000\n",
      "trainer/ZF1 Loss                       35.77\n",
      "trainer/ZF2 Loss                       36.1981\n",
      "trainer/ZF Expert Reward                8.40725\n",
      "trainer/ZF Policy Reward                0.335989\n",
      "trainer/ZF CHI2 Term                   40.2957\n",
      "trainer/Policy Loss                  -272.775\n",
      "trainer/expert_lambda Loss              7.11578\n",
      "trainer/expert_lambda Value             9.59757\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               78.0264\n",
      "trainer/Policy Param Norm              32.0877\n",
      "trainer/Zf1 Grad Norm                2821.26\n",
      "trainer/Zf1 Param Norm                 78.0458\n",
      "trainer/Zf2 Grad Norm                1706.06\n",
      "trainer/Zf2 Param Norm                 78.0809\n",
      "trainer/Z Expert Predictions Mean     406.124\n",
      "trainer/Z Expert Predictions Std       43.2052\n",
      "trainer/Z Expert Predictions Max      522.275\n",
      "trainer/Z Expert Predictions Min      287.925\n",
      "trainer/Z Policy Predictions Mean     272.75\n",
      "trainer/Z Policy Predictions Std      101.026\n",
      "trainer/Z Policy Predictions Max      415.683\n",
      "trainer/Z Policy Predictions Min      -44.4758\n",
      "trainer/Z Expert Targets Mean         397.717\n",
      "trainer/Z Expert Targets Std           43.7164\n",
      "trainer/Z Expert Targets Max          509.73\n",
      "trainer/Z Expert Targets Min          275.364\n",
      "trainer/Z Policy Targets Mean         272.413\n",
      "trainer/Z Policy Targets Std           99.0114\n",
      "trainer/Z Policy Targets Max          399.941\n",
      "trainer/Z Policy Targets Min          -43.8856\n",
      "trainer/Log Pis Mean                   25.666\n",
      "trainer/Log Pis Std                     5.90909\n",
      "trainer/Policy mu Mean                  0.0242959\n",
      "trainer/Policy mu Std                   0.904848\n",
      "trainer/Policy log std Mean            -4.13647\n",
      "trainer/Policy log std Std              1.16309\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        154482\n",
      "exploration/num paths total           263\n",
      "evaluation/num steps total         261372\n",
      "evaluation/num paths total            303\n",
      "evaluation/path length Mean           906.4\n",
      "evaluation/path length Std            280.8\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             64\n",
      "evaluation/Rewards Mean                 4.50568\n",
      "evaluation/Rewards Std                  1.05963\n",
      "evaluation/Rewards Max                  7.1592\n",
      "evaluation/Rewards Min                 -1.4728\n",
      "evaluation/Returns Mean              4083.95\n",
      "evaluation/Returns Std               1357.38\n",
      "evaluation/Returns Max               4689.28\n",
      "evaluation/Returns Min                 25.8682\n",
      "evaluation/Estimation Bias Mean       267.462\n",
      "evaluation/Estimation Bias Std        139.792\n",
      "evaluation/EB/Q_True Mean              46.0141\n",
      "evaluation/EB/Q_True Std              134.853\n",
      "evaluation/EB/Q_Pred Mean             313.476\n",
      "evaluation/EB/Q_Pred Std               30.1687\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4083.95\n",
      "evaluation/Actions Mean                 0.0463537\n",
      "evaluation/Actions Std                  0.310663\n",
      "evaluation/Actions Max                  0.999114\n",
      "evaluation/Actions Min                 -0.999153\n",
      "time/backward_policy (s)                8.10482\n",
      "time/backward_zf1 (s)                  10.1063\n",
      "time/backward_zf2 (s)                   9.76961\n",
      "time/data sampling (s)                  1.33944\n",
      "time/data storing (s)                   0.079467\n",
      "time/evaluation sampling (s)            3.67936\n",
      "time/exploration sampling (s)           1.57914\n",
      "time/logging (s)                        0.0149079\n",
      "time/preback_alpha (s)                  0.00515146\n",
      "time/preback_policy (s)                 9.78738\n",
      "time/preback_start (s)                  0.686623\n",
      "time/preback_zf (s)                    28.8178\n",
      "time/saving (s)                         3.041e-06\n",
      "time/training (s)                      10.7274\n",
      "time/epoch (s)                         84.6974\n",
      "time/total (s)                       2600.24\n",
      "Epoch                                  29\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:04:40.745850 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 30 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 165000\n",
      "trainer/ZF1 Loss                       43.9882\n",
      "trainer/ZF2 Loss                       42.4954\n",
      "trainer/ZF Expert Reward               10.8406\n",
      "trainer/ZF Policy Reward                2.58088\n",
      "trainer/ZF CHI2 Term                   47.6458\n",
      "trainer/Policy Loss                  -244.068\n",
      "trainer/expert_lambda Loss              6.40858\n",
      "trainer/expert_lambda Value             9.56667\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               70.5548\n",
      "trainer/Policy Param Norm              32.2405\n",
      "trainer/Zf1 Grad Norm                1808.98\n",
      "trainer/Zf1 Param Norm                 78.2076\n",
      "trainer/Zf2 Grad Norm                2181.51\n",
      "trainer/Zf2 Param Norm                 78.2811\n",
      "trainer/Z Expert Predictions Mean     385.735\n",
      "trainer/Z Expert Predictions Std       39.4425\n",
      "trainer/Z Expert Predictions Max      479.006\n",
      "trainer/Z Expert Predictions Min      289.232\n",
      "trainer/Z Policy Predictions Mean     243.975\n",
      "trainer/Z Policy Predictions Std      110.411\n",
      "trainer/Z Policy Predictions Max      386.54\n",
      "trainer/Z Policy Predictions Min      -74.7374\n",
      "trainer/Z Expert Targets Mean         374.895\n",
      "trainer/Z Expert Targets Std           39.3809\n",
      "trainer/Z Expert Targets Max          471.065\n",
      "trainer/Z Expert Targets Min          280.505\n",
      "trainer/Z Policy Targets Mean         241.394\n",
      "trainer/Z Policy Targets Std          108.977\n",
      "trainer/Z Policy Targets Max          379.774\n",
      "trainer/Z Policy Targets Min          -71.837\n",
      "trainer/Log Pis Mean                   25.4768\n",
      "trainer/Log Pis Std                     6.08725\n",
      "trainer/Policy mu Mean                  0.0104407\n",
      "trainer/Policy mu Std                   0.945302\n",
      "trainer/Policy log std Mean            -4.08373\n",
      "trainer/Policy log std Std              1.26984\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        160482\n",
      "exploration/num paths total           269\n",
      "evaluation/num steps total         271317\n",
      "evaluation/num paths total            314\n",
      "evaluation/path length Mean           904.091\n",
      "evaluation/path length Std            235.575\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            194\n",
      "evaluation/Rewards Mean                 4.12011\n",
      "evaluation/Rewards Std                  1.62696\n",
      "evaluation/Rewards Max                  7.56147\n",
      "evaluation/Rewards Min                 -2.56192\n",
      "evaluation/Returns Mean              3724.95\n",
      "evaluation/Returns Std               1003.72\n",
      "evaluation/Returns Max               4588.6\n",
      "evaluation/Returns Min                821.882\n",
      "evaluation/Estimation Bias Mean       241.057\n",
      "evaluation/Estimation Bias Std        115.35\n",
      "evaluation/EB/Q_True Mean              36.5\n",
      "evaluation/EB/Q_True Std              112.708\n",
      "evaluation/EB/Q_Pred Mean             277.557\n",
      "evaluation/EB/Q_Pred Std               40.0914\n",
      "evaluation/Num Paths                   11\n",
      "evaluation/Average Returns           3724.95\n",
      "evaluation/Actions Mean                 0.0496925\n",
      "evaluation/Actions Std                  0.340942\n",
      "evaluation/Actions Max                  0.997818\n",
      "evaluation/Actions Min                 -0.999957\n",
      "time/backward_policy (s)                8.36028\n",
      "time/backward_zf1 (s)                  10.3094\n",
      "time/backward_zf2 (s)                  10.006\n",
      "time/data sampling (s)                  1.38755\n",
      "time/data storing (s)                   0.0797098\n",
      "time/evaluation sampling (s)            3.72476\n",
      "time/exploration sampling (s)           1.60538\n",
      "time/logging (s)                        0.0204285\n",
      "time/preback_alpha (s)                  0.00506436\n",
      "time/preback_policy (s)                 9.51029\n",
      "time/preback_start (s)                  0.668598\n",
      "time/preback_zf (s)                    28.8375\n",
      "time/saving (s)                         5.628e-06\n",
      "time/training (s)                      10.4753\n",
      "time/epoch (s)                         84.9902\n",
      "time/total (s)                       2685.23\n",
      "Epoch                                  30\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:06:05.807051 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 31 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 170000\n",
      "trainer/ZF1 Loss                       47.4543\n",
      "trainer/ZF2 Loss                       40.2416\n",
      "trainer/ZF Expert Reward               11.7306\n",
      "trainer/ZF Policy Reward                2.17255\n",
      "trainer/ZF CHI2 Term                   48.9035\n",
      "trainer/Policy Loss                  -226.151\n",
      "trainer/expert_lambda Loss              8.0762\n",
      "trainer/expert_lambda Value             9.53748\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               79.9196\n",
      "trainer/Policy Param Norm              32.404\n",
      "trainer/Zf1 Grad Norm                2348.02\n",
      "trainer/Zf1 Param Norm                 78.3805\n",
      "trainer/Zf2 Grad Norm                2087.55\n",
      "trainer/Zf2 Param Norm                 78.4337\n",
      "trainer/Z Expert Predictions Mean     361.964\n",
      "trainer/Z Expert Predictions Std       38.538\n",
      "trainer/Z Expert Predictions Max      453.365\n",
      "trainer/Z Expert Predictions Min      257.629\n",
      "trainer/Z Policy Predictions Mean     230.002\n",
      "trainer/Z Policy Predictions Std      106.108\n",
      "trainer/Z Policy Predictions Max      366.962\n",
      "trainer/Z Policy Predictions Min     -101.101\n",
      "trainer/Z Expert Targets Mean         350.234\n",
      "trainer/Z Expert Targets Std           38.2161\n",
      "trainer/Z Expert Targets Max          447.096\n",
      "trainer/Z Expert Targets Min          250.338\n",
      "trainer/Z Policy Targets Mean         227.829\n",
      "trainer/Z Policy Targets Std          104.766\n",
      "trainer/Z Policy Targets Max          362.65\n",
      "trainer/Z Policy Targets Min          -93.9197\n",
      "trainer/Log Pis Mean                   26.0341\n",
      "trainer/Log Pis Std                     5.79591\n",
      "trainer/Policy mu Mean                 -0.00677876\n",
      "trainer/Policy mu Std                   0.939052\n",
      "trainer/Policy log std Mean            -4.17436\n",
      "trainer/Policy log std Std              1.23425\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        165093\n",
      "exploration/num paths total           274\n",
      "evaluation/num steps total         278842\n",
      "evaluation/num paths total            325\n",
      "evaluation/path length Mean           684.091\n",
      "evaluation/path length Std            425.249\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             14\n",
      "evaluation/Rewards Mean                 4.34534\n",
      "evaluation/Rewards Std                  1.35264\n",
      "evaluation/Rewards Max                  7.6063\n",
      "evaluation/Rewards Min                 -2.49847\n",
      "evaluation/Returns Mean              2972.61\n",
      "evaluation/Returns Std               1887.51\n",
      "evaluation/Returns Max               4686.3\n",
      "evaluation/Returns Min                 20.8376\n",
      "evaluation/Estimation Bias Mean       206.871\n",
      "evaluation/Estimation Bias Std        146.346\n",
      "evaluation/EB/Q_True Mean              53.2912\n",
      "evaluation/EB/Q_True Std              141.122\n",
      "evaluation/EB/Q_Pred Mean             260.162\n",
      "evaluation/EB/Q_Pred Std               32.4795\n",
      "evaluation/Num Paths                   11\n",
      "evaluation/Average Returns           2972.61\n",
      "evaluation/Actions Mean                 0.0446821\n",
      "evaluation/Actions Std                  0.322249\n",
      "evaluation/Actions Max                  0.999177\n",
      "evaluation/Actions Min                 -0.999247\n",
      "time/backward_policy (s)                8.40491\n",
      "time/backward_zf1 (s)                  10.3876\n",
      "time/backward_zf2 (s)                  10.1007\n",
      "time/data sampling (s)                  1.32671\n",
      "time/data storing (s)                   0.0798195\n",
      "time/evaluation sampling (s)            3.49249\n",
      "time/exploration sampling (s)           1.5774\n",
      "time/logging (s)                        0.0117573\n",
      "time/preback_alpha (s)                  0.00506545\n",
      "time/preback_policy (s)                 9.53681\n",
      "time/preback_start (s)                  0.666417\n",
      "time/preback_zf (s)                    28.737\n",
      "time/saving (s)                         4.39e-06\n",
      "time/training (s)                      10.5184\n",
      "time/epoch (s)                         84.845\n",
      "time/total (s)                       2770.08\n",
      "Epoch                                  31\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:07:30.298439 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 32 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 175000\n",
      "trainer/ZF1 Loss                       34.0189\n",
      "trainer/ZF2 Loss                       32.5466\n",
      "trainer/ZF Expert Reward                9.90921\n",
      "trainer/ZF Policy Reward                0.961219\n",
      "trainer/ZF CHI2 Term                   38.0314\n",
      "trainer/Policy Loss                  -212.197\n",
      "trainer/expert_lambda Loss              4.77386\n",
      "trainer/expert_lambda Value             9.50515\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               68.9602\n",
      "trainer/Policy Param Norm              32.5851\n",
      "trainer/Zf1 Grad Norm                1578.57\n",
      "trainer/Zf1 Param Norm                 78.5865\n",
      "trainer/Zf2 Grad Norm                1584.56\n",
      "trainer/Zf2 Param Norm                 78.6713\n",
      "trainer/Z Expert Predictions Mean     347.903\n",
      "trainer/Z Expert Predictions Std       39.3013\n",
      "trainer/Z Expert Predictions Max      447.252\n",
      "trainer/Z Expert Predictions Min      243.743\n",
      "trainer/Z Policy Predictions Mean     214.498\n",
      "trainer/Z Policy Predictions Std       99.4672\n",
      "trainer/Z Policy Predictions Max      328.125\n",
      "trainer/Z Policy Predictions Min     -110.467\n",
      "trainer/Z Expert Targets Mean         337.994\n",
      "trainer/Z Expert Targets Std           38.8336\n",
      "trainer/Z Expert Targets Max          435.758\n",
      "trainer/Z Expert Targets Min          234.396\n",
      "trainer/Z Policy Targets Mean         213.537\n",
      "trainer/Z Policy Targets Std           98.5291\n",
      "trainer/Z Policy Targets Max          316.947\n",
      "trainer/Z Policy Targets Min         -115.391\n",
      "trainer/Log Pis Mean                   25.6936\n",
      "trainer/Log Pis Std                     5.6245\n",
      "trainer/Policy mu Mean                  0.0144667\n",
      "trainer/Policy mu Std                   0.827645\n",
      "trainer/Policy log std Mean            -4.18529\n",
      "trainer/Policy log std Std              1.16944\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        170404\n",
      "exploration/num paths total           280\n",
      "evaluation/num steps total         287873\n",
      "evaluation/num paths total            335\n",
      "evaluation/path length Mean           903.1\n",
      "evaluation/path length Std            290.7\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             31\n",
      "evaluation/Rewards Mean                 4.20861\n",
      "evaluation/Rewards Std                  1.28373\n",
      "evaluation/Rewards Max                  6.95977\n",
      "evaluation/Rewards Min                 -2.30086\n",
      "evaluation/Returns Mean              3800.79\n",
      "evaluation/Returns Std               1289.74\n",
      "evaluation/Returns Max               4620.52\n",
      "evaluation/Returns Min                 29.9937\n",
      "evaluation/Estimation Bias Mean       204.644\n",
      "evaluation/Estimation Bias Std        132.96\n",
      "evaluation/EB/Q_True Mean              44.3391\n",
      "evaluation/EB/Q_True Std              128.831\n",
      "evaluation/EB/Q_Pred Mean             248.983\n",
      "evaluation/EB/Q_Pred Std               31.5121\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3800.79\n",
      "evaluation/Actions Mean                 0.0461524\n",
      "evaluation/Actions Std                  0.322555\n",
      "evaluation/Actions Max                  0.999521\n",
      "evaluation/Actions Min                 -0.99843\n",
      "time/backward_policy (s)                8.06668\n",
      "time/backward_zf1 (s)                  10.0015\n",
      "time/backward_zf2 (s)                   9.714\n",
      "time/data sampling (s)                  1.36374\n",
      "time/data storing (s)                   0.0796111\n",
      "time/evaluation sampling (s)            3.72823\n",
      "time/exploration sampling (s)           1.58279\n",
      "time/logging (s)                        0.0115021\n",
      "time/preback_alpha (s)                  0.00500855\n",
      "time/preback_policy (s)                 9.72598\n",
      "time/preback_start (s)                  0.661369\n",
      "time/preback_zf (s)                    28.7079\n",
      "time/saving (s)                         3.098e-06\n",
      "time/training (s)                      10.6378\n",
      "time/epoch (s)                         84.2862\n",
      "time/total (s)                       2854.37\n",
      "Epoch                                  32\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:08:56.223851 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 33 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 180000\n",
      "trainer/ZF1 Loss                       84.1266\n",
      "trainer/ZF2 Loss                       84.0428\n",
      "trainer/ZF Expert Reward               10.0014\n",
      "trainer/ZF Policy Reward                2.95522\n",
      "trainer/ZF CHI2 Term                   87.8822\n",
      "trainer/Policy Loss                  -206.125\n",
      "trainer/expert_lambda Loss              5.62923\n",
      "trainer/expert_lambda Value             9.48138\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               73.5798\n",
      "trainer/Policy Param Norm              32.7579\n",
      "trainer/Zf1 Grad Norm                2530.52\n",
      "trainer/Zf1 Param Norm                 78.79\n",
      "trainer/Zf2 Grad Norm                2495.91\n",
      "trainer/Zf2 Param Norm                 78.882\n",
      "trainer/Z Expert Predictions Mean     330.665\n",
      "trainer/Z Expert Predictions Std       36.7331\n",
      "trainer/Z Expert Predictions Max      427.117\n",
      "trainer/Z Expert Predictions Min      233.24\n",
      "trainer/Z Policy Predictions Mean     207.122\n",
      "trainer/Z Policy Predictions Std       96.9113\n",
      "trainer/Z Policy Predictions Max      312.335\n",
      "trainer/Z Policy Predictions Min     -113.627\n",
      "trainer/Z Expert Targets Mean         320.663\n",
      "trainer/Z Expert Targets Std           36.0364\n",
      "trainer/Z Expert Targets Max          413.646\n",
      "trainer/Z Expert Targets Min          226.007\n",
      "trainer/Z Policy Targets Mean         204.167\n",
      "trainer/Z Policy Targets Std           97.0307\n",
      "trainer/Z Policy Targets Max          307.622\n",
      "trainer/Z Policy Targets Min         -115.929\n",
      "trainer/Log Pis Mean                   25.6699\n",
      "trainer/Log Pis Std                     6.17082\n",
      "trainer/Policy mu Mean                  0.0279051\n",
      "trainer/Policy mu Std                   0.849719\n",
      "trainer/Policy log std Mean            -4.20572\n",
      "trainer/Policy log std Std              1.24366\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        175838\n",
      "exploration/num paths total           288\n",
      "evaluation/num steps total         296527\n",
      "evaluation/num paths total            345\n",
      "evaluation/path length Mean           865.4\n",
      "evaluation/path length Std            273.42\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            220\n",
      "evaluation/Rewards Mean                 3.78596\n",
      "evaluation/Rewards Std                  2.33413\n",
      "evaluation/Rewards Max                  7.23943\n",
      "evaluation/Rewards Min                 -3.02906\n",
      "evaluation/Returns Mean              3276.37\n",
      "evaluation/Returns Std               2163.27\n",
      "evaluation/Returns Max               4705.41\n",
      "evaluation/Returns Min              -1969.16\n",
      "evaluation/Estimation Bias Mean       158.078\n",
      "evaluation/Estimation Bias Std        156.682\n",
      "evaluation/EB/Q_True Mean              47.4802\n",
      "evaluation/EB/Q_True Std              135.656\n",
      "evaluation/EB/Q_Pred Mean             205.558\n",
      "evaluation/EB/Q_Pred Std               92.8704\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3276.37\n",
      "evaluation/Actions Mean                 0.0320537\n",
      "evaluation/Actions Std                  0.417625\n",
      "evaluation/Actions Max                  0.999985\n",
      "evaluation/Actions Min                 -0.999994\n",
      "time/backward_policy (s)                8.38241\n",
      "time/backward_zf1 (s)                  10.3893\n",
      "time/backward_zf2 (s)                  10.0687\n",
      "time/data sampling (s)                  1.39036\n",
      "time/data storing (s)                   0.0800177\n",
      "time/evaluation sampling (s)            4.02043\n",
      "time/exploration sampling (s)           1.61941\n",
      "time/logging (s)                        0.0191899\n",
      "time/preback_alpha (s)                  0.00512774\n",
      "time/preback_policy (s)                 9.59444\n",
      "time/preback_start (s)                  0.668529\n",
      "time/preback_zf (s)                    28.8405\n",
      "time/saving (s)                         4.841e-06\n",
      "time/training (s)                      10.6452\n",
      "time/epoch (s)                         85.7237\n",
      "time/total (s)                       2940.1\n",
      "Epoch                                  33\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:10:21.169895 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 34 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 185000\n",
      "trainer/ZF1 Loss                       27.5833\n",
      "trainer/ZF2 Loss                       31.3945\n",
      "trainer/ZF Expert Reward               10.0862\n",
      "trainer/ZF Policy Reward                1.50815\n",
      "trainer/ZF CHI2 Term                   34.0549\n",
      "trainer/Policy Loss                  -182.434\n",
      "trainer/expert_lambda Loss              4.03995\n",
      "trainer/expert_lambda Value             9.46897\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               77.77\n",
      "trainer/Policy Param Norm              32.9408\n",
      "trainer/Zf1 Grad Norm                1235.27\n",
      "trainer/Zf1 Param Norm                 78.9929\n",
      "trainer/Zf2 Grad Norm                1826.18\n",
      "trainer/Zf2 Param Norm                 79.0801\n",
      "trainer/Z Expert Predictions Mean     313.773\n",
      "trainer/Z Expert Predictions Std       38.6045\n",
      "trainer/Z Expert Predictions Max      418.14\n",
      "trainer/Z Expert Predictions Min      212.182\n",
      "trainer/Z Policy Predictions Mean     183.212\n",
      "trainer/Z Policy Predictions Std       95.6717\n",
      "trainer/Z Policy Predictions Max      308.412\n",
      "trainer/Z Policy Predictions Min     -110.772\n",
      "trainer/Z Expert Targets Mean         303.687\n",
      "trainer/Z Expert Targets Std           38.1751\n",
      "trainer/Z Expert Targets Max          407.202\n",
      "trainer/Z Expert Targets Min          203.779\n",
      "trainer/Z Policy Targets Mean         181.704\n",
      "trainer/Z Policy Targets Std           94.4203\n",
      "trainer/Z Policy Targets Max          294.456\n",
      "trainer/Z Policy Targets Min         -114.993\n",
      "trainer/Log Pis Mean                   25.99\n",
      "trainer/Log Pis Std                     5.87569\n",
      "trainer/Policy mu Mean                 -0.00506737\n",
      "trainer/Policy mu Std                   0.941763\n",
      "trainer/Policy log std Mean            -4.15674\n",
      "trainer/Policy log std Std              1.24651\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        180340\n",
      "exploration/num paths total           293\n",
      "evaluation/num steps total         305412\n",
      "evaluation/num paths total            356\n",
      "evaluation/path length Mean           807.727\n",
      "evaluation/path length Std            333.861\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             99\n",
      "evaluation/Rewards Mean                 4.44131\n",
      "evaluation/Rewards Std                  1.28241\n",
      "evaluation/Rewards Max                  7.31611\n",
      "evaluation/Rewards Min                 -2.81575\n",
      "evaluation/Returns Mean              3587.36\n",
      "evaluation/Returns Std               1619.31\n",
      "evaluation/Returns Max               4699.5\n",
      "evaluation/Returns Min                 36.7179\n",
      "evaluation/Estimation Bias Mean       173.182\n",
      "evaluation/Estimation Bias Std        141.818\n",
      "evaluation/EB/Q_True Mean              46.6881\n",
      "evaluation/EB/Q_True Std              135.588\n",
      "evaluation/EB/Q_Pred Mean             219.87\n",
      "evaluation/EB/Q_Pred Std               32.3873\n",
      "evaluation/Num Paths                   11\n",
      "evaluation/Average Returns           3587.36\n",
      "evaluation/Actions Mean                 0.0485011\n",
      "evaluation/Actions Std                  0.320977\n",
      "evaluation/Actions Max                  0.999385\n",
      "evaluation/Actions Min                 -0.998224\n",
      "time/backward_policy (s)                8.16336\n",
      "time/backward_zf1 (s)                  10.1286\n",
      "time/backward_zf2 (s)                   9.80383\n",
      "time/data sampling (s)                  1.38986\n",
      "time/data storing (s)                   0.0792952\n",
      "time/evaluation sampling (s)            3.74136\n",
      "time/exploration sampling (s)           1.58859\n",
      "time/logging (s)                        0.0185873\n",
      "time/preback_alpha (s)                  0.00508409\n",
      "time/preback_policy (s)                 9.68608\n",
      "time/preback_start (s)                  0.664359\n",
      "time/preback_zf (s)                    28.8245\n",
      "time/saving (s)                         3.313e-06\n",
      "time/training (s)                      10.645\n",
      "time/epoch (s)                         84.7385\n",
      "time/total (s)                       3024.84\n",
      "Epoch                                  34\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:11:47.117982 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 35 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 190000\n",
      "trainer/ZF1 Loss                       34.1569\n",
      "trainer/ZF2 Loss                       63.6578\n",
      "trainer/ZF Expert Reward                5.08309\n",
      "trainer/ZF Policy Reward               -0.736225\n",
      "trainer/ZF CHI2 Term                   52.096\n",
      "trainer/Policy Loss                  -170.998\n",
      "trainer/expert_lambda Loss             15.0974\n",
      "trainer/expert_lambda Value             9.46779\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               72.4718\n",
      "trainer/Policy Param Norm              33.1225\n",
      "trainer/Zf1 Grad Norm                1731.73\n",
      "trainer/Zf1 Param Norm                 79.1893\n",
      "trainer/Zf2 Grad Norm                5260.91\n",
      "trainer/Zf2 Param Norm                 79.3158\n",
      "trainer/Z Expert Predictions Mean     293.252\n",
      "trainer/Z Expert Predictions Std       38.1004\n",
      "trainer/Z Expert Predictions Max      394.542\n",
      "trainer/Z Expert Predictions Min      197.994\n",
      "trainer/Z Policy Predictions Mean     169.81\n",
      "trainer/Z Policy Predictions Std       92.7756\n",
      "trainer/Z Policy Predictions Max      316.291\n",
      "trainer/Z Policy Predictions Min      -97.1602\n",
      "trainer/Z Expert Targets Mean         288.169\n",
      "trainer/Z Expert Targets Std           38.8862\n",
      "trainer/Z Expert Targets Max          393.791\n",
      "trainer/Z Expert Targets Min          192.926\n",
      "trainer/Z Policy Targets Mean         170.547\n",
      "trainer/Z Policy Targets Std           91.7431\n",
      "trainer/Z Policy Targets Max          285.357\n",
      "trainer/Z Policy Targets Min          -96.6876\n",
      "trainer/Log Pis Mean                   25.8579\n",
      "trainer/Log Pis Std                     6.37581\n",
      "trainer/Policy mu Mean                  0.0155613\n",
      "trainer/Policy mu Std                   0.988235\n",
      "trainer/Policy log std Mean            -4.08596\n",
      "trainer/Policy log std Std              1.35904\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        185340\n",
      "exploration/num paths total           298\n",
      "evaluation/num steps total         313570\n",
      "evaluation/num paths total            366\n",
      "evaluation/path length Mean           815.8\n",
      "evaluation/path length Std            369.51\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             15\n",
      "evaluation/Rewards Mean                 4.58796\n",
      "evaluation/Rewards Std                  1.16477\n",
      "evaluation/Rewards Max                  7.4458\n",
      "evaluation/Rewards Min                 -2.12364\n",
      "evaluation/Returns Mean              3742.86\n",
      "evaluation/Returns Std               1739.52\n",
      "evaluation/Returns Max               5005.05\n",
      "evaluation/Returns Min                  6.0012\n",
      "evaluation/Estimation Bias Mean       157.782\n",
      "evaluation/Estimation Bias Std        140.473\n",
      "evaluation/EB/Q_True Mean              49.0016\n",
      "evaluation/EB/Q_True Std              134.886\n",
      "evaluation/EB/Q_Pred Mean             206.784\n",
      "evaluation/EB/Q_Pred Std               29.6205\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3742.86\n",
      "evaluation/Actions Mean                 0.0380612\n",
      "evaluation/Actions Std                  0.314621\n",
      "evaluation/Actions Max                  0.999409\n",
      "evaluation/Actions Min                 -0.994736\n",
      "time/backward_policy (s)                8.55066\n",
      "time/backward_zf1 (s)                  10.5673\n",
      "time/backward_zf2 (s)                  10.2324\n",
      "time/data sampling (s)                  1.40084\n",
      "time/data storing (s)                   0.0816062\n",
      "time/evaluation sampling (s)            3.68906\n",
      "time/exploration sampling (s)           1.598\n",
      "time/logging (s)                        0.0136036\n",
      "time/preback_alpha (s)                  0.00521023\n",
      "time/preback_policy (s)                 9.49664\n",
      "time/preback_start (s)                  0.676403\n",
      "time/preback_zf (s)                    28.9395\n",
      "time/saving (s)                         4.927e-06\n",
      "time/training (s)                      10.4819\n",
      "time/epoch (s)                         85.7331\n",
      "time/total (s)                       3110.57\n",
      "Epoch                                  35\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:13:14.291237 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 36 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 195000\n",
      "trainer/ZF1 Loss                       30.5842\n",
      "trainer/ZF2 Loss                       29.2463\n",
      "trainer/ZF Expert Reward                9.7263\n",
      "trainer/ZF Policy Reward                1.60112\n",
      "trainer/ZF CHI2 Term                   34.2504\n",
      "trainer/Policy Loss                  -164.811\n",
      "trainer/expert_lambda Loss              3.9758\n",
      "trainer/expert_lambda Value             9.47651\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               64.5846\n",
      "trainer/Policy Param Norm              33.2987\n",
      "trainer/Zf1 Grad Norm                1298.63\n",
      "trainer/Zf1 Param Norm                 79.395\n",
      "trainer/Zf2 Grad Norm                1463.26\n",
      "trainer/Zf2 Param Norm                 79.5124\n",
      "trainer/Z Expert Predictions Mean     288.512\n",
      "trainer/Z Expert Predictions Std       37.0493\n",
      "trainer/Z Expert Predictions Max      375.79\n",
      "trainer/Z Expert Predictions Min      188.123\n",
      "trainer/Z Policy Predictions Mean     162.681\n",
      "trainer/Z Policy Predictions Std       90.4226\n",
      "trainer/Z Policy Predictions Max      309.381\n",
      "trainer/Z Policy Predictions Min      -88.0816\n",
      "trainer/Z Expert Targets Mean         278.786\n",
      "trainer/Z Expert Targets Std           36.5524\n",
      "trainer/Z Expert Targets Max          366.515\n",
      "trainer/Z Expert Targets Min          183.834\n",
      "trainer/Z Policy Targets Mean         161.08\n",
      "trainer/Z Policy Targets Std           89.2396\n",
      "trainer/Z Policy Targets Max          314.79\n",
      "trainer/Z Policy Targets Min          -87.4359\n",
      "trainer/Log Pis Mean                   25.7419\n",
      "trainer/Log Pis Std                     5.93787\n",
      "trainer/Policy mu Mean                 -0.00169859\n",
      "trainer/Policy mu Std                   0.916209\n",
      "trainer/Policy log std Mean            -4.12872\n",
      "trainer/Policy log std Std              1.26022\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        190754\n",
      "exploration/num paths total           304\n",
      "evaluation/num steps total         322924\n",
      "evaluation/num paths total            377\n",
      "evaluation/path length Mean           850.364\n",
      "evaluation/path length Std            293.114\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             45\n",
      "evaluation/Rewards Mean                 4.57372\n",
      "evaluation/Rewards Std                  1.10213\n",
      "evaluation/Rewards Max                  7.3347\n",
      "evaluation/Rewards Min                 -1.85713\n",
      "evaluation/Returns Mean              3889.32\n",
      "evaluation/Returns Std               1397.81\n",
      "evaluation/Returns Max               4796.17\n",
      "evaluation/Returns Min                 73.4883\n",
      "evaluation/Estimation Bias Mean       157.749\n",
      "evaluation/Estimation Bias Std        138.437\n",
      "evaluation/EB/Q_True Mean              45.066\n",
      "evaluation/EB/Q_True Std              134.622\n",
      "evaluation/EB/Q_Pred Mean             202.815\n",
      "evaluation/EB/Q_Pred Std               28.9374\n",
      "evaluation/Num Paths                   11\n",
      "evaluation/Average Returns           3889.32\n",
      "evaluation/Actions Mean                 0.0418374\n",
      "evaluation/Actions Std                  0.31107\n",
      "evaluation/Actions Max                  0.999149\n",
      "evaluation/Actions Min                 -0.998554\n",
      "time/backward_policy (s)                8.58333\n",
      "time/backward_zf1 (s)                  10.7706\n",
      "time/backward_zf2 (s)                  10.363\n",
      "time/data sampling (s)                  1.47393\n",
      "time/data storing (s)                   0.0814286\n",
      "time/evaluation sampling (s)            3.45597\n",
      "time/exploration sampling (s)           1.61024\n",
      "time/logging (s)                        0.0117081\n",
      "time/preback_alpha (s)                  0.0051873\n",
      "time/preback_policy (s)                 9.82869\n",
      "time/preback_start (s)                  0.698481\n",
      "time/preback_zf (s)                    29.3419\n",
      "time/saving (s)                         2.863e-06\n",
      "time/training (s)                      10.7275\n",
      "time/epoch (s)                         86.952\n",
      "time/total (s)                       3197.53\n",
      "Epoch                                  36\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:14:40.547926 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 37 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 200000\n",
      "trainer/ZF1 Loss                       28.3044\n",
      "trainer/ZF2 Loss                       31.7325\n",
      "trainer/ZF Expert Reward                9.17431\n",
      "trainer/ZF Policy Reward                0.925152\n",
      "trainer/ZF CHI2 Term                   34.4193\n",
      "trainer/Policy Loss                  -159.726\n",
      "trainer/expert_lambda Loss              4.83103\n",
      "trainer/expert_lambda Value             9.48677\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               85.233\n",
      "trainer/Policy Param Norm              33.4632\n",
      "trainer/Zf1 Grad Norm                1309.26\n",
      "trainer/Zf1 Param Norm                 79.6048\n",
      "trainer/Zf2 Grad Norm                2219.64\n",
      "trainer/Zf2 Param Norm                 79.7854\n",
      "trainer/Z Expert Predictions Mean     277.679\n",
      "trainer/Z Expert Predictions Std       36.8017\n",
      "trainer/Z Expert Predictions Max      372.176\n",
      "trainer/Z Expert Predictions Min      189.311\n",
      "trainer/Z Policy Predictions Mean     159.911\n",
      "trainer/Z Policy Predictions Std       75.7353\n",
      "trainer/Z Policy Predictions Max      272.779\n",
      "trainer/Z Policy Predictions Min      -84.4534\n",
      "trainer/Z Expert Targets Mean         268.504\n",
      "trainer/Z Expert Targets Std           37.0453\n",
      "trainer/Z Expert Targets Max          369.02\n",
      "trainer/Z Expert Targets Min          178.182\n",
      "trainer/Z Policy Targets Mean         158.986\n",
      "trainer/Z Policy Targets Std           75.0473\n",
      "trainer/Z Policy Targets Max          278.273\n",
      "trainer/Z Policy Targets Min          -82.5767\n",
      "trainer/Log Pis Mean                   25.9\n",
      "trainer/Log Pis Std                     5.84619\n",
      "trainer/Policy mu Mean                 -0.00991621\n",
      "trainer/Policy mu Std                   0.909483\n",
      "trainer/Policy log std Mean            -4.15377\n",
      "trainer/Policy log std Std              1.25061\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        195145\n",
      "exploration/num paths total           310\n",
      "evaluation/num steps total         331609\n",
      "evaluation/num paths total            387\n",
      "evaluation/path length Mean           868.5\n",
      "evaluation/path length Std            246.479\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            317\n",
      "evaluation/Rewards Mean                 4.48892\n",
      "evaluation/Rewards Std                  1.17622\n",
      "evaluation/Rewards Max                  7.22065\n",
      "evaluation/Rewards Min                 -3.1\n",
      "evaluation/Returns Mean              3898.62\n",
      "evaluation/Returns Std               1137.53\n",
      "evaluation/Returns Max               4758.76\n",
      "evaluation/Returns Min               1312.28\n",
      "evaluation/Estimation Bias Mean       139.526\n",
      "evaluation/Estimation Bias Std        147.456\n",
      "evaluation/EB/Q_True Mean              50.0957\n",
      "evaluation/EB/Q_True Std              142.946\n",
      "evaluation/EB/Q_Pred Mean             189.621\n",
      "evaluation/EB/Q_Pred Std               29.0783\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3898.62\n",
      "evaluation/Actions Mean                 0.0547826\n",
      "evaluation/Actions Std                  0.317903\n",
      "evaluation/Actions Max                  0.999022\n",
      "evaluation/Actions Min                 -0.997852\n",
      "time/backward_policy (s)                8.672\n",
      "time/backward_zf1 (s)                  10.6557\n",
      "time/backward_zf2 (s)                  10.377\n",
      "time/data sampling (s)                  1.38808\n",
      "time/data storing (s)                   0.0803142\n",
      "time/evaluation sampling (s)            3.75452\n",
      "time/exploration sampling (s)           1.58403\n",
      "time/logging (s)                        0.0108734\n",
      "time/preback_alpha (s)                  0.00509286\n",
      "time/preback_policy (s)                 9.41649\n",
      "time/preback_start (s)                  0.674784\n",
      "time/preback_zf (s)                    28.9171\n",
      "time/saving (s)                         2.794e-06\n",
      "time/training (s)                      10.5102\n",
      "time/epoch (s)                         86.0462\n",
      "time/total (s)                       3283.57\n",
      "Epoch                                  37\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:16:06.021064 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 38 finished\n",
      "---------------------------------  ----------------\n",
      "replay_buffer/size                 205000\n",
      "trainer/ZF1 Loss                       38.9531\n",
      "trainer/ZF2 Loss                       26.3394\n",
      "trainer/ZF Expert Reward               12.5407\n",
      "trainer/ZF Policy Reward                3.91155\n",
      "trainer/ZF CHI2 Term                   37.2365\n",
      "trainer/Policy Loss                  -142.052\n",
      "trainer/expert_lambda Loss              9.10704\n",
      "trainer/expert_lambda Value             9.49667\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               74.8929\n",
      "trainer/Policy Param Norm              33.6289\n",
      "trainer/Zf1 Grad Norm                2579.58\n",
      "trainer/Zf1 Param Norm                 79.8326\n",
      "trainer/Zf2 Grad Norm                1346.11\n",
      "trainer/Zf2 Param Norm                 80.0638\n",
      "trainer/Z Expert Predictions Mean     271.184\n",
      "trainer/Z Expert Predictions Std       42.2224\n",
      "trainer/Z Expert Predictions Max      368.129\n",
      "trainer/Z Expert Predictions Min       16.6867\n",
      "trainer/Z Policy Predictions Mean     143.956\n",
      "trainer/Z Policy Predictions Std       83.8791\n",
      "trainer/Z Policy Predictions Max      264.101\n",
      "trainer/Z Policy Predictions Min      -88.6183\n",
      "trainer/Z Expert Targets Mean         258.643\n",
      "trainer/Z Expert Targets Std           41.8082\n",
      "trainer/Z Expert Targets Max          361.629\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean         140.045\n",
      "trainer/Z Policy Targets Std           81.6848\n",
      "trainer/Z Policy Targets Max          249.112\n",
      "trainer/Z Policy Targets Min          -90.0728\n",
      "trainer/Log Pis Mean                   25.6101\n",
      "trainer/Log Pis Std                     6.35137\n",
      "trainer/Policy mu Mean                  0.000891854\n",
      "trainer/Policy mu Std                   1.08349\n",
      "trainer/Policy log std Mean            -3.99966\n",
      "trainer/Policy log std Std              1.38135\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        199145\n",
      "exploration/num paths total           314\n",
      "evaluation/num steps total         339250\n",
      "evaluation/num paths total            397\n",
      "evaluation/path length Mean           764.1\n",
      "evaluation/path length Std            327.525\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             42\n",
      "evaluation/Rewards Mean                 4.48524\n",
      "evaluation/Rewards Std                  1.23745\n",
      "evaluation/Rewards Max                  7.54626\n",
      "evaluation/Rewards Min                 -2.62552\n",
      "evaluation/Returns Mean              3427.17\n",
      "evaluation/Returns Std               1561.22\n",
      "evaluation/Returns Max               4801.49\n",
      "evaluation/Returns Min                 94.2593\n",
      "evaluation/Estimation Bias Mean       127.958\n",
      "evaluation/Estimation Bias Std        154.637\n",
      "evaluation/EB/Q_True Mean              55.6266\n",
      "evaluation/EB/Q_True Std              147.323\n",
      "evaluation/EB/Q_Pred Mean             183.584\n",
      "evaluation/EB/Q_Pred Std               29.2898\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3427.17\n",
      "evaluation/Actions Mean                 0.041866\n",
      "evaluation/Actions Std                  0.317115\n",
      "evaluation/Actions Max                  0.99858\n",
      "evaluation/Actions Min                 -0.999876\n",
      "time/backward_policy (s)                8.43944\n",
      "time/backward_zf1 (s)                  10.4182\n",
      "time/backward_zf2 (s)                  10.1125\n",
      "time/data sampling (s)                  1.36579\n",
      "time/data storing (s)                   0.0799434\n",
      "time/evaluation sampling (s)            3.80559\n",
      "time/exploration sampling (s)           1.59282\n",
      "time/logging (s)                        0.0100975\n",
      "time/preback_alpha (s)                  0.00507946\n",
      "time/preback_policy (s)                 9.48615\n",
      "time/preback_start (s)                  0.667905\n",
      "time/preback_zf (s)                    28.799\n",
      "time/saving (s)                         2.905e-06\n",
      "time/training (s)                      10.4827\n",
      "time/epoch (s)                         85.2652\n",
      "time/total (s)                       3368.84\n",
      "Epoch                                  38\n",
      "---------------------------------  ----------------\n",
      "2024-11-07 18:17:31.950178 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 39 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 210000\n",
      "trainer/ZF1 Loss                       50.5324\n",
      "trainer/ZF2 Loss                       59.0128\n",
      "trainer/ZF Expert Reward               10.3126\n",
      "trainer/ZF Policy Reward                3.01954\n",
      "trainer/ZF CHI2 Term                   58.6989\n",
      "trainer/Policy Loss                  -144.976\n",
      "trainer/expert_lambda Loss              5.30559\n",
      "trainer/expert_lambda Value             9.50497\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               81.6209\n",
      "trainer/Policy Param Norm              33.7951\n",
      "trainer/Zf1 Grad Norm                1786.66\n",
      "trainer/Zf1 Param Norm                 80.0822\n",
      "trainer/Zf2 Grad Norm                1783.72\n",
      "trainer/Zf2 Param Norm                 80.3836\n",
      "trainer/Z Expert Predictions Mean     262.015\n",
      "trainer/Z Expert Predictions Std       36.8417\n",
      "trainer/Z Expert Predictions Max      357.317\n",
      "trainer/Z Expert Predictions Min      168.604\n",
      "trainer/Z Policy Predictions Mean     145.619\n",
      "trainer/Z Policy Predictions Std       73.6543\n",
      "trainer/Z Policy Predictions Max      244.016\n",
      "trainer/Z Policy Predictions Min     -103.9\n",
      "trainer/Z Expert Targets Mean         251.703\n",
      "trainer/Z Expert Targets Std           36.4289\n",
      "trainer/Z Expert Targets Max          339.097\n",
      "trainer/Z Expert Targets Min          156.991\n",
      "trainer/Z Policy Targets Mean         142.6\n",
      "trainer/Z Policy Targets Std           72.911\n",
      "trainer/Z Policy Targets Max          223.615\n",
      "trainer/Z Policy Targets Min          -98.6788\n",
      "trainer/Log Pis Mean                   26.4897\n",
      "trainer/Log Pis Std                     6.00227\n",
      "trainer/Policy mu Mean                 -0.0325946\n",
      "trainer/Policy mu Std                   0.996953\n",
      "trainer/Policy log std Mean            -4.19754\n",
      "trainer/Policy log std Std              1.29496\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        205145\n",
      "exploration/num paths total           320\n",
      "evaluation/num steps total         346754\n",
      "evaluation/num paths total            407\n",
      "evaluation/path length Mean           750.4\n",
      "evaluation/path length Std            388.072\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             30\n",
      "evaluation/Rewards Mean                 4.53636\n",
      "evaluation/Rewards Std                  1.19851\n",
      "evaluation/Rewards Max                  7.30664\n",
      "evaluation/Rewards Min                 -2.60446\n",
      "evaluation/Returns Mean              3404.09\n",
      "evaluation/Returns Std               1834.7\n",
      "evaluation/Returns Max               4914.5\n",
      "evaluation/Returns Min                 41.8026\n",
      "evaluation/Estimation Bias Mean       122.77\n",
      "evaluation/Estimation Bias Std        152.715\n",
      "evaluation/EB/Q_True Mean              55.8604\n",
      "evaluation/EB/Q_True Std              147.117\n",
      "evaluation/EB/Q_Pred Mean             178.63\n",
      "evaluation/EB/Q_Pred Std               28.472\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3404.09\n",
      "evaluation/Actions Mean                 0.0452369\n",
      "evaluation/Actions Std                  0.320041\n",
      "evaluation/Actions Max                  0.995946\n",
      "evaluation/Actions Min                 -0.999167\n",
      "time/backward_policy (s)                8.34284\n",
      "time/backward_zf1 (s)                  10.3805\n",
      "time/backward_zf2 (s)                  10.0187\n",
      "time/data sampling (s)                  1.42917\n",
      "time/data storing (s)                   0.0810597\n",
      "time/evaluation sampling (s)            3.88471\n",
      "time/exploration sampling (s)           1.59751\n",
      "time/logging (s)                        0.00977486\n",
      "time/preback_alpha (s)                  0.00533057\n",
      "time/preback_policy (s)                 9.68131\n",
      "time/preback_start (s)                  0.689841\n",
      "time/preback_zf (s)                    28.9731\n",
      "time/saving (s)                         2.855e-06\n",
      "time/training (s)                      10.6251\n",
      "time/epoch (s)                         85.7189\n",
      "time/total (s)                       3454.56\n",
      "Epoch                                  39\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:18:59.454309 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 40 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 215000\n",
      "trainer/ZF1 Loss                       41.7186\n",
      "trainer/ZF2 Loss                       34.2479\n",
      "trainer/ZF Expert Reward               12.4065\n",
      "trainer/ZF Policy Reward                3.26299\n",
      "trainer/ZF CHI2 Term                   42.8301\n",
      "trainer/Policy Loss                  -137.67\n",
      "trainer/expert_lambda Loss              8.45301\n",
      "trainer/expert_lambda Value             9.51949\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               70.2957\n",
      "trainer/Policy Param Norm              33.9487\n",
      "trainer/Zf1 Grad Norm                2741.61\n",
      "trainer/Zf1 Param Norm                 80.3127\n",
      "trainer/Zf2 Grad Norm                1294.31\n",
      "trainer/Zf2 Param Norm                 80.7075\n",
      "trainer/Z Expert Predictions Mean     258.79\n",
      "trainer/Z Expert Predictions Std       33.015\n",
      "trainer/Z Expert Predictions Max      337.259\n",
      "trainer/Z Expert Predictions Min      169.862\n",
      "trainer/Z Policy Predictions Mean     139.089\n",
      "trainer/Z Policy Predictions Std       72.9303\n",
      "trainer/Z Policy Predictions Max      230.539\n",
      "trainer/Z Policy Predictions Min     -101.513\n",
      "trainer/Z Expert Targets Mean         246.384\n",
      "trainer/Z Expert Targets Std           32.3212\n",
      "trainer/Z Expert Targets Max          324.75\n",
      "trainer/Z Expert Targets Min          155.142\n",
      "trainer/Z Policy Targets Mean         135.826\n",
      "trainer/Z Policy Targets Std           70.0112\n",
      "trainer/Z Policy Targets Max          237.611\n",
      "trainer/Z Policy Targets Min          -99.4061\n",
      "trainer/Log Pis Mean                   26.0027\n",
      "trainer/Log Pis Std                     6.20749\n",
      "trainer/Policy mu Mean                  0.0417843\n",
      "trainer/Policy mu Std                   0.951887\n",
      "trainer/Policy log std Mean            -4.14161\n",
      "trainer/Policy log std Std              1.31224\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        209929\n",
      "exploration/num paths total           325\n",
      "evaluation/num steps total         354822\n",
      "evaluation/num paths total            419\n",
      "evaluation/path length Mean           672.333\n",
      "evaluation/path length Std            391.82\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             12\n",
      "evaluation/Rewards Mean                 4.49554\n",
      "evaluation/Rewards Std                  1.29545\n",
      "evaluation/Rewards Max                  7.47409\n",
      "evaluation/Rewards Min                 -2.38879\n",
      "evaluation/Returns Mean              3022.5\n",
      "evaluation/Returns Std               1808.44\n",
      "evaluation/Returns Max               4738.3\n",
      "evaluation/Returns Min                 15.2022\n",
      "evaluation/Estimation Bias Mean       110.305\n",
      "evaluation/Estimation Bias Std        151.566\n",
      "evaluation/EB/Q_True Mean              53.2356\n",
      "evaluation/EB/Q_True Std              145.223\n",
      "evaluation/EB/Q_Pred Mean             163.541\n",
      "evaluation/EB/Q_Pred Std               29.378\n",
      "evaluation/Num Paths                   12\n",
      "evaluation/Average Returns           3022.5\n",
      "evaluation/Actions Mean                 0.0461779\n",
      "evaluation/Actions Std                  0.321221\n",
      "evaluation/Actions Max                  0.997036\n",
      "evaluation/Actions Min                 -0.997207\n",
      "time/backward_policy (s)                8.56335\n",
      "time/backward_zf1 (s)                  10.7107\n",
      "time/backward_zf2 (s)                  10.3225\n",
      "time/data sampling (s)                  1.45285\n",
      "time/data storing (s)                   0.0818924\n",
      "time/evaluation sampling (s)            4.10509\n",
      "time/exploration sampling (s)           1.64304\n",
      "time/logging (s)                        0.0106884\n",
      "time/preback_alpha (s)                  0.00518015\n",
      "time/preback_policy (s)                 9.79327\n",
      "time/preback_start (s)                  0.69278\n",
      "time/preback_zf (s)                    29.1784\n",
      "time/saving (s)                         3.98e-06\n",
      "time/training (s)                      10.7311\n",
      "time/epoch (s)                         87.2908\n",
      "time/total (s)                       3541.86\n",
      "Epoch                                  40\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:20:25.992725 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 41 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 220000\n",
      "trainer/ZF1 Loss                       35.4069\n",
      "trainer/ZF2 Loss                       34.799\n",
      "trainer/ZF Expert Reward                8.86109\n",
      "trainer/ZF Policy Reward                1.56324\n",
      "trainer/ZF CHI2 Term                   39.0309\n",
      "trainer/Policy Loss                  -132.977\n",
      "trainer/expert_lambda Loss              3.93108\n",
      "trainer/expert_lambda Value             9.53461\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               94.0826\n",
      "trainer/Policy Param Norm              34.0915\n",
      "trainer/Zf1 Grad Norm                1867.07\n",
      "trainer/Zf1 Param Norm                 80.5953\n",
      "trainer/Zf2 Grad Norm                1952.72\n",
      "trainer/Zf2 Param Norm                 81.073\n",
      "trainer/Z Expert Predictions Mean     247.69\n",
      "trainer/Z Expert Predictions Std       32.9658\n",
      "trainer/Z Expert Predictions Max      330.757\n",
      "trainer/Z Expert Predictions Min      158.325\n",
      "trainer/Z Policy Predictions Mean     132.974\n",
      "trainer/Z Policy Predictions Std       71.9133\n",
      "trainer/Z Policy Predictions Max      238.925\n",
      "trainer/Z Policy Predictions Min      -89.8626\n",
      "trainer/Z Expert Targets Mean         238.829\n",
      "trainer/Z Expert Targets Std           33.1164\n",
      "trainer/Z Expert Targets Max          322\n",
      "trainer/Z Expert Targets Min          146.581\n",
      "trainer/Z Policy Targets Mean         131.411\n",
      "trainer/Z Policy Targets Std           70.1566\n",
      "trainer/Z Policy Targets Max          244.249\n",
      "trainer/Z Policy Targets Min          -87.34\n",
      "trainer/Log Pis Mean                   26.4377\n",
      "trainer/Log Pis Std                     6.28295\n",
      "trainer/Policy mu Mean                  0.0166853\n",
      "trainer/Policy mu Std                   1.00897\n",
      "trainer/Policy log std Mean            -4.15926\n",
      "trainer/Policy log std Std              1.35464\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        214999\n",
      "exploration/num paths total           331\n",
      "evaluation/num steps total         364592\n",
      "evaluation/num paths total            429\n",
      "evaluation/path length Mean           977\n",
      "evaluation/path length Std             69\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            770\n",
      "evaluation/Rewards Mean                 4.5536\n",
      "evaluation/Rewards Std                  1.02248\n",
      "evaluation/Rewards Max                  7.24596\n",
      "evaluation/Rewards Min                 -2.26653\n",
      "evaluation/Returns Mean              4448.87\n",
      "evaluation/Returns Std                395.497\n",
      "evaluation/Returns Max               4837.12\n",
      "evaluation/Returns Min               3353.61\n",
      "evaluation/Estimation Bias Mean       129.746\n",
      "evaluation/Estimation Bias Std        126.915\n",
      "evaluation/EB/Q_True Mean              39.4999\n",
      "evaluation/EB/Q_True Std              123.169\n",
      "evaluation/EB/Q_Pred Mean             169.246\n",
      "evaluation/EB/Q_Pred Std               23.5106\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4448.87\n",
      "evaluation/Actions Mean                 0.0403799\n",
      "evaluation/Actions Std                  0.312912\n",
      "evaluation/Actions Max                  0.998625\n",
      "evaluation/Actions Min                 -0.99747\n",
      "time/backward_policy (s)                8.45786\n",
      "time/backward_zf1 (s)                  10.5549\n",
      "time/backward_zf2 (s)                  10.1172\n",
      "time/data sampling (s)                  1.44484\n",
      "time/data storing (s)                   0.0807466\n",
      "time/evaluation sampling (s)            3.66329\n",
      "time/exploration sampling (s)           1.5983\n",
      "time/logging (s)                        0.0144811\n",
      "time/preback_alpha (s)                  0.00516588\n",
      "time/preback_policy (s)                 9.7747\n",
      "time/preback_start (s)                  0.687693\n",
      "time/preback_zf (s)                    29.197\n",
      "time/saving (s)                         4.815e-06\n",
      "time/training (s)                      10.7327\n",
      "time/epoch (s)                         86.3289\n",
      "time/total (s)                       3628.19\n",
      "Epoch                                  41\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:21:52.335235 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 42 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 225000\n",
      "trainer/ZF1 Loss                       38.3691\n",
      "trainer/ZF2 Loss                       38.5274\n",
      "trainer/ZF Expert Reward               13.8505\n",
      "trainer/ZF Policy Reward                4.60336\n",
      "trainer/ZF CHI2 Term                   43.3506\n",
      "trainer/Policy Loss                  -122.713\n",
      "trainer/expert_lambda Loss             14.3036\n",
      "trainer/expert_lambda Value             9.5516\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               69.4492\n",
      "trainer/Policy Param Norm              34.2342\n",
      "trainer/Zf1 Grad Norm                2575.19\n",
      "trainer/Zf1 Param Norm                 80.9024\n",
      "trainer/Zf2 Grad Norm                2603.44\n",
      "trainer/Zf2 Param Norm                 81.3984\n",
      "trainer/Z Expert Predictions Mean     244.253\n",
      "trainer/Z Expert Predictions Std       34.8733\n",
      "trainer/Z Expert Predictions Max      333.869\n",
      "trainer/Z Expert Predictions Min      160.429\n",
      "trainer/Z Policy Predictions Mean     125.949\n",
      "trainer/Z Policy Predictions Std       77.5087\n",
      "trainer/Z Policy Predictions Max      226.23\n",
      "trainer/Z Policy Predictions Min      -89.8413\n",
      "trainer/Z Expert Targets Mean         230.402\n",
      "trainer/Z Expert Targets Std           33.992\n",
      "trainer/Z Expert Targets Max          316.184\n",
      "trainer/Z Expert Targets Min          148.758\n",
      "trainer/Z Policy Targets Mean         121.346\n",
      "trainer/Z Policy Targets Std           74.6078\n",
      "trainer/Z Policy Targets Max          206.821\n",
      "trainer/Z Policy Targets Min          -86.4849\n",
      "trainer/Log Pis Mean                   26.7591\n",
      "trainer/Log Pis Std                     5.71414\n",
      "trainer/Policy mu Mean                 -0.0233292\n",
      "trainer/Policy mu Std                   1.11614\n",
      "trainer/Policy log std Mean            -4.12505\n",
      "trainer/Policy log std Std              1.34891\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        221720\n",
      "exploration/num paths total           339\n",
      "evaluation/num steps total         374582\n",
      "evaluation/num paths total            439\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              3\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            990\n",
      "evaluation/Rewards Mean                 4.42856\n",
      "evaluation/Rewards Std                  1.17449\n",
      "evaluation/Rewards Max                  7.07237\n",
      "evaluation/Rewards Min                 -2.51618\n",
      "evaluation/Returns Mean              4424.14\n",
      "evaluation/Returns Std                158.501\n",
      "evaluation/Returns Max               4773.98\n",
      "evaluation/Returns Min               4135.1\n",
      "evaluation/Estimation Bias Mean       116.473\n",
      "evaluation/Estimation Bias Std        128.083\n",
      "evaluation/EB/Q_True Mean              41.468\n",
      "evaluation/EB/Q_True Std              127.423\n",
      "evaluation/EB/Q_Pred Mean             157.941\n",
      "evaluation/EB/Q_Pred Std               25.6855\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4424.14\n",
      "evaluation/Actions Mean                 0.0437259\n",
      "evaluation/Actions Std                  0.319148\n",
      "evaluation/Actions Max                  0.998423\n",
      "evaluation/Actions Min                 -0.999323\n",
      "time/backward_policy (s)                8.92532\n",
      "time/backward_zf1 (s)                  10.912\n",
      "time/backward_zf2 (s)                  10.6163\n",
      "time/data sampling (s)                  1.37063\n",
      "time/data storing (s)                   0.0805646\n",
      "time/evaluation sampling (s)            3.47276\n",
      "time/exploration sampling (s)           1.59988\n",
      "time/logging (s)                        0.012864\n",
      "time/preback_alpha (s)                  0.00525565\n",
      "time/preback_policy (s)                 9.21514\n",
      "time/preback_start (s)                  0.672501\n",
      "time/preback_zf (s)                    28.8705\n",
      "time/saving (s)                         3.154e-06\n",
      "time/training (s)                      10.3772\n",
      "time/epoch (s)                         86.1308\n",
      "time/total (s)                       3714.32\n",
      "Epoch                                  42\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:23:17.788799 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 43 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 230000\n",
      "trainer/ZF1 Loss                       20.6586\n",
      "trainer/ZF2 Loss                       24.7064\n",
      "trainer/ZF Expert Reward               11.2276\n",
      "trainer/ZF Policy Reward                1.87982\n",
      "trainer/ZF CHI2 Term                   27.6344\n",
      "trainer/Policy Loss                  -116.666\n",
      "trainer/expert_lambda Loss              7.12705\n",
      "trainer/expert_lambda Value             9.57392\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               83.2553\n",
      "trainer/Policy Param Norm              34.3827\n",
      "trainer/Zf1 Grad Norm                1250.65\n",
      "trainer/Zf1 Param Norm                 81.1707\n",
      "trainer/Zf2 Grad Norm                1334.38\n",
      "trainer/Zf2 Param Norm                 81.7381\n",
      "trainer/Z Expert Predictions Mean     241.144\n",
      "trainer/Z Expert Predictions Std       37.0119\n",
      "trainer/Z Expert Predictions Max      326.304\n",
      "trainer/Z Expert Predictions Min      150.431\n",
      "trainer/Z Policy Predictions Mean     118.445\n",
      "trainer/Z Policy Predictions Std       76.0054\n",
      "trainer/Z Policy Predictions Max      228.034\n",
      "trainer/Z Policy Predictions Min      -90.6667\n",
      "trainer/Z Expert Targets Mean         229.917\n",
      "trainer/Z Expert Targets Std           36.2\n",
      "trainer/Z Expert Targets Max          311.894\n",
      "trainer/Z Expert Targets Min          142.855\n",
      "trainer/Z Policy Targets Mean         116.565\n",
      "trainer/Z Policy Targets Std           74.7332\n",
      "trainer/Z Policy Targets Max          206.459\n",
      "trainer/Z Policy Targets Min          -90.8821\n",
      "trainer/Log Pis Mean                   26.5303\n",
      "trainer/Log Pis Std                     5.37625\n",
      "trainer/Policy mu Mean                 -0.0120073\n",
      "trainer/Policy mu Std                   1.0357\n",
      "trainer/Policy log std Mean            -4.12678\n",
      "trainer/Policy log std Std              1.36194\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        224720\n",
      "exploration/num paths total           342\n",
      "evaluation/num steps total         383705\n",
      "evaluation/num paths total            449\n",
      "evaluation/path length Mean           912.3\n",
      "evaluation/path length Std            263.1\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            123\n",
      "evaluation/Rewards Mean                 4.05147\n",
      "evaluation/Rewards Std                  1.88434\n",
      "evaluation/Rewards Max                  7.34219\n",
      "evaluation/Rewards Min                 -2.72001\n",
      "evaluation/Returns Mean              3696.16\n",
      "evaluation/Returns Std               1681.6\n",
      "evaluation/Returns Max               4672\n",
      "evaluation/Returns Min                 92.4643\n",
      "evaluation/Estimation Bias Mean       125.955\n",
      "evaluation/Estimation Bias Std         95.7534\n",
      "evaluation/EB/Q_True Mean               3.85411\n",
      "evaluation/EB/Q_True Std               70.687\n",
      "evaluation/EB/Q_Pred Mean             129.809\n",
      "evaluation/EB/Q_Pred Std               62.8887\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3696.16\n",
      "evaluation/Actions Mean                 0.0299024\n",
      "evaluation/Actions Std                  0.3704\n",
      "evaluation/Actions Max                  1\n",
      "evaluation/Actions Min                 -0.999988\n",
      "time/backward_policy (s)                8.42009\n",
      "time/backward_zf1 (s)                  10.3398\n",
      "time/backward_zf2 (s)                  10.0683\n",
      "time/data sampling (s)                  1.31022\n",
      "time/data storing (s)                   0.0793679\n",
      "time/evaluation sampling (s)            4.16425\n",
      "time/exploration sampling (s)           1.56767\n",
      "time/logging (s)                        0.0112572\n",
      "time/preback_alpha (s)                  0.00501724\n",
      "time/preback_policy (s)                 9.43306\n",
      "time/preback_start (s)                  0.659647\n",
      "time/preback_zf (s)                    28.7548\n",
      "time/saving (s)                         2.872e-06\n",
      "time/training (s)                      10.431\n",
      "time/epoch (s)                         85.2445\n",
      "time/total (s)                       3799.57\n",
      "Epoch                                  43\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:24:49.060306 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 44 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 235000\n",
      "trainer/ZF1 Loss                       24.6431\n",
      "trainer/ZF2 Loss                       28.2976\n",
      "trainer/ZF Expert Reward               10.6672\n",
      "trainer/ZF Policy Reward                2.67231\n",
      "trainer/ZF CHI2 Term                   30.7465\n",
      "trainer/Policy Loss                  -118.424\n",
      "trainer/expert_lambda Loss              4.88718\n",
      "trainer/expert_lambda Value             9.60017\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               69.6865\n",
      "trainer/Policy Param Norm              34.5163\n",
      "trainer/Zf1 Grad Norm                1143.73\n",
      "trainer/Zf1 Param Norm                 81.4362\n",
      "trainer/Zf2 Grad Norm                1321.64\n",
      "trainer/Zf2 Param Norm                 82.0766\n",
      "trainer/Z Expert Predictions Mean     233.569\n",
      "trainer/Z Expert Predictions Std       32.0896\n",
      "trainer/Z Expert Predictions Max      316.329\n",
      "trainer/Z Expert Predictions Min      142.513\n",
      "trainer/Z Policy Predictions Mean     119.716\n",
      "trainer/Z Policy Predictions Std       65.4606\n",
      "trainer/Z Policy Predictions Max      220.864\n",
      "trainer/Z Policy Predictions Min      -93.1059\n",
      "trainer/Z Expert Targets Mean         222.902\n",
      "trainer/Z Expert Targets Std           31.925\n",
      "trainer/Z Expert Targets Max          301.11\n",
      "trainer/Z Expert Targets Min          130.199\n",
      "trainer/Z Policy Targets Mean         117.044\n",
      "trainer/Z Policy Targets Std           63.3503\n",
      "trainer/Z Policy Targets Max          227.28\n",
      "trainer/Z Policy Targets Min          -87.5031\n",
      "trainer/Log Pis Mean                   26.6878\n",
      "trainer/Log Pis Std                     5.48128\n",
      "trainer/Policy mu Mean                  0.0206576\n",
      "trainer/Policy mu Std                   0.894498\n",
      "trainer/Policy log std Mean            -4.25115\n",
      "trainer/Policy log std Std              1.22941\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        231385\n",
      "exploration/num paths total           350\n",
      "evaluation/num steps total         393029\n",
      "evaluation/num paths total            459\n",
      "evaluation/path length Mean           932.4\n",
      "evaluation/path length Std            202.8\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            324\n",
      "evaluation/Rewards Mean                 4.55448\n",
      "evaluation/Rewards Std                  1.17654\n",
      "evaluation/Rewards Max                  7.52117\n",
      "evaluation/Rewards Min                 -1.51392\n",
      "evaluation/Returns Mean              4246.59\n",
      "evaluation/Returns Std                919.472\n",
      "evaluation/Returns Max               4844.63\n",
      "evaluation/Returns Min               1523.01\n",
      "evaluation/Estimation Bias Mean       101.344\n",
      "evaluation/Estimation Bias Std        135.235\n",
      "evaluation/EB/Q_True Mean              44.8194\n",
      "evaluation/EB/Q_True Std              132.619\n",
      "evaluation/EB/Q_Pred Mean             146.164\n",
      "evaluation/EB/Q_Pred Std               24.6086\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4246.59\n",
      "evaluation/Actions Mean                 0.0372264\n",
      "evaluation/Actions Std                  0.317462\n",
      "evaluation/Actions Max                  0.99977\n",
      "evaluation/Actions Min                 -0.997469\n",
      "time/backward_policy (s)                8.97924\n",
      "time/backward_zf1 (s)                  11.6188\n",
      "time/backward_zf2 (s)                  11.0051\n",
      "time/data sampling (s)                  1.62975\n",
      "time/data storing (s)                   0.0905285\n",
      "time/evaluation sampling (s)            2.89098\n",
      "time/exploration sampling (s)           1.70827\n",
      "time/logging (s)                        0.0146082\n",
      "time/preback_alpha (s)                  0.00553609\n",
      "time/preback_policy (s)                10.6499\n",
      "time/preback_start (s)                  0.742812\n",
      "time/preback_zf (s)                    30.5013\n",
      "time/saving (s)                         5.17e-06\n",
      "time/training (s)                      11.197\n",
      "time/epoch (s)                         91.0338\n",
      "time/total (s)                       3890.6\n",
      "Epoch                                  44\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:26:27.423304 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 45 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 240000\n",
      "trainer/ZF1 Loss                       29.2104\n",
      "trainer/ZF2 Loss                       37.9304\n",
      "trainer/ZF Expert Reward               12.3155\n",
      "trainer/ZF Policy Reward                3.94853\n",
      "trainer/ZF CHI2 Term                   38.0328\n",
      "trainer/Policy Loss                  -110.979\n",
      "trainer/expert_lambda Loss              6.87052\n",
      "trainer/expert_lambda Value             9.63132\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               60.3751\n",
      "trainer/Policy Param Norm              34.6422\n",
      "trainer/Zf1 Grad Norm                1260.12\n",
      "trainer/Zf1 Param Norm                 81.7158\n",
      "trainer/Zf2 Grad Norm                2141.75\n",
      "trainer/Zf2 Param Norm                 82.3936\n",
      "trainer/Z Expert Predictions Mean     230.75\n",
      "trainer/Z Expert Predictions Std       33.9676\n",
      "trainer/Z Expert Predictions Max      305.774\n",
      "trainer/Z Expert Predictions Min      134.576\n",
      "trainer/Z Policy Predictions Mean     112.084\n",
      "trainer/Z Policy Predictions Std       72.4551\n",
      "trainer/Z Policy Predictions Max      201.068\n",
      "trainer/Z Policy Predictions Min      -92.7108\n",
      "trainer/Z Expert Targets Mean         218.435\n",
      "trainer/Z Expert Targets Std           33.4248\n",
      "trainer/Z Expert Targets Max          293.137\n",
      "trainer/Z Expert Targets Min          124.124\n",
      "trainer/Z Policy Targets Mean         108.136\n",
      "trainer/Z Policy Targets Std           70.3055\n",
      "trainer/Z Policy Targets Max          193.548\n",
      "trainer/Z Policy Targets Min          -90.0615\n",
      "trainer/Log Pis Mean                   26.9802\n",
      "trainer/Log Pis Std                     5.34943\n",
      "trainer/Policy mu Mean                  0.00870834\n",
      "trainer/Policy mu Std                   1.01119\n",
      "trainer/Policy log std Mean            -4.20518\n",
      "trainer/Policy log std Std              1.30726\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        234419\n",
      "exploration/num paths total           354\n",
      "evaluation/num steps total         402590\n",
      "evaluation/num paths total            469\n",
      "evaluation/path length Mean           956.1\n",
      "evaluation/path length Std            131.7\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            561\n",
      "evaluation/Rewards Mean                 4.44993\n",
      "evaluation/Rewards Std                  1.26537\n",
      "evaluation/Rewards Max                  7.65253\n",
      "evaluation/Rewards Min                 -3.16379\n",
      "evaluation/Returns Mean              4254.58\n",
      "evaluation/Returns Std                670.113\n",
      "evaluation/Returns Max               4730.01\n",
      "evaluation/Returns Min               2345.9\n",
      "evaluation/Estimation Bias Mean       100.225\n",
      "evaluation/Estimation Bias Std        136.774\n",
      "evaluation/EB/Q_True Mean              44.5408\n",
      "evaluation/EB/Q_True Std              133.507\n",
      "evaluation/EB/Q_Pred Mean             144.765\n",
      "evaluation/EB/Q_Pred Std               27.5946\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4254.58\n",
      "evaluation/Actions Mean                 0.0397358\n",
      "evaluation/Actions Std                  0.321002\n",
      "evaluation/Actions Max                  0.996205\n",
      "evaluation/Actions Min                 -0.999683\n",
      "time/backward_policy (s)                9.90187\n",
      "time/backward_zf1 (s)                  13.2039\n",
      "time/backward_zf2 (s)                  12.3227\n",
      "time/data sampling (s)                  1.94302\n",
      "time/data storing (s)                   0.100389\n",
      "time/evaluation sampling (s)            2.18353\n",
      "time/exploration sampling (s)           1.83347\n",
      "time/logging (s)                        0.0124783\n",
      "time/preback_alpha (s)                  0.00613279\n",
      "time/preback_policy (s)                11.6315\n",
      "time/preback_start (s)                  0.848975\n",
      "time/preback_zf (s)                    32.065\n",
      "time/saving (s)                         3.414e-06\n",
      "time/training (s)                      12.0301\n",
      "time/epoch (s)                         98.0831\n",
      "time/total (s)                       3988.69\n",
      "Epoch                                  45\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:27:55.740417 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 46 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 245000\n",
      "trainer/ZF1 Loss                       28.4099\n",
      "trainer/ZF2 Loss                       20.9202\n",
      "trainer/ZF Expert Reward               10.8537\n",
      "trainer/ZF Policy Reward                1.92508\n",
      "trainer/ZF CHI2 Term                   29.402\n",
      "trainer/Policy Loss                  -107.133\n",
      "trainer/expert_lambda Loss              4.81942\n",
      "trainer/expert_lambda Value             9.66806\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               64.2304\n",
      "trainer/Policy Param Norm              34.7574\n",
      "trainer/Zf1 Grad Norm                2225.61\n",
      "trainer/Zf1 Param Norm                 82.0032\n",
      "trainer/Zf2 Grad Norm                1754.13\n",
      "trainer/Zf2 Param Norm                 82.7326\n",
      "trainer/Z Expert Predictions Mean     221.689\n",
      "trainer/Z Expert Predictions Std       36.6236\n",
      "trainer/Z Expert Predictions Max      305.035\n",
      "trainer/Z Expert Predictions Min       15.7965\n",
      "trainer/Z Policy Predictions Mean     106.651\n",
      "trainer/Z Policy Predictions Std       64.7177\n",
      "trainer/Z Policy Predictions Max      211.758\n",
      "trainer/Z Policy Predictions Min      -88.3125\n",
      "trainer/Z Expert Targets Mean         210.835\n",
      "trainer/Z Expert Targets Std           36.35\n",
      "trainer/Z Expert Targets Max          290.266\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean         104.726\n",
      "trainer/Z Policy Targets Std           63.3346\n",
      "trainer/Z Policy Targets Max          198.413\n",
      "trainer/Z Policy Targets Min          -87.2942\n",
      "trainer/Log Pis Mean                   25.4426\n",
      "trainer/Log Pis Std                     5.7698\n",
      "trainer/Policy mu Mean                  0.00909795\n",
      "trainer/Policy mu Std                   0.999317\n",
      "trainer/Policy log std Mean            -4.05313\n",
      "trainer/Policy log std Std              1.34644\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        241535\n",
      "exploration/num paths total           362\n",
      "evaluation/num steps total         412325\n",
      "evaluation/num paths total            479\n",
      "evaluation/path length Mean           973.5\n",
      "evaluation/path length Std             79.5\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            735\n",
      "evaluation/Rewards Mean                 4.10987\n",
      "evaluation/Rewards Std                  1.894\n",
      "evaluation/Rewards Max                  7.29198\n",
      "evaluation/Rewards Min                 -2.41792\n",
      "evaluation/Returns Mean              4000.96\n",
      "evaluation/Returns Std               1355.11\n",
      "evaluation/Returns Max               4762.71\n",
      "evaluation/Returns Min                128.821\n",
      "evaluation/Estimation Bias Mean        82.211\n",
      "evaluation/Estimation Bias Std        134.959\n",
      "evaluation/EB/Q_True Mean              43.2633\n",
      "evaluation/EB/Q_True Std              131.999\n",
      "evaluation/EB/Q_Pred Mean             125.474\n",
      "evaluation/EB/Q_Pred Std               36.789\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4000.96\n",
      "evaluation/Actions Mean                 0.0779992\n",
      "evaluation/Actions Std                  0.360848\n",
      "evaluation/Actions Max                  0.999986\n",
      "evaluation/Actions Min                 -0.999039\n",
      "time/backward_policy (s)                8.65659\n",
      "time/backward_zf1 (s)                  10.8365\n",
      "time/backward_zf2 (s)                  10.3498\n",
      "time/data sampling (s)                  1.48477\n",
      "time/data storing (s)                   0.083047\n",
      "time/evaluation sampling (s)            3.75353\n",
      "time/exploration sampling (s)           1.64763\n",
      "time/logging (s)                        0.0125294\n",
      "time/preback_alpha (s)                  0.00539179\n",
      "time/preback_policy (s)                10.0812\n",
      "time/preback_start (s)                  0.711142\n",
      "time/preback_zf (s)                    29.4069\n",
      "time/saving (s)                         3.296e-06\n",
      "time/training (s)                      11.0674\n",
      "time/epoch (s)                         88.0965\n",
      "time/total (s)                       4076.79\n",
      "Epoch                                  46\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:29:23.576537 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 47 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 250000\n",
      "trainer/ZF1 Loss                       25.5728\n",
      "trainer/ZF2 Loss                       25.267\n",
      "trainer/ZF Expert Reward                9.02988\n",
      "trainer/ZF Policy Reward                0.044614\n",
      "trainer/ZF CHI2 Term                   30.1876\n",
      "trainer/Policy Loss                  -103.818\n",
      "trainer/expert_lambda Loss              3.33546\n",
      "trainer/expert_lambda Value             9.702\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm              107.895\n",
      "trainer/Policy Param Norm              34.8891\n",
      "trainer/Zf1 Grad Norm                1301.46\n",
      "trainer/Zf1 Param Norm                 82.3126\n",
      "trainer/Zf2 Grad Norm                1488.07\n",
      "trainer/Zf2 Param Norm                 83.0519\n",
      "trainer/Z Expert Predictions Mean     214.811\n",
      "trainer/Z Expert Predictions Std       34.7724\n",
      "trainer/Z Expert Predictions Max      303.885\n",
      "trainer/Z Expert Predictions Min        7.40608\n",
      "trainer/Z Policy Predictions Mean     105.07\n",
      "trainer/Z Policy Predictions Std       60.4091\n",
      "trainer/Z Policy Predictions Max      209.28\n",
      "trainer/Z Policy Predictions Min     -100.194\n",
      "trainer/Z Expert Targets Mean         205.781\n",
      "trainer/Z Expert Targets Std           34.7564\n",
      "trainer/Z Expert Targets Max          293.029\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean         105.025\n",
      "trainer/Z Policy Targets Std           60.2758\n",
      "trainer/Z Policy Targets Max          220.285\n",
      "trainer/Z Policy Targets Min          -98.8834\n",
      "trainer/Log Pis Mean                   25.7727\n",
      "trainer/Log Pis Std                     6.07248\n",
      "trainer/Policy mu Mean                  0.020231\n",
      "trainer/Policy mu Std                   0.884081\n",
      "trainer/Policy log std Mean            -4.21691\n",
      "trainer/Policy log std Std              1.25201\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        244535\n",
      "exploration/num paths total           365\n",
      "evaluation/num steps total         422325\n",
      "evaluation/num paths total            489\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 4.22031\n",
      "evaluation/Rewards Std                  1.49683\n",
      "evaluation/Rewards Max                  6.93037\n",
      "evaluation/Rewards Min                 -2.60024\n",
      "evaluation/Returns Mean              4220.31\n",
      "evaluation/Returns Std                737.706\n",
      "evaluation/Returns Max               4701.25\n",
      "evaluation/Returns Min               2055.79\n",
      "evaluation/Estimation Bias Mean        88.2255\n",
      "evaluation/Estimation Bias Std        125.303\n",
      "evaluation/EB/Q_True Mean              40.7657\n",
      "evaluation/EB/Q_True Std              125.764\n",
      "evaluation/EB/Q_Pred Mean             128.991\n",
      "evaluation/EB/Q_Pred Std               27.3323\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4220.31\n",
      "evaluation/Actions Mean                 0.0335987\n",
      "evaluation/Actions Std                  0.343195\n",
      "evaluation/Actions Max                  0.995517\n",
      "evaluation/Actions Min                 -0.999551\n",
      "time/backward_policy (s)                8.66051\n",
      "time/backward_zf1 (s)                  10.8166\n",
      "time/backward_zf2 (s)                  10.3497\n",
      "time/data sampling (s)                  1.49417\n",
      "time/data storing (s)                   0.0842376\n",
      "time/evaluation sampling (s)            3.46253\n",
      "time/exploration sampling (s)           1.62598\n",
      "time/logging (s)                        0.0200649\n",
      "time/preback_alpha (s)                  0.00538345\n",
      "time/preback_policy (s)                10.0633\n",
      "time/preback_start (s)                  0.737154\n",
      "time/preback_zf (s)                    29.3103\n",
      "time/saving (s)                         4.5e-06\n",
      "time/training (s)                      10.9937\n",
      "time/epoch (s)                         87.6236\n",
      "time/total (s)                       4164.41\n",
      "Epoch                                  47\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:30:51.434582 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 48 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 255000\n",
      "trainer/ZF1 Loss                       44.5896\n",
      "trainer/ZF2 Loss                       38.173\n",
      "trainer/ZF Expert Reward               12.683\n",
      "trainer/ZF Policy Reward                3.31836\n",
      "trainer/ZF CHI2 Term                   46.3395\n",
      "trainer/Policy Loss                   -96.0309\n",
      "trainer/expert_lambda Loss              7.75452\n",
      "trainer/expert_lambda Value             9.74372\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               79.2708\n",
      "trainer/Policy Param Norm              35.0155\n",
      "trainer/Zf1 Grad Norm                1942.72\n",
      "trainer/Zf1 Param Norm                 82.5749\n",
      "trainer/Zf2 Grad Norm                1315.42\n",
      "trainer/Zf2 Param Norm                 83.3787\n",
      "trainer/Z Expert Predictions Mean     213.564\n",
      "trainer/Z Expert Predictions Std       34.2634\n",
      "trainer/Z Expert Predictions Max      300.416\n",
      "trainer/Z Expert Predictions Min       71.0497\n",
      "trainer/Z Policy Predictions Mean      97.6249\n",
      "trainer/Z Policy Predictions Std       61.1875\n",
      "trainer/Z Policy Predictions Max      180.481\n",
      "trainer/Z Policy Predictions Min      -89.668\n",
      "trainer/Z Expert Targets Mean         200.881\n",
      "trainer/Z Expert Targets Std           34.0601\n",
      "trainer/Z Expert Targets Max          278.297\n",
      "trainer/Z Expert Targets Min           55.2369\n",
      "trainer/Z Policy Targets Mean          94.3065\n",
      "trainer/Z Policy Targets Std           60.1658\n",
      "trainer/Z Policy Targets Max          187.919\n",
      "trainer/Z Policy Targets Min          -89.973\n",
      "trainer/Log Pis Mean                   25.6973\n",
      "trainer/Log Pis Std                     5.44973\n",
      "trainer/Policy mu Mean                  0.0463232\n",
      "trainer/Policy mu Std                   1.00167\n",
      "trainer/Policy log std Mean            -4.10153\n",
      "trainer/Policy log std Std              1.32116\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        251107\n",
      "exploration/num paths total           372\n",
      "evaluation/num steps total         432325\n",
      "evaluation/num paths total            499\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 4.00807\n",
      "evaluation/Rewards Std                  2.11598\n",
      "evaluation/Rewards Max                  7.27513\n",
      "evaluation/Rewards Min                 -2.07661\n",
      "evaluation/Returns Mean              4008.07\n",
      "evaluation/Returns Std               1819.05\n",
      "evaluation/Returns Max               4719.45\n",
      "evaluation/Returns Min              -1435.24\n",
      "evaluation/Estimation Bias Mean        74.8496\n",
      "evaluation/Estimation Bias Std        132.962\n",
      "evaluation/EB/Q_True Mean              42.5984\n",
      "evaluation/EB/Q_True Std              132.468\n",
      "evaluation/EB/Q_Pred Mean             117.448\n",
      "evaluation/EB/Q_Pred Std               37.5711\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4008.07\n",
      "evaluation/Actions Mean                 0.0689452\n",
      "evaluation/Actions Std                  0.380191\n",
      "evaluation/Actions Max                  0.99999\n",
      "evaluation/Actions Min                 -0.99588\n",
      "time/backward_policy (s)                8.80704\n",
      "time/backward_zf1 (s)                  10.9669\n",
      "time/backward_zf2 (s)                  10.4407\n",
      "time/data sampling (s)                  1.49114\n",
      "time/data storing (s)                   0.0857805\n",
      "time/evaluation sampling (s)            3.02972\n",
      "time/exploration sampling (s)           1.66639\n",
      "time/logging (s)                        0.0139845\n",
      "time/preback_alpha (s)                  0.00549729\n",
      "time/preback_policy (s)                 9.98783\n",
      "time/preback_start (s)                  0.753016\n",
      "time/preback_zf (s)                    29.3696\n",
      "time/saving (s)                         4.3e-06\n",
      "time/training (s)                      11.0126\n",
      "time/epoch (s)                         87.6303\n",
      "time/total (s)                       4252.05\n",
      "Epoch                                  48\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:32:16.692577 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 49 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 260000\n",
      "trainer/ZF1 Loss                       36.0328\n",
      "trainer/ZF2 Loss                       38.4829\n",
      "trainer/ZF Expert Reward                7.87344\n",
      "trainer/ZF Policy Reward                1.97601\n",
      "trainer/ZF CHI2 Term                   40.4822\n",
      "trainer/Policy Loss                   -93.228\n",
      "trainer/expert_lambda Loss              6.4204\n",
      "trainer/expert_lambda Value             9.79353\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               71.6472\n",
      "trainer/Policy Param Norm              35.1446\n",
      "trainer/Zf1 Grad Norm                1540.22\n",
      "trainer/Zf1 Param Norm                 82.8534\n",
      "trainer/Zf2 Grad Norm                2774.66\n",
      "trainer/Zf2 Param Norm                 83.6892\n",
      "trainer/Z Expert Predictions Mean     203.451\n",
      "trainer/Z Expert Predictions Std       34.1567\n",
      "trainer/Z Expert Predictions Max      286.842\n",
      "trainer/Z Expert Predictions Min        7.25753\n",
      "trainer/Z Policy Predictions Mean      92.3235\n",
      "trainer/Z Policy Predictions Std       64.2211\n",
      "trainer/Z Policy Predictions Max      186.726\n",
      "trainer/Z Policy Predictions Min     -100.464\n",
      "trainer/Z Expert Targets Mean         195.577\n",
      "trainer/Z Expert Targets Std           34.5798\n",
      "trainer/Z Expert Targets Max          279.179\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean          90.3475\n",
      "trainer/Z Policy Targets Std           63.7603\n",
      "trainer/Z Policy Targets Max          172.637\n",
      "trainer/Z Policy Targets Min          -97.8646\n",
      "trainer/Log Pis Mean                   26.1078\n",
      "trainer/Log Pis Std                     5.27898\n",
      "trainer/Policy mu Mean                  0.0192281\n",
      "trainer/Policy mu Std                   0.976366\n",
      "trainer/Policy log std Mean            -4.13175\n",
      "trainer/Policy log std Std              1.31042\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        254107\n",
      "exploration/num paths total           375\n",
      "evaluation/num steps total         441635\n",
      "evaluation/num paths total            509\n",
      "evaluation/path length Mean           931\n",
      "evaluation/path length Std            207\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            310\n",
      "evaluation/Rewards Mean                 4.51142\n",
      "evaluation/Rewards Std                  1.15981\n",
      "evaluation/Rewards Max                  7.29794\n",
      "evaluation/Rewards Min                 -1.40737\n",
      "evaluation/Returns Mean              4200.13\n",
      "evaluation/Returns Std                947.979\n",
      "evaluation/Returns Max               4744.02\n",
      "evaluation/Returns Min               1405.18\n",
      "evaluation/Estimation Bias Mean        75.7247\n",
      "evaluation/Estimation Bias Std        137.399\n",
      "evaluation/EB/Q_True Mean              44.9045\n",
      "evaluation/EB/Q_True Std              133.338\n",
      "evaluation/EB/Q_Pred Mean             120.629\n",
      "evaluation/EB/Q_Pred Std               22.3405\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4200.13\n",
      "evaluation/Actions Mean                 0.0441491\n",
      "evaluation/Actions Std                  0.313381\n",
      "evaluation/Actions Max                  0.994395\n",
      "evaluation/Actions Min                 -0.999829\n",
      "time/backward_policy (s)                8.14318\n",
      "time/backward_zf1 (s)                  10.4102\n",
      "time/backward_zf2 (s)                   9.84859\n",
      "time/data sampling (s)                  1.47699\n",
      "time/data storing (s)                   0.0843191\n",
      "time/evaluation sampling (s)            2.49398\n",
      "time/exploration sampling (s)           1.61475\n",
      "time/logging (s)                        0.0119075\n",
      "time/preback_alpha (s)                  0.00541915\n",
      "time/preback_policy (s)                10.119\n",
      "time/preback_start (s)                  0.743396\n",
      "time/preback_zf (s)                    29.1957\n",
      "time/saving (s)                         3.518e-06\n",
      "time/training (s)                      10.8881\n",
      "time/epoch (s)                         85.0356\n",
      "time/total (s)                       4337.09\n",
      "Epoch                                  49\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:33:43.638272 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 50 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 265000\n",
      "trainer/ZF1 Loss                       20.8419\n",
      "trainer/ZF2 Loss                       19.0847\n",
      "trainer/ZF Expert Reward                9.88006\n",
      "trainer/ZF Policy Reward               -0.257581\n",
      "trainer/ZF CHI2 Term                   25.3116\n",
      "trainer/Policy Loss                   -92.1581\n",
      "trainer/expert_lambda Loss              3.20542\n",
      "trainer/expert_lambda Value             9.84509\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               62.6455\n",
      "trainer/Policy Param Norm              35.2727\n",
      "trainer/Zf1 Grad Norm                1153.75\n",
      "trainer/Zf1 Param Norm                 83.1317\n",
      "trainer/Zf2 Grad Norm                 988.62\n",
      "trainer/Zf2 Param Norm                 84.0152\n",
      "trainer/Z Expert Predictions Mean     202.143\n",
      "trainer/Z Expert Predictions Std       30.2187\n",
      "trainer/Z Expert Predictions Max      273.962\n",
      "trainer/Z Expert Predictions Min      112.737\n",
      "trainer/Z Policy Predictions Mean      93.7986\n",
      "trainer/Z Policy Predictions Std       53.9773\n",
      "trainer/Z Policy Predictions Max      167.561\n",
      "trainer/Z Policy Predictions Min      -85.2937\n",
      "trainer/Z Expert Targets Mean         192.263\n",
      "trainer/Z Expert Targets Std           30.0854\n",
      "trainer/Z Expert Targets Max          263.387\n",
      "trainer/Z Expert Targets Min          105.838\n",
      "trainer/Z Policy Targets Mean          94.0562\n",
      "trainer/Z Policy Targets Std           53.272\n",
      "trainer/Z Policy Targets Max          171.92\n",
      "trainer/Z Policy Targets Min          -84.3869\n",
      "trainer/Log Pis Mean                   26.7848\n",
      "trainer/Log Pis Std                     5.26857\n",
      "trainer/Policy mu Mean                  0.0438401\n",
      "trainer/Policy mu Std                   0.98301\n",
      "trainer/Policy log std Mean            -4.23995\n",
      "trainer/Policy log std Std              1.2771\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        261370\n",
      "exploration/num paths total           383\n",
      "evaluation/num steps total         450865\n",
      "evaluation/num paths total            520\n",
      "evaluation/path length Mean           839.091\n",
      "evaluation/path length Std            312.283\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             67\n",
      "evaluation/Rewards Mean                 4.49888\n",
      "evaluation/Rewards Std                  1.35541\n",
      "evaluation/Rewards Max                  7.43556\n",
      "evaluation/Rewards Min                 -2.8552\n",
      "evaluation/Returns Mean              3774.97\n",
      "evaluation/Returns Std               1424.92\n",
      "evaluation/Returns Max               4713.06\n",
      "evaluation/Returns Min                199.606\n",
      "evaluation/Estimation Bias Mean        70.9875\n",
      "evaluation/Estimation Bias Std        140.227\n",
      "evaluation/EB/Q_True Mean              46.6524\n",
      "evaluation/EB/Q_True Std              137.595\n",
      "evaluation/EB/Q_Pred Mean             117.64\n",
      "evaluation/EB/Q_Pred Std               24.2157\n",
      "evaluation/Num Paths                   11\n",
      "evaluation/Average Returns           3774.97\n",
      "evaluation/Actions Mean                 0.0415401\n",
      "evaluation/Actions Std                  0.326504\n",
      "evaluation/Actions Max                  0.999994\n",
      "evaluation/Actions Min                 -0.998888\n",
      "time/backward_policy (s)                8.464\n",
      "time/backward_zf1 (s)                  10.6311\n",
      "time/backward_zf2 (s)                  10.1319\n",
      "time/data sampling (s)                  1.49989\n",
      "time/data storing (s)                   0.0845473\n",
      "time/evaluation sampling (s)            3.6753\n",
      "time/exploration sampling (s)           1.64979\n",
      "time/logging (s)                        0.0119967\n",
      "time/preback_alpha (s)                  0.00534804\n",
      "time/preback_policy (s)                 9.91668\n",
      "time/preback_start (s)                  0.727832\n",
      "time/preback_zf (s)                    29.1412\n",
      "time/saving (s)                         3.31e-06\n",
      "time/training (s)                      10.7902\n",
      "time/epoch (s)                         86.7298\n",
      "time/total (s)                       4423.82\n",
      "Epoch                                  50\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:35:07.034004 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 51 finished\n",
      "---------------------------------  --------------\n",
      "replay_buffer/size                 270000\n",
      "trainer/ZF1 Loss                       27.4357\n",
      "trainer/ZF2 Loss                       26.5157\n",
      "trainer/ZF Expert Reward               12.1984\n",
      "trainer/ZF Policy Reward                4.22311\n",
      "trainer/ZF CHI2 Term                   31.2349\n",
      "trainer/Policy Loss                   -88.4358\n",
      "trainer/expert_lambda Loss              6.84867\n",
      "trainer/expert_lambda Value             9.89859\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               70.4574\n",
      "trainer/Policy Param Norm              35.4055\n",
      "trainer/Zf1 Grad Norm                1819.66\n",
      "trainer/Zf1 Param Norm                 83.433\n",
      "trainer/Zf2 Grad Norm                1057.21\n",
      "trainer/Zf2 Param Norm                 84.3203\n",
      "trainer/Z Expert Predictions Mean     199.508\n",
      "trainer/Z Expert Predictions Std       29.4508\n",
      "trainer/Z Expert Predictions Max      269.482\n",
      "trainer/Z Expert Predictions Min      117.679\n",
      "trainer/Z Policy Predictions Mean      91.8855\n",
      "trainer/Z Policy Predictions Std       54.4958\n",
      "trainer/Z Policy Predictions Max      176.338\n",
      "trainer/Z Policy Predictions Min     -100.869\n",
      "trainer/Z Expert Targets Mean         187.31\n",
      "trainer/Z Expert Targets Std           29.4989\n",
      "trainer/Z Expert Targets Max          253.687\n",
      "trainer/Z Expert Targets Min          104.051\n",
      "trainer/Z Policy Targets Mean          87.6624\n",
      "trainer/Z Policy Targets Std           52.1936\n",
      "trainer/Z Policy Targets Max          165.757\n",
      "trainer/Z Policy Targets Min         -101.055\n",
      "trainer/Log Pis Mean                   25.411\n",
      "trainer/Log Pis Std                     5.95334\n",
      "trainer/Policy mu Mean                  0.0548925\n",
      "trainer/Policy mu Std                   0.922054\n",
      "trainer/Policy log std Mean            -4.09223\n",
      "trainer/Policy log std Std              1.25828\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        264370\n",
      "exploration/num paths total           386\n",
      "evaluation/num steps total         460865\n",
      "evaluation/num paths total            530\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 4.47881\n",
      "evaluation/Rewards Std                  0.998316\n",
      "evaluation/Rewards Max                  7.37637\n",
      "evaluation/Rewards Min                 -1.37667\n",
      "evaluation/Returns Mean              4478.81\n",
      "evaluation/Returns Std                116.313\n",
      "evaluation/Returns Max               4614.26\n",
      "evaluation/Returns Min               4223.06\n",
      "evaluation/Estimation Bias Mean        74.0851\n",
      "evaluation/Estimation Bias Std        126.003\n",
      "evaluation/EB/Q_True Mean              40.7914\n",
      "evaluation/EB/Q_True Std              125.916\n",
      "evaluation/EB/Q_Pred Mean             114.877\n",
      "evaluation/EB/Q_Pred Std               19.2298\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4478.81\n",
      "evaluation/Actions Mean                 0.0486263\n",
      "evaluation/Actions Std                  0.313796\n",
      "evaluation/Actions Max                  0.993014\n",
      "evaluation/Actions Min                 -0.995863\n",
      "time/backward_policy (s)                7.58849\n",
      "time/backward_zf1 (s)                   9.5394\n",
      "time/backward_zf2 (s)                   9.20191\n",
      "time/data sampling (s)                  1.3694\n",
      "time/data storing (s)                   0.079484\n",
      "time/evaluation sampling (s)            3.55017\n",
      "time/exploration sampling (s)           1.56509\n",
      "time/logging (s)                        0.0189064\n",
      "time/preback_alpha (s)                  0.0049765\n",
      "time/preback_policy (s)                10.0845\n",
      "time/preback_start (s)                  0.660417\n",
      "time/preback_zf (s)                    28.6819\n",
      "time/saving (s)                         5.17e-06\n",
      "time/training (s)                      10.853\n",
      "time/epoch (s)                         83.1977\n",
      "time/total (s)                       4507.02\n",
      "Epoch                                  51\n",
      "---------------------------------  --------------\n",
      "2024-11-07 18:36:34.686064 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 52 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 275000\n",
      "trainer/ZF1 Loss                       22.5446\n",
      "trainer/ZF2 Loss                       20.3856\n",
      "trainer/ZF Expert Reward               10.8776\n",
      "trainer/ZF Policy Reward                0.956448\n",
      "trainer/ZF CHI2 Term                   26.6977\n",
      "trainer/Policy Loss                   -82.4544\n",
      "trainer/expert_lambda Loss              2.91328\n",
      "trainer/expert_lambda Value             9.95451\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               54.7343\n",
      "trainer/Policy Param Norm              35.527\n",
      "trainer/Zf1 Grad Norm                 896.094\n",
      "trainer/Zf1 Param Norm                 83.7245\n",
      "trainer/Zf2 Grad Norm                 852.259\n",
      "trainer/Zf2 Param Norm                 84.6283\n",
      "trainer/Z Expert Predictions Mean     194.554\n",
      "trainer/Z Expert Predictions Std       27.2848\n",
      "trainer/Z Expert Predictions Max      260.382\n",
      "trainer/Z Expert Predictions Min       58.0449\n",
      "trainer/Z Policy Predictions Mean      83.4687\n",
      "trainer/Z Policy Predictions Std       55.3984\n",
      "trainer/Z Policy Predictions Max      171.694\n",
      "trainer/Z Policy Predictions Min      -99.3067\n",
      "trainer/Z Expert Targets Mean         183.676\n",
      "trainer/Z Expert Targets Std           26.8226\n",
      "trainer/Z Expert Targets Max          251.358\n",
      "trainer/Z Expert Targets Min           46.5498\n",
      "trainer/Z Policy Targets Mean          82.5123\n",
      "trainer/Z Policy Targets Std           54.7205\n",
      "trainer/Z Policy Targets Max          161.639\n",
      "trainer/Z Policy Targets Min          -99.5586\n",
      "trainer/Log Pis Mean                   25.5685\n",
      "trainer/Log Pis Std                     4.9947\n",
      "trainer/Policy mu Mean                  0.0460739\n",
      "trainer/Policy mu Std                   1.02474\n",
      "trainer/Policy log std Mean            -4.05676\n",
      "trainer/Policy log std Std              1.2926\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        271370\n",
      "exploration/num paths total           393\n",
      "evaluation/num steps total         470865\n",
      "evaluation/num paths total            540\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 4.63798\n",
      "evaluation/Rewards Std                  1.04678\n",
      "evaluation/Rewards Max                  7.42252\n",
      "evaluation/Rewards Min                 -1.41644\n",
      "evaluation/Returns Mean              4637.98\n",
      "evaluation/Returns Std                112.45\n",
      "evaluation/Returns Max               4788.65\n",
      "evaluation/Returns Min               4430.45\n",
      "evaluation/Estimation Bias Mean        70.5101\n",
      "evaluation/Estimation Bias Std        134.616\n",
      "evaluation/EB/Q_True Mean              43.0909\n",
      "evaluation/EB/Q_True Std              133.226\n",
      "evaluation/EB/Q_Pred Mean             113.601\n",
      "evaluation/EB/Q_Pred Std               20.1798\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4637.98\n",
      "evaluation/Actions Mean                 0.0392632\n",
      "evaluation/Actions Std                  0.310117\n",
      "evaluation/Actions Max                  0.995794\n",
      "evaluation/Actions Min                 -0.992013\n",
      "time/backward_policy (s)                8.53565\n",
      "time/backward_zf1 (s)                  10.7316\n",
      "time/backward_zf2 (s)                  10.2446\n",
      "time/data sampling (s)                  1.5024\n",
      "time/data storing (s)                   0.0829689\n",
      "time/evaluation sampling (s)            3.36005\n",
      "time/exploration sampling (s)           1.63055\n",
      "time/logging (s)                        0.0175516\n",
      "time/preback_alpha (s)                  0.00532871\n",
      "time/preback_policy (s)                10.1161\n",
      "time/preback_start (s)                  0.72385\n",
      "time/preback_zf (s)                    29.449\n",
      "time/saving (s)                         5.117e-06\n",
      "time/training (s)                      11.0294\n",
      "time/epoch (s)                         87.4291\n",
      "time/total (s)                       4594.45\n",
      "Epoch                                  52\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:38:01.693957 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 53 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 280000\n",
      "trainer/ZF1 Loss                       24.0447\n",
      "trainer/ZF2 Loss                       20.764\n",
      "trainer/ZF Expert Reward               10.556\n",
      "trainer/ZF Policy Reward                1.55977\n",
      "trainer/ZF CHI2 Term                   27.1791\n",
      "trainer/Policy Loss                   -89.9094\n",
      "trainer/expert_lambda Loss              3.78628\n",
      "trainer/expert_lambda Value            10.0188\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               68.3377\n",
      "trainer/Policy Param Norm              35.6431\n",
      "trainer/Zf1 Grad Norm                1128.12\n",
      "trainer/Zf1 Param Norm                 84.0231\n",
      "trainer/Zf2 Grad Norm                1017.78\n",
      "trainer/Zf2 Param Norm                 84.9363\n",
      "trainer/Z Expert Predictions Mean     192.309\n",
      "trainer/Z Expert Predictions Std       28.7148\n",
      "trainer/Z Expert Predictions Max      263.81\n",
      "trainer/Z Expert Predictions Min       56.7772\n",
      "trainer/Z Policy Predictions Mean      91.8375\n",
      "trainer/Z Policy Predictions Std       46.3642\n",
      "trainer/Z Policy Predictions Max      184.841\n",
      "trainer/Z Policy Predictions Min      -85.7765\n",
      "trainer/Z Expert Targets Mean         181.753\n",
      "trainer/Z Expert Targets Std           28.353\n",
      "trainer/Z Expert Targets Max          251.436\n",
      "trainer/Z Expert Targets Min           48.6009\n",
      "trainer/Z Policy Targets Mean          90.2778\n",
      "trainer/Z Policy Targets Std           45.1154\n",
      "trainer/Z Policy Targets Max          158.796\n",
      "trainer/Z Policy Targets Min          -85.5225\n",
      "trainer/Log Pis Mean                   26.4155\n",
      "trainer/Log Pis Std                     5.17906\n",
      "trainer/Policy mu Mean                  0.0185959\n",
      "trainer/Policy mu Std                   0.873\n",
      "trainer/Policy log std Mean            -4.28978\n",
      "trainer/Policy log std Std              1.15751\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        274370\n",
      "exploration/num paths total           396\n",
      "evaluation/num steps total         480449\n",
      "evaluation/num paths total            550\n",
      "evaluation/path length Mean           958.4\n",
      "evaluation/path length Std             99.9872\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min            668\n",
      "evaluation/Rewards Mean                 4.51982\n",
      "evaluation/Rewards Std                  1.15614\n",
      "evaluation/Rewards Max                  7.49528\n",
      "evaluation/Rewards Min                 -3.49269\n",
      "evaluation/Returns Mean              4331.8\n",
      "evaluation/Returns Std                569.021\n",
      "evaluation/Returns Max               4731.9\n",
      "evaluation/Returns Min               2689.79\n",
      "evaluation/Estimation Bias Mean        63.7058\n",
      "evaluation/Estimation Bias Std        132.977\n",
      "evaluation/EB/Q_True Mean              43.009\n",
      "evaluation/EB/Q_True Std              129.768\n",
      "evaluation/EB/Q_Pred Mean             106.715\n",
      "evaluation/EB/Q_Pred Std               20.301\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4331.8\n",
      "evaluation/Actions Mean                 0.0451875\n",
      "evaluation/Actions Std                  0.311838\n",
      "evaluation/Actions Max                  0.996743\n",
      "evaluation/Actions Min                 -0.999879\n",
      "time/backward_policy (s)                8.27669\n",
      "time/backward_zf1 (s)                  10.3609\n",
      "time/backward_zf2 (s)                   9.96499\n",
      "time/data sampling (s)                  1.45136\n",
      "time/data storing (s)                   0.0829575\n",
      "time/evaluation sampling (s)            4.01452\n",
      "time/exploration sampling (s)           1.61908\n",
      "time/logging (s)                        0.0116373\n",
      "time/preback_alpha (s)                  0.00525764\n",
      "time/preback_policy (s)                10.0631\n",
      "time/preback_start (s)                  0.716848\n",
      "time/preback_zf (s)                    29.2477\n",
      "time/saving (s)                         3.025e-06\n",
      "time/training (s)                      10.9697\n",
      "time/epoch (s)                         86.7847\n",
      "time/total (s)                       4681.24\n",
      "Epoch                                  53\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:39:28.659955 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 54 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 285000\n",
      "trainer/ZF1 Loss                       19.0538\n",
      "trainer/ZF2 Loss                       21.049\n",
      "trainer/ZF Expert Reward                8.11456\n",
      "trainer/ZF Policy Reward                0.452524\n",
      "trainer/ZF CHI2 Term                   24.1522\n",
      "trainer/Policy Loss                   -81.5687\n",
      "trainer/expert_lambda Loss              6.16773\n",
      "trainer/expert_lambda Value            10.0872\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               49.1015\n",
      "trainer/Policy Param Norm              35.7495\n",
      "trainer/Zf1 Grad Norm                1339.52\n",
      "trainer/Zf1 Param Norm                 84.3027\n",
      "trainer/Zf2 Grad Norm                1518.09\n",
      "trainer/Zf2 Param Norm                 85.2661\n",
      "trainer/Z Expert Predictions Mean     183.776\n",
      "trainer/Z Expert Predictions Std       28.4074\n",
      "trainer/Z Expert Predictions Max      263.618\n",
      "trainer/Z Expert Predictions Min       58.9908\n",
      "trainer/Z Policy Predictions Mean      82.6514\n",
      "trainer/Z Policy Predictions Std       51.9986\n",
      "trainer/Z Policy Predictions Max      147.391\n",
      "trainer/Z Policy Predictions Min      -97.3085\n",
      "trainer/Z Expert Targets Mean         175.661\n",
      "trainer/Z Expert Targets Std           29.0356\n",
      "trainer/Z Expert Targets Max          254.191\n",
      "trainer/Z Expert Targets Min           43.5094\n",
      "trainer/Z Policy Targets Mean          82.1988\n",
      "trainer/Z Policy Targets Std           51.3832\n",
      "trainer/Z Policy Targets Max          145.734\n",
      "trainer/Z Policy Targets Min         -101.116\n",
      "trainer/Log Pis Mean                   25.7133\n",
      "trainer/Log Pis Std                     5.16662\n",
      "trainer/Policy mu Mean                  0.0246531\n",
      "trainer/Policy mu Std                   0.960075\n",
      "trainer/Policy log std Mean            -4.12135\n",
      "trainer/Policy log std Std              1.28715\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        281370\n",
      "exploration/num paths total           403\n",
      "evaluation/num steps total         490449\n",
      "evaluation/num paths total            560\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 4.60575\n",
      "evaluation/Rewards Std                  0.960364\n",
      "evaluation/Rewards Max                  7.07192\n",
      "evaluation/Rewards Min                 -1.36164\n",
      "evaluation/Returns Mean              4605.75\n",
      "evaluation/Returns Std                 74.0199\n",
      "evaluation/Returns Max               4709.92\n",
      "evaluation/Returns Min               4481.36\n",
      "evaluation/Estimation Bias Mean        67.6724\n",
      "evaluation/Estimation Bias Std        130.416\n",
      "evaluation/EB/Q_True Mean              42.0336\n",
      "evaluation/EB/Q_True Std              129.457\n",
      "evaluation/EB/Q_Pred Mean             109.706\n",
      "evaluation/EB/Q_Pred Std               19.1441\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4605.75\n",
      "evaluation/Actions Mean                 0.0434417\n",
      "evaluation/Actions Std                  0.308828\n",
      "evaluation/Actions Max                  0.997638\n",
      "evaluation/Actions Min                 -0.993462\n",
      "time/backward_policy (s)                8.49958\n",
      "time/backward_zf1 (s)                  10.6723\n",
      "time/backward_zf2 (s)                  10.2346\n",
      "time/data sampling (s)                  1.47317\n",
      "time/data storing (s)                   0.0809797\n",
      "time/evaluation sampling (s)            3.76138\n",
      "time/exploration sampling (s)           1.62029\n",
      "time/logging (s)                        0.0126335\n",
      "time/preback_alpha (s)                  0.00524111\n",
      "time/preback_policy (s)                 9.80504\n",
      "time/preback_start (s)                  0.718286\n",
      "time/preback_zf (s)                    29.2035\n",
      "time/saving (s)                         6.713e-06\n",
      "time/training (s)                      10.6635\n",
      "time/epoch (s)                         86.7505\n",
      "time/total (s)                       4767.99\n",
      "Epoch                                  54\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:40:56.333372 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 55 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 290000\n",
      "trainer/ZF1 Loss                       26.5298\n",
      "trainer/ZF2 Loss                       26.7444\n",
      "trainer/ZF Expert Reward               10.9933\n",
      "trainer/ZF Policy Reward                2.47919\n",
      "trainer/ZF CHI2 Term                   31.172\n",
      "trainer/Policy Loss                   -81.1247\n",
      "trainer/expert_lambda Loss              4.9519\n",
      "trainer/expert_lambda Value            10.1473\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               54.8419\n",
      "trainer/Policy Param Norm              35.8588\n",
      "trainer/Zf1 Grad Norm                1489.85\n",
      "trainer/Zf1 Param Norm                 84.6423\n",
      "trainer/Zf2 Grad Norm                1211.15\n",
      "trainer/Zf2 Param Norm                 85.6119\n",
      "trainer/Z Expert Predictions Mean     187.683\n",
      "trainer/Z Expert Predictions Std       31.1698\n",
      "trainer/Z Expert Predictions Max      264.884\n",
      "trainer/Z Expert Predictions Min       18.9179\n",
      "trainer/Z Policy Predictions Mean      83.1938\n",
      "trainer/Z Policy Predictions Std       51.5725\n",
      "trainer/Z Policy Predictions Max      177.139\n",
      "trainer/Z Policy Predictions Min      -92.8555\n",
      "trainer/Z Expert Targets Mean         176.69\n",
      "trainer/Z Expert Targets Std           30.9696\n",
      "trainer/Z Expert Targets Max          252.765\n",
      "trainer/Z Expert Targets Min            0\n",
      "trainer/Z Policy Targets Mean          80.7146\n",
      "trainer/Z Policy Targets Std           49.9668\n",
      "trainer/Z Policy Targets Max          158.804\n",
      "trainer/Z Policy Targets Min          -93.1352\n",
      "trainer/Log Pis Mean                   26.7434\n",
      "trainer/Log Pis Std                     5.32707\n",
      "trainer/Policy mu Mean                  0.0441786\n",
      "trainer/Policy mu Std                   1.03165\n",
      "trainer/Policy log std Mean            -4.22314\n",
      "trainer/Policy log std Std              1.28974\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        284370\n",
      "exploration/num paths total           406\n",
      "evaluation/num steps total         499196\n",
      "evaluation/num paths total            570\n",
      "evaluation/path length Mean           874.7\n",
      "evaluation/path length Std            293.238\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             33\n",
      "evaluation/Rewards Mean                 4.54428\n",
      "evaluation/Rewards Std                  1.09153\n",
      "evaluation/Rewards Max                  7.2084\n",
      "evaluation/Rewards Min                 -1.37862\n",
      "evaluation/Returns Mean              3974.88\n",
      "evaluation/Returns Std               1397.01\n",
      "evaluation/Returns Max               4664.47\n",
      "evaluation/Returns Min                 32.7658\n",
      "evaluation/Estimation Bias Mean        52.6751\n",
      "evaluation/Estimation Bias Std        143.434\n",
      "evaluation/EB/Q_True Mean              48.9975\n",
      "evaluation/EB/Q_True Std              140.234\n",
      "evaluation/EB/Q_Pred Mean             101.673\n",
      "evaluation/EB/Q_Pred Std               18.591\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3974.88\n",
      "evaluation/Actions Mean                 0.0443771\n",
      "evaluation/Actions Std                  0.309119\n",
      "evaluation/Actions Max                  0.996076\n",
      "evaluation/Actions Min                 -0.989704\n",
      "time/backward_policy (s)                8.71677\n",
      "time/backward_zf1 (s)                  10.882\n",
      "time/backward_zf2 (s)                  10.4202\n",
      "time/data sampling (s)                  1.45174\n",
      "time/data storing (s)                   0.0833355\n",
      "time/evaluation sampling (s)            3.8936\n",
      "time/exploration sampling (s)           1.62407\n",
      "time/logging (s)                        0.0155537\n",
      "time/preback_alpha (s)                  0.00533809\n",
      "time/preback_policy (s)                 9.69716\n",
      "time/preback_start (s)                  0.716999\n",
      "time/preback_zf (s)                    29.2216\n",
      "time/saving (s)                         4.558e-06\n",
      "time/training (s)                      10.7303\n",
      "time/epoch (s)                         87.4587\n",
      "time/total (s)                       4855.45\n",
      "Epoch                                  55\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:42:24.049695 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 56 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 295000\n",
      "trainer/ZF1 Loss                       23.9897\n",
      "trainer/ZF2 Loss                       20.0673\n",
      "trainer/ZF Expert Reward               10.1144\n",
      "trainer/ZF Policy Reward                1.97281\n",
      "trainer/ZF CHI2 Term                   26.3725\n",
      "trainer/Policy Loss                   -77.5227\n",
      "trainer/expert_lambda Loss              3.46122\n",
      "trainer/expert_lambda Value            10.2116\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               64.3052\n",
      "trainer/Policy Param Norm              35.9767\n",
      "trainer/Zf1 Grad Norm                1290.33\n",
      "trainer/Zf1 Param Norm                 84.9576\n",
      "trainer/Zf2 Grad Norm                 854.673\n",
      "trainer/Zf2 Param Norm                 85.93\n",
      "trainer/Z Expert Predictions Mean     184.66\n",
      "trainer/Z Expert Predictions Std       26.9341\n",
      "trainer/Z Expert Predictions Max      257.692\n",
      "trainer/Z Expert Predictions Min       50.6689\n",
      "trainer/Z Policy Predictions Mean      78.8419\n",
      "trainer/Z Policy Predictions Std       51.1395\n",
      "trainer/Z Policy Predictions Max      160.327\n",
      "trainer/Z Policy Predictions Min      -86.9262\n",
      "trainer/Z Expert Targets Mean         174.545\n",
      "trainer/Z Expert Targets Std           27.1535\n",
      "trainer/Z Expert Targets Max          246.054\n",
      "trainer/Z Expert Targets Min           40.043\n",
      "trainer/Z Policy Targets Mean          76.8691\n",
      "trainer/Z Policy Targets Std           50.1444\n",
      "trainer/Z Policy Targets Max          151.865\n",
      "trainer/Z Policy Targets Min          -90.4119\n",
      "trainer/Log Pis Mean                   26.4033\n",
      "trainer/Log Pis Std                     5.08961\n",
      "trainer/Policy mu Mean                  0.0419894\n",
      "trainer/Policy mu Std                   0.941634\n",
      "trainer/Policy log std Mean            -4.2201\n",
      "trainer/Policy log std Std              1.30959\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        291524\n",
      "exploration/num paths total           414\n",
      "evaluation/num steps total         509196\n",
      "evaluation/num paths total            580\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 4.51392\n",
      "evaluation/Rewards Std                  1.0197\n",
      "evaluation/Rewards Max                  6.99851\n",
      "evaluation/Rewards Min                 -2.51715\n",
      "evaluation/Returns Mean              4513.92\n",
      "evaluation/Returns Std                106.682\n",
      "evaluation/Returns Max               4667.21\n",
      "evaluation/Returns Min               4341.48\n",
      "evaluation/Estimation Bias Mean        61.2198\n",
      "evaluation/Estimation Bias Std        124.48\n",
      "evaluation/EB/Q_True Mean              40.1928\n",
      "evaluation/EB/Q_True Std              124.032\n",
      "evaluation/EB/Q_Pred Mean             101.413\n",
      "evaluation/EB/Q_Pred Std               17.4515\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           4513.92\n",
      "evaluation/Actions Mean                 0.0438794\n",
      "evaluation/Actions Std                  0.314294\n",
      "evaluation/Actions Max                  0.999733\n",
      "evaluation/Actions Min                 -0.997727\n",
      "time/backward_policy (s)                8.54914\n",
      "time/backward_zf1 (s)                  10.746\n",
      "time/backward_zf2 (s)                  10.3447\n",
      "time/data sampling (s)                  1.507\n",
      "time/data storing (s)                   0.0833342\n",
      "time/evaluation sampling (s)            3.93043\n",
      "time/exploration sampling (s)           1.6213\n",
      "time/logging (s)                        0.0127232\n",
      "time/preback_alpha (s)                  0.00527585\n",
      "time/preback_policy (s)                 9.90913\n",
      "time/preback_start (s)                  0.705167\n",
      "time/preback_zf (s)                    29.2645\n",
      "time/saving (s)                         2.846e-06\n",
      "time/training (s)                      10.8183\n",
      "time/epoch (s)                         87.4969\n",
      "time/total (s)                       4942.95\n",
      "Epoch                                  56\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:43:51.371786 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 57 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 300000\n",
      "trainer/ZF1 Loss                       22.6985\n",
      "trainer/ZF2 Loss                       31.9534\n",
      "trainer/ZF Expert Reward                8.59786\n",
      "trainer/ZF Policy Reward                0.0447884\n",
      "trainer/ZF CHI2 Term                   31.8771\n",
      "trainer/Policy Loss                   -78.6074\n",
      "trainer/expert_lambda Loss              6.16686\n",
      "trainer/expert_lambda Value            10.2783\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               70.3908\n",
      "trainer/Policy Param Norm              36.0888\n",
      "trainer/Zf1 Grad Norm                1264.99\n",
      "trainer/Zf1 Param Norm                 85.2641\n",
      "trainer/Zf2 Grad Norm                2309.82\n",
      "trainer/Zf2 Param Norm                 86.2686\n",
      "trainer/Z Expert Predictions Mean     179.3\n",
      "trainer/Z Expert Predictions Std       25.4852\n",
      "trainer/Z Expert Predictions Max      276.165\n",
      "trainer/Z Expert Predictions Min      114.639\n",
      "trainer/Z Policy Predictions Mean      78.5927\n",
      "trainer/Z Policy Predictions Std       48.33\n",
      "trainer/Z Policy Predictions Max      143.609\n",
      "trainer/Z Policy Predictions Min      -98.5402\n",
      "trainer/Z Expert Targets Mean         170.702\n",
      "trainer/Z Expert Targets Std           25.6838\n",
      "trainer/Z Expert Targets Max          263.831\n",
      "trainer/Z Expert Targets Min          107.224\n",
      "trainer/Z Policy Targets Mean          78.5479\n",
      "trainer/Z Policy Targets Std           48.0681\n",
      "trainer/Z Policy Targets Max          164.665\n",
      "trainer/Z Policy Targets Min          -96.4959\n",
      "trainer/Log Pis Mean                   26.4219\n",
      "trainer/Log Pis Std                     5.17652\n",
      "trainer/Policy mu Mean                  0.0544159\n",
      "trainer/Policy mu Std                   0.928213\n",
      "trainer/Policy log std Mean            -4.25706\n",
      "trainer/Policy log std Std              1.27507\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        294094\n",
      "exploration/num paths total           418\n",
      "evaluation/num steps total         519196\n",
      "evaluation/num paths total            590\n",
      "evaluation/path length Mean          1000\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min           1000\n",
      "evaluation/Rewards Mean                 3.07348\n",
      "evaluation/Rewards Std                  2.66924\n",
      "evaluation/Rewards Max                  7.19699\n",
      "evaluation/Rewards Min                 -2.80024\n",
      "evaluation/Returns Mean              3073.48\n",
      "evaluation/Returns Std               1869.27\n",
      "evaluation/Returns Max               4723.51\n",
      "evaluation/Returns Min              -1669.75\n",
      "evaluation/Estimation Bias Mean        90.3466\n",
      "evaluation/Estimation Bias Std         78.3823\n",
      "evaluation/EB/Q_True Mean             -16.7237\n",
      "evaluation/EB/Q_True Std               52.321\n",
      "evaluation/EB/Q_Pred Mean              73.6229\n",
      "evaluation/EB/Q_Pred Std               51.1145\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3073.48\n",
      "evaluation/Actions Mean                 0.0132084\n",
      "evaluation/Actions Std                  0.459496\n",
      "evaluation/Actions Max                  0.999993\n",
      "evaluation/Actions Min                 -0.999936\n",
      "time/backward_policy (s)                8.41534\n",
      "time/backward_zf1 (s)                  10.5805\n",
      "time/backward_zf2 (s)                  10.1312\n",
      "time/data sampling (s)                  1.43426\n",
      "time/data storing (s)                   0.085244\n",
      "time/evaluation sampling (s)            4.26339\n",
      "time/exploration sampling (s)           1.63226\n",
      "time/logging (s)                        0.01885\n",
      "time/preback_alpha (s)                  0.00524152\n",
      "time/preback_policy (s)                 9.86555\n",
      "time/preback_start (s)                  0.700342\n",
      "time/preback_zf (s)                    29.1972\n",
      "time/saving (s)                         3.311e-06\n",
      "time/training (s)                      10.7838\n",
      "time/epoch (s)                         87.1131\n",
      "time/total (s)                       5030.06\n",
      "Epoch                                  57\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:45:16.897786 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 58 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 305000\n",
      "trainer/ZF1 Loss                       31.4723\n",
      "trainer/ZF2 Loss                       31.1629\n",
      "trainer/ZF Expert Reward               12.9086\n",
      "trainer/ZF Policy Reward                3.17832\n",
      "trainer/ZF CHI2 Term                   36.4576\n",
      "trainer/Policy Loss                   -80.2133\n",
      "trainer/expert_lambda Loss              6.53761\n",
      "trainer/expert_lambda Value            10.3437\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               56.4646\n",
      "trainer/Policy Param Norm              36.2027\n",
      "trainer/Zf1 Grad Norm                1408.13\n",
      "trainer/Zf1 Param Norm                 85.6058\n",
      "trainer/Zf2 Grad Norm                1386.6\n",
      "trainer/Zf2 Param Norm                 86.6238\n",
      "trainer/Z Expert Predictions Mean     182.168\n",
      "trainer/Z Expert Predictions Std       26.4958\n",
      "trainer/Z Expert Predictions Max      253.271\n",
      "trainer/Z Expert Predictions Min      108.848\n",
      "trainer/Z Policy Predictions Mean      81.9699\n",
      "trainer/Z Policy Predictions Std       43.3848\n",
      "trainer/Z Policy Predictions Max      150.263\n",
      "trainer/Z Policy Predictions Min      -93.1773\n",
      "trainer/Z Expert Targets Mean         169.259\n",
      "trainer/Z Expert Targets Std           26.18\n",
      "trainer/Z Expert Targets Max          239.863\n",
      "trainer/Z Expert Targets Min           95.5556\n",
      "trainer/Z Policy Targets Mean          78.7916\n",
      "trainer/Z Policy Targets Std           42.8778\n",
      "trainer/Z Policy Targets Max          159.583\n",
      "trainer/Z Policy Targets Min          -94.1508\n",
      "trainer/Log Pis Mean                   25.7716\n",
      "trainer/Log Pis Std                     5.50204\n",
      "trainer/Policy mu Mean                  0.0558569\n",
      "trainer/Policy mu Std                   0.756438\n",
      "trainer/Policy log std Mean            -4.31537\n",
      "trainer/Policy log std Std              1.12374\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        300964\n",
      "exploration/num paths total           425\n",
      "evaluation/num steps total         527732\n",
      "evaluation/num paths total            600\n",
      "evaluation/path length Mean           853.6\n",
      "evaluation/path length Std            310.939\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             34\n",
      "evaluation/Rewards Mean                 3.9795\n",
      "evaluation/Rewards Std                  1.87685\n",
      "evaluation/Rewards Max                  7.1404\n",
      "evaluation/Rewards Min                 -2.56364\n",
      "evaluation/Returns Mean              3396.9\n",
      "evaluation/Returns Std               1805.68\n",
      "evaluation/Returns Max               4792.41\n",
      "evaluation/Returns Min                 24.2321\n",
      "evaluation/Estimation Bias Mean        41.4933\n",
      "evaluation/Estimation Bias Std        133.953\n",
      "evaluation/EB/Q_True Mean              47.0153\n",
      "evaluation/EB/Q_True Std              133.539\n",
      "evaluation/EB/Q_Pred Mean              88.5086\n",
      "evaluation/EB/Q_Pred Std               21.5373\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3396.9\n",
      "evaluation/Actions Mean                 0.0477532\n",
      "evaluation/Actions Std                  0.361456\n",
      "evaluation/Actions Max                  0.997468\n",
      "evaluation/Actions Min                 -0.999917\n",
      "time/backward_policy (s)                8.35565\n",
      "time/backward_zf1 (s)                  10.3502\n",
      "time/backward_zf2 (s)                   9.98381\n",
      "time/data sampling (s)                  1.39365\n",
      "time/data storing (s)                   0.080309\n",
      "time/evaluation sampling (s)            3.58035\n",
      "time/exploration sampling (s)           1.60922\n",
      "time/logging (s)                        0.0172042\n",
      "time/preback_alpha (s)                  0.00515505\n",
      "time/preback_policy (s)                 9.67884\n",
      "time/preback_start (s)                  0.683342\n",
      "time/preback_zf (s)                    28.9212\n",
      "time/saving (s)                         3.543e-06\n",
      "time/training (s)                      10.6553\n",
      "time/epoch (s)                         85.3143\n",
      "time/total (s)                       5115.38\n",
      "Epoch                                  58\n",
      "---------------------------------  ---------------\n",
      "2024-11-07 18:46:44.767386 +0330 | [ant_2024_11_07_17_19_50_0000--s-0] Epoch 59 finished\n",
      "---------------------------------  ---------------\n",
      "replay_buffer/size                 310000\n",
      "trainer/ZF1 Loss                       17.8459\n",
      "trainer/ZF2 Loss                       27.0425\n",
      "trainer/ZF Expert Reward                8.60825\n",
      "trainer/ZF Policy Reward                0.15971\n",
      "trainer/ZF CHI2 Term                   26.9443\n",
      "trainer/Policy Loss                   -70.4343\n",
      "trainer/expert_lambda Loss              5.05087\n",
      "trainer/expert_lambda Value            10.4129\n",
      "trainer/policy_lambda Value             5\n",
      "trainer/Policy Grad Norm               62.5817\n",
      "trainer/Policy Param Norm              36.3098\n",
      "trainer/Zf1 Grad Norm                 867.242\n",
      "trainer/Zf1 Param Norm                 85.923\n",
      "trainer/Zf2 Grad Norm                1743.66\n",
      "trainer/Zf2 Param Norm                 86.9731\n",
      "trainer/Z Expert Predictions Mean     176.74\n",
      "trainer/Z Expert Predictions Std       26.4173\n",
      "trainer/Z Expert Predictions Max      243.264\n",
      "trainer/Z Expert Predictions Min       41.5313\n",
      "trainer/Z Policy Predictions Mean      72.6347\n",
      "trainer/Z Policy Predictions Std       44.636\n",
      "trainer/Z Policy Predictions Max      135.279\n",
      "trainer/Z Policy Predictions Min      -90.5781\n",
      "trainer/Z Expert Targets Mean         168.132\n",
      "trainer/Z Expert Targets Std           26.8251\n",
      "trainer/Z Expert Targets Max          235.738\n",
      "trainer/Z Expert Targets Min           35.5395\n",
      "trainer/Z Policy Targets Mean          72.475\n",
      "trainer/Z Policy Targets Std           44.761\n",
      "trainer/Z Policy Targets Max          141.065\n",
      "trainer/Z Policy Targets Min          -90.7188\n",
      "trainer/Log Pis Mean                   25.7037\n",
      "trainer/Log Pis Std                     5.98215\n",
      "trainer/Policy mu Mean                  0.0871135\n",
      "trainer/Policy mu Std                   0.873108\n",
      "trainer/Policy log std Mean            -4.21907\n",
      "trainer/Policy log std Std              1.21786\n",
      "trainer/Alpha                           0.01\n",
      "exploration/num steps total        303692\n",
      "exploration/num paths total           428\n",
      "evaluation/num steps total         536042\n",
      "evaluation/num paths total            610\n",
      "evaluation/path length Mean           831\n",
      "evaluation/path length Std            336.284\n",
      "evaluation/path length Max           1000\n",
      "evaluation/path length Min             61\n",
      "evaluation/Rewards Mean                 4.48337\n",
      "evaluation/Rewards Std                  1.29936\n",
      "evaluation/Rewards Max                  7.40112\n",
      "evaluation/Rewards Min                 -1.77632\n",
      "evaluation/Returns Mean              3725.68\n",
      "evaluation/Returns Std               1596.39\n",
      "evaluation/Returns Max               4690.8\n",
      "evaluation/Returns Min                201.044\n",
      "evaluation/Estimation Bias Mean        42.5119\n",
      "evaluation/Estimation Bias Std        141.232\n",
      "evaluation/EB/Q_True Mean              49.313\n",
      "evaluation/EB/Q_True Std              138.752\n",
      "evaluation/EB/Q_Pred Mean              91.8248\n",
      "evaluation/EB/Q_Pred Std               21.7025\n",
      "evaluation/Num Paths                   10\n",
      "evaluation/Average Returns           3725.68\n",
      "evaluation/Actions Mean                 0.0453375\n",
      "evaluation/Actions Std                  0.323991\n",
      "evaluation/Actions Max                  0.999972\n",
      "evaluation/Actions Min                 -0.998778\n",
      "time/backward_policy (s)                8.46648\n",
      "time/backward_zf1 (s)                  10.7183\n",
      "time/backward_zf2 (s)                  10.2548\n",
      "time/data sampling (s)                  1.47531\n",
      "time/data storing (s)                   0.0833496\n",
      "time/evaluation sampling (s)            4.00319\n",
      "time/exploration sampling (s)           1.61875\n",
      "time/logging (s)                        0.0131749\n",
      "time/preback_alpha (s)                  0.00523484\n",
      "time/preback_policy (s)                10.0041\n",
      "time/preback_start (s)                  0.707213\n",
      "time/preback_zf (s)                    29.3294\n",
      "time/saving (s)                         4.292e-06\n",
      "time/training (s)                      10.9685\n",
      "time/epoch (s)                         87.6477\n",
      "time/total (s)                       5203.03\n",
      "Epoch                                  59\n",
      "---------------------------------  ---------------\n"
     ]
    }
   ],
   "source": [
    "!python train.py --env ant --loss 'value' --alpha 0.01 --reward_type 'sparse' --noise_std 1.0 --sparse_prob 0.5 --sparse_type 'empty' --seed 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0913d-3149-472a-8496-869775ffddd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
