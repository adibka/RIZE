{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ffaaae-4896-4492-890f-ffae6067362d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f8f58-52ac-4454-a2e7-75eece343723",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # cql args\n",
    "        # cql_kwargs = args['cql_kwargs']\n",
    "        # self.use_cql = cql_kwargs['use_cql']\n",
    "        # self.num_random = cql_kwargs['num_random']\n",
    "        # self.cql_weight = cql_kwargs['cql_weight']\n",
    "        # self.with_lagrange = cql_kwargs['with_lagrange']\n",
    "        # lagrange_thresh = cql_kwargs['lagrange_thresh']\n",
    "        # if lagrange_thresh < 0:\n",
    "        #     self.with_lagrange = False\n",
    "        # if self.with_lagrange:\n",
    "        #     self.target_action_gap = lagrange_thresh\n",
    "        #     self.log_alpha_prime = torch.zeros(1, device=self.device, requires_grad=True)\n",
    "        #     self.alpha_prime_optimizer = Adam([self.log_alpha_prime], lr=zf_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0c3c3-4c36-4b56-9309-fa4449a241d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CQL (new mate)\n",
    "\"\"\"\n",
    "if self.use_cql:\n",
    "    presum_tauom_actions = torch.FloatTensor(policy_actions.shape[0] * self.num_presum_tauom,\n",
    "                                       policy_actions.shape[-1]).uniform_(-1,1).to(self.device)\n",
    "    \n",
    "    _, tau_hat, _ = self.get_tau(policy_obs, policy_actions, fp=self.fp)\n",
    "    tau_index = np.presum_tauom.presum_tauint(0, self.num_quantiles)\n",
    "    presum_tauom_tau_hat = tau_hat[:, tau_index:tau_index+1]\n",
    "    \n",
    "    z1_presum_tau, z2_presum_tau = self._get_tensor_values(policy_obs, presum_tauom_actions, presum_tauom_tau_hat)\n",
    "\n",
    "    z1_presum_tau = torch.logsumexp(z1_presum_tau, dim=1).mean()\n",
    "    z2_presum_tau = torch.logsumexp(z2_presum_tau, dim=1).mean()\n",
    "    cql_z1_loss = (z1_presum_tau - expert_z1_pred.mean()) * self.cql_weight\n",
    "    cql_z2_loss = (z2_presum_tau - expert_z2_pred.mean()) * self.cql_weight\n",
    "    \n",
    "    zf1_loss += cql_z1_loss\n",
    "    zf2_loss += cql_z2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e417f-bfa2-4c6c-8bf3-f1a094016be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CQL (old mate)\n",
    "\"\"\"\n",
    "if self.use_cql:\n",
    "    presum_tauom_actions = torch.FloatTensor(policy_actions.shape[0] * self.num_presum_tauom,\n",
    "                                       policy_actions.shape[-1]).uniform_(-1,1).to(self.device)\n",
    "    current_actions, current_log_pi = self._get_policy_actions(policy_obs, \n",
    "                                                               num_actions=self.num_presum_tauom,\n",
    "                                                               network=self.policy)\n",
    "    next_actions, next_log_pi = self._get_policy_actions(policy_next_obs, \n",
    "                                                         num_actions=self.num_presum_tauom,\n",
    "                                                         network=self.policy)\n",
    "    \n",
    "    _, tau_hat, _ = self.get_tau(policy_obs, policy_actions, fp=self.fp)\n",
    "    tau_index = np.presum_tauom.presum_tauint(0, self.num_quantiles)\n",
    "    presum_tauom_tau_hat = tau_hat[:, tau_index:tau_index+1]\n",
    "\n",
    "    z1_presum_tau, z2_presum_tau = self._get_tensor_values(policy_obs, presum_tauom_actions, presum_tauom_tau_hat)\n",
    "    z1_current, z2_current = self._get_tensor_values(policy_obs, current_actions, presum_tauom_tau_hat)\n",
    "    z1_next, z2_next = self._get_tensor_values(policy_obs, next_actions, presum_tauom_tau_hat)\n",
    "\n",
    "    presum_tauom_density = np.log(0.5 ** current_actions.shape[-1])\n",
    "    cat_z1 = torch.cat(\n",
    "        [z1_presum_tau - alpha * presum_tauom_density, \n",
    "         z1_next - alpha * next_log_pi.detach(),\n",
    "         z1_current - alpha * current_log_pi.detach()], 1\n",
    "    )\n",
    "    cat_z2 = torch.cat(\n",
    "        [z2_presum_tau - alpha * presum_tauom_density, \n",
    "         z2_next - alpha * next_log_pi.detach(),\n",
    "         z2_current - alpha * current_log_pi.detach()], 1\n",
    "    )\n",
    "    min_zf1_loss = torch.logsumexp(cat_z1, dim=1).mean()\n",
    "    min_zf2_loss = torch.logsumexp(cat_z2, dim=1).mean()\n",
    "    min_zf1_loss = (min_zf1_loss - expert_z1_pred.mean()) * self.min_z_weight\n",
    "    min_zf2_loss = (min_zf2_loss - expert_z2_pred.mean()) * self.min_z_weight\n",
    "    \n",
    "    if self.with_lagrange:\n",
    "        alpha_prime = torch.clamp(self.log_alpha_prime.exp(), min=0.0, max=1000000.0)\n",
    "        min_zf1_loss = alpha_prime * (min_zf1_loss - self.target_action_gap)\n",
    "        min_zf2_loss = alpha_prime * (min_zf2_loss - self.target_action_gap)\n",
    "\n",
    "        self.alpha_prime_optimizer.zero_grad()\n",
    "        alpha_prime_loss = (-min_zf1_loss - min_zf2_loss) * 0.5\n",
    "        alpha_prime_loss.backward(retain_graph=True)\n",
    "        self.alpha_prime_optimizer.step()\n",
    "\n",
    "    zf1_loss += min_zf1_loss\n",
    "    zf2_loss += min_zf2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17f747-28a7-4dc3-afdd-167e30b1c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"\n",
    "        CQL (std!)\n",
    "        \"\"\"\n",
    "        if self.use_cql:\n",
    "            # q1_pred = torch.sum(presum_tau * policy_z1_pred, dim=1, keepdims=True)\n",
    "            # q2_pred = torch.sum(presum_tau * policy_z2_pred, dim=1, keepdims=True)\n",
    "            # q1_std = presum_tau * (policy_z1_pred - q1_pred).pow(2)\n",
    "            # q2_std = presum_tau * (policy_z2_pred - q2_pred).pow(2)\n",
    "            # cql1_term = self.cql_weight * q1_std.sum(dim=1).sqrt().mean()\n",
    "            # cql2_term = self.cql_weight * q2_std.sum(dim=1).sqrt().mean()\n",
    "\n",
    "            cql1_term = 4.5 - self.cql_weight * policy_z1_pred.std()\n",
    "            cql2_term = 4.5 - self.cql_weight * policy_z2_pred.std()\n",
    "            \n",
    "            zf1_loss += cql1_term\n",
    "            zf2_loss += cql2_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd879f-190b-4c06-84c4-bb61e80e6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CQL (old mate; expert observations with policy actions)\n",
    "\"\"\"\n",
    "if self.use_cql:\n",
    "    presum_tauom_actions = torch.FloatTensor(policy_actions.shape[0] * self.num_presum_tauom,\n",
    "                                       policy_actions.shape[-1]).uniform_(-1,1).to(self.device)\n",
    "    current_actions, current_log_pi = self._get_policy_actions(expert_obs, \n",
    "                                                               num_actions=self.num_presum_tauom,\n",
    "                                                               network=self.policy)\n",
    "    next_actions, next_log_pi = self._get_policy_actions(expert_next_obs, \n",
    "                                                         num_actions=self.num_presum_tauom,\n",
    "                                                         network=self.policy)\n",
    "    \n",
    "    _, tau_hat, _ = self.get_tau(expert_obs, policy_actions, fp=self.fp)\n",
    "    tau_index = np.presum_tauom.presum_tauint(0, self.num_quantiles)\n",
    "    presum_tauom_tau_hat = tau_hat[:, tau_index:tau_index+1]\n",
    "\n",
    "    z1_presum_tau, z2_presum_tau = self._get_tensor_values(expert_obs, presum_tauom_actions, presum_tauom_tau_hat)\n",
    "    z1_current, z2_current = self._get_tensor_values(expert_obs, current_actions, presum_tauom_tau_hat)\n",
    "    z1_next, z2_next = self._get_tensor_values(expert_obs, next_actions, presum_tauom_tau_hat)\n",
    "\n",
    "    presum_tauom_density = np.log(0.5 ** current_actions.shape[-1])\n",
    "    cat_z1 = torch.cat(\n",
    "        [z1_presum_tau - alpha * presum_tauom_density, \n",
    "         z1_next - alpha * next_log_pi.detach(),\n",
    "         z1_current - alpha * current_log_pi.detach()], 1\n",
    "    )\n",
    "    cat_z2 = torch.cat(\n",
    "        [z2_presum_tau - alpha * presum_tauom_density, \n",
    "         z2_next - alpha * next_log_pi.detach(),\n",
    "         z2_current - alpha * current_log_pi.detach()], 1\n",
    "    )\n",
    "    min_zf1_loss = torch.logsumexp(cat_z1, dim=1).mean() * self.cql_weight\n",
    "    min_zf2_loss = torch.logsumexp(cat_z2, dim=1).mean() * self.cql_weight\n",
    "\n",
    "    zf1_loss += min_zf1_loss\n",
    "    zf2_loss += min_zf2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b6a1b-7330-4c08-b46a-6c2720e3f021",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# alpha decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7fa6d8-0621-462c-90d1-a9d8f6be7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # linear schedule entropy temperature\n",
    "        # self.start_alpha = 0.01\n",
    "        # self.end_alpha = 0.0001\n",
    "        # self.duration = 150000\n",
    "        # self.slope = (self.end_alpha - self.start_alpha) / self.duration\n",
    "        # self.alpha_decay(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d482995-e651-4042-a2bf-27817ddb7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def alpha_decay(self, epoch):\n",
    "    #     self.alpha = max(self.slope * epoch + self.start_alpha, self.end_alpha)\n",
    "\n",
    "    # @property\n",
    "    # def alpha(self):\n",
    "    #     return self.log_alpha.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b4a57-1bb4-4ed1-ab53-70140ef4c32b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43653bff-652d-4e80-ad0f-1f6d070e2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Monitor: OOD action values\n",
    "        # policy_z1_new, policy_z2_new = self.getZ(policy_obs, new_actions, tau_hat)\n",
    "        # policy_q1_new = torch.sum(presum_tau * policy_z1_new, dim=1, keepdims=True)\n",
    "        # policy_q2_new = torch.sum(presum_tau * policy_z2_new, dim=1, keepdims=True)\n",
    "        # q1_ood = (policy_q1_new - policy_q1_pred) / (torch.abs(policy_q1_pred) + self.EPS)\n",
    "        # q2_ood = (policy_q2_new - policy_q2_pred) / (torch.abs(policy_q2_pred) + self.EPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964ab4a-77c6-4311-99aa-3cc3a47a2ea6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732a250-d463-4626-bbf2-5eab2b1232b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_tqc(variant):\n",
    "    dummy_env = make_env(variant['env'])\n",
    "    obs_dim = dummy_env.observation_space.low.size\n",
    "    action_dim = dummy_env.action_space.low.size\n",
    "    expl_env = VectorEnv([lambda: make_env(variant['env']) for _ in range(variant['expl_env_num'])])\n",
    "    expl_env.seed(variant[\"seed\"])\n",
    "    expl_env.action_space.seed(variant[\"seed\"])\n",
    "    eval_env = SubprocVectorEnv([lambda: make_env(variant['env']) for _ in range(variant['eval_env_num'])])\n",
    "    eval_env.seed(variant[\"seed\"])\n",
    "\n",
    "    M = variant['layer_size']\n",
    "    num_quantiles = variant['num_quantiles']\n",
    "    n_nets = variant['n_nets']\n",
    "    \n",
    "    zf = Critic(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        num_quantiles=num_quantiles,\n",
    "        hidden_sizes=[M, M],\n",
    "        n_nets=n_nets,\n",
    "    )\n",
    "    target_zf = Critic(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        num_quantiles=num_quantiles,\n",
    "        hidden_sizes=[M, M],\n",
    "        n_nets=n_nets,\n",
    "    )\n",
    "    policy = TanhGaussianPolicy(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    eval_policy = MakeDeterministic(policy)\n",
    "    # fraction proposal network\n",
    "    fp = target_fp = None\n",
    "    if variant['trainer_kwargs'].get('tau_type') == 'fqf':\n",
    "        fp = FlattenMlp(\n",
    "            input_size=obs_dim + action_dim,\n",
    "            output_size=num_quantiles,\n",
    "            hidden_sizes=[M // 2, M // 2],\n",
    "            output_activation=softmax,\n",
    "        )\n",
    "        target_fp = FlattenMlp(\n",
    "            input_size=obs_dim + action_dim,\n",
    "            output_size=num_quantiles,\n",
    "            hidden_sizes=[M // 2, M // 2],\n",
    "            output_activation=softmax,\n",
    "        )\n",
    "    eval_path_collector = VecMdpPathCollector(\n",
    "        eval_env,\n",
    "        eval_policy,\n",
    "    )\n",
    "    expl_path_collector = VecMdpStepCollector(\n",
    "        expl_env,\n",
    "        policy,\n",
    "    )\n",
    "    replay_buffer = TorchReplayBuffer(\n",
    "        variant['replay_buffer_size'],\n",
    "        dummy_env,\n",
    "    )\n",
    "    expert_buffer = TorchReplayBuffer(\n",
    "        variant['replay_buffer_size'] // 10,\n",
    "        dummy_env,\n",
    "    )\n",
    "    iq_args = variant['iq_kwargs']\n",
    "    expert_buffer.load(iq_args['expert_path'], iq_args['demos'], \n",
    "                       iq_args['subsample_freq'], variant['seed']\n",
    "                      )\n",
    "    trainer = TruncIDSACTrainer(\n",
    "        args=variant,\n",
    "        env=dummy_env,\n",
    "        policy=policy,\n",
    "        zf=zf,\n",
    "        target_zf=target_zf,\n",
    "        fp=fp,\n",
    "        target_fp=target_fp,\n",
    "        num_quantiles=num_quantiles,\n",
    "        **variant['trainer_kwargs'],\n",
    "    )\n",
    "    algorithm = TorchVecOnlineIQAlgorithm(\n",
    "        trainer=trainer,\n",
    "        exploration_env=expl_env,\n",
    "        evaluation_env=eval_env,\n",
    "        exploration_data_collector=expl_path_collector,\n",
    "        evaluation_data_collector=eval_path_collector,\n",
    "        replay_buffer=replay_buffer,\n",
    "        expert_buffer=expert_buffer,\n",
    "        **variant['algorithm_kwargs'],\n",
    "    )\n",
    "    algorithm.to(ptu.device)\n",
    "    algorithm.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d2246-c593-4151-8f28-e3f7d7bfdcd1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# iq_distr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc852f6-b1ae-4eec-967e-fb189f3e0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iq_distr_loss(\n",
    "    input_expert, \n",
    "    target_expert,\n",
    "    input_policy,\n",
    "    target_policy,\n",
    "    policy_reward,\n",
    "    log_pi,\n",
    "    alpha,\n",
    "    tau,\n",
    "    weight,\n",
    "    args\n",
    "):\n",
    "    expert_reward = input_expert - target_expert\n",
    "    policy_reward = input_policy - alpha * log_pi - target_policy\n",
    "    loss = - expert_reward.mean() + policy_reward.mean()\n",
    "\n",
    "    if args['regularize']:\n",
    "        expert_rho = quantile_regression_loss(input_expert, 10 + target_expert, tau, weight)\n",
    "        policy_rho = quantile_regression_loss(input_policy, policy_reward + target_policy, tau, weight)\n",
    "        chi2_loss = 1/(4 * args['alpha']) * torch.cat([expert_rho, policy_rho], dim=-1).mean()\n",
    "        loss += chi2_loss\n",
    "    \n",
    "    loss_dict = {\n",
    "        'expert_reward': expert_reward.mean().item(),\n",
    "        'policy_reward': policy_reward.mean().item(),\n",
    "        'chi2_loss': chi2_loss.item() if args['regularize'] else 0,\n",
    "    }\n",
    "    return loss, loss_dict\n",
    "\n",
    "\n",
    "def quantile_regression_loss(inputs, targets, tau, weight):\n",
    "    \"\"\"\n",
    "    input: (N, T)\n",
    "    target: (N, T)\n",
    "    tau: (N, T)\n",
    "    \"\"\"\n",
    "    inputs = inputs.unsqueeze(-1)\n",
    "    targets = targets.detach().unsqueeze(-2)\n",
    "    tau = tau.detach().unsqueeze(-1)\n",
    "    # weight = weight.detach().unsqueeze(-2)\n",
    "    expanded_inputs, expanded_targets = torch.broadcast_tensors(inputs, targets)\n",
    "    L = (expanded_inputs - expanded_targets)**2\n",
    "    sign = torch.sign(expanded_inputs - expanded_targets) / 2. + 0.5\n",
    "    rho = torch.abs(tau - sign) * L\n",
    "    return rho.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ee30d-094c-4a0a-ae59-7ceb9559a9a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9a96c6-976d-45e7-8fd5-1c24ab784e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import csv\n",
    "from rlkit.envs import make_env\n",
    "from rlkit.envs.vecenv import SubprocVectorEnv, VectorEnv\n",
    "from rlkit.torch.idsac.networks import QuantileMlp, Critic\n",
    "from rlkit.torch.idsac.policies import TanhGaussianPolicy\n",
    "from rlkit.data_management.torch_replay_buffer import TorchReplayBuffer\n",
    "import rlkit.torch.pytorch_util as ptu\n",
    "from rlkit.torch.core import eval_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f976d60-f9f2-4579-bc7c-57ece766a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([10], dtype=torch.float, requires_grad=True).to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a81dcd4-7d82-4313-813e-84dc8061876a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.], device='cuda:0', grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7925026a-4990-4bf7-99f4-9ecba0cbec7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1011.], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce979ec-578a-401c-99d1-5d34011bdcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
