{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd095125-6be1-4c22-8760-d1fba60ab86d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## experiment (tqc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603bc9d-5ced-4410-9d8b-bc74345fd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_tqc(variant):\n",
    "    dummy_env = make_env(variant['env'])\n",
    "    obs_dim = dummy_env.observation_space.low.size\n",
    "    action_dim = dummy_env.action_space.low.size\n",
    "    expl_env = VectorEnv([lambda: make_env(variant['env']) for _ in range(variant['expl_env_num'])])\n",
    "    expl_env.seed(variant[\"seed\"])\n",
    "    expl_env.action_space.seed(variant[\"seed\"])\n",
    "    eval_env = SubprocVectorEnv([lambda: make_env(variant['env']) for _ in range(variant['eval_env_num'])])\n",
    "    eval_env.seed(variant[\"seed\"])\n",
    "\n",
    "    M = variant['layer_size']\n",
    "    num_quantiles = variant['num_quantiles']\n",
    "    n_nets = variant['n_nets']\n",
    "    \n",
    "    zf = Critic(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        num_quantiles=num_quantiles,\n",
    "        hidden_sizes=[M, M],\n",
    "        n_nets=n_nets,\n",
    "    )\n",
    "    target_zf = Critic(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        num_quantiles=num_quantiles,\n",
    "        hidden_sizes=[M, M],\n",
    "        n_nets=n_nets,\n",
    "    )\n",
    "    policy = TanhGaussianPolicy(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    eval_policy = MakeDeterministic(policy)\n",
    "    # fraction proposal network\n",
    "    fp = target_fp = None\n",
    "    if variant['trainer_kwargs'].get('tau_type') == 'fqf':\n",
    "        fp = FlattenMlp(\n",
    "            input_size=obs_dim + action_dim,\n",
    "            output_size=num_quantiles,\n",
    "            hidden_sizes=[M // 2, M // 2],\n",
    "            output_activation=softmax,\n",
    "        )\n",
    "        target_fp = FlattenMlp(\n",
    "            input_size=obs_dim + action_dim,\n",
    "            output_size=num_quantiles,\n",
    "            hidden_sizes=[M // 2, M // 2],\n",
    "            output_activation=softmax,\n",
    "        )\n",
    "    eval_path_collector = VecMdpPathCollector(\n",
    "        eval_env,\n",
    "        eval_policy,\n",
    "    )\n",
    "    expl_path_collector = VecMdpStepCollector(\n",
    "        expl_env,\n",
    "        policy,\n",
    "    )\n",
    "    replay_buffer = TorchReplayBuffer(\n",
    "        variant['replay_buffer_size'],\n",
    "        dummy_env,\n",
    "    )\n",
    "    expert_buffer = TorchReplayBuffer(\n",
    "        variant['replay_buffer_size'] // 10,\n",
    "        dummy_env,\n",
    "    )\n",
    "    iq_args = variant['iq_kwargs']\n",
    "    expert_buffer.load(iq_args['expert_path'], iq_args['demos'], \n",
    "                       iq_args['subsample_freq'], variant['seed']\n",
    "                      )\n",
    "    trainer = TruncIDSACTrainer(\n",
    "        args=variant,\n",
    "        env=dummy_env,\n",
    "        policy=policy,\n",
    "        zf=zf,\n",
    "        target_zf=target_zf,\n",
    "        fp=fp,\n",
    "        target_fp=target_fp,\n",
    "        num_quantiles=num_quantiles,\n",
    "        **variant['trainer_kwargs'],\n",
    "    )\n",
    "    algorithm = TorchVecOnlineIQAlgorithm(\n",
    "        trainer=trainer,\n",
    "        exploration_env=expl_env,\n",
    "        evaluation_env=eval_env,\n",
    "        exploration_data_collector=expl_path_collector,\n",
    "        evaluation_data_collector=eval_path_collector,\n",
    "        replay_buffer=replay_buffer,\n",
    "        expert_buffer=expert_buffer,\n",
    "        **variant['algorithm_kwargs'],\n",
    "    )\n",
    "    algorithm.to(ptu.device)\n",
    "    algorithm.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b40f96-5af4-4fc9-8f5d-34bacdc440d8",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df783e4e-ddf6-45d9-8b26-6863e89107f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No personal conf_private.py found.\n",
      "doodad not detected\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "import rlkit.torch.pytorch_util as ptu\n",
    "from rlkit.data_management.torch_replay_buffer import TorchReplayBuffer\n",
    "from rlkit.envs import make_env\n",
    "from rlkit.envs.vecenv import SubprocVectorEnv, VectorEnv\n",
    "from rlkit.launchers.launcher_util import set_seed, setup_logger\n",
    "from rlkit.samplers.data_collector import (VecMdpPathCollector, VecMdpStepCollector)\n",
    "from rlkit.torch.idsac.idsac import IDSACTrainer\n",
    "from rlkit.torch.idsac.networks import QuantileMlp, Critic, softmax\n",
    "from rlkit.torch.networks import FlattenMlp\n",
    "from rlkit.torch.sac.policies import MakeDeterministic, TanhGaussianPolicy\n",
    "from rlkit.torch.torch_iq_algorithm import TorchVecOnlineIQAlgorithm\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "torch.set_num_interop_threads(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba84c8-f8f8-4272-8885-83a6c028ee37",
   "metadata": {},
   "source": [
    "# experiment (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c847e24-53fa-4f9b-850b-c7b648d30304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(variant):\n",
    "    dummy_env = make_env(variant['env'])\n",
    "    obs_dim = dummy_env.observation_space.low.size\n",
    "    action_dim = dummy_env.action_space.low.size\n",
    "    expl_env = VectorEnv([lambda: make_env(variant['env']) for _ in range(variant['expl_env_num'])])\n",
    "    expl_env.seed(variant[\"seed\"])\n",
    "    expl_env.action_space.seed(variant[\"seed\"])\n",
    "    eval_env = SubprocVectorEnv([lambda: make_env(variant['env']) for _ in range(variant['eval_env_num'])])\n",
    "    eval_env.seed(variant[\"seed\"])\n",
    "\n",
    "    M = variant[\"layer_size\"]\n",
    "    num_quantiles = variant[\"num_quantiles\"]\n",
    "    tau_type = variant[\"trainer_kwargs\"][\"tau_type\"]\n",
    "    \n",
    "    zf1 = QuantileMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        num_quantiles=num_quantiles,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    zf2 = QuantileMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        num_quantiles=num_quantiles,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    target_zf1 = QuantileMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        num_quantiles=num_quantiles,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    target_zf2 = QuantileMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        num_quantiles=num_quantiles,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    policy = TanhGaussianPolicy(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_sizes=[M, M, M // 2],\n",
    "    )\n",
    "    eval_policy = MakeDeterministic(policy)\n",
    "    target_policy = TanhGaussianPolicy(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_sizes=[M, M, M // 2],\n",
    "    )\n",
    "    # fraction proposal network\n",
    "    fp = target_fp = None\n",
    "    if variant['trainer_kwargs'].get('tau_type') == 'fqf':\n",
    "        fp = FlattenMlp(\n",
    "            input_size=obs_dim + action_dim,\n",
    "            output_size=num_quantiles,\n",
    "            hidden_sizes=[M // 2, M // 2],\n",
    "            output_activation=softmax,\n",
    "        )\n",
    "        target_fp = FlattenMlp(\n",
    "            input_size=obs_dim + action_dim,\n",
    "            output_size=num_quantiles,\n",
    "            hidden_sizes=[M // 2, M // 2],\n",
    "            output_activation=softmax,\n",
    "        )\n",
    "    eval_path_collector = VecMdpPathCollector(\n",
    "        eval_env,\n",
    "        eval_policy,\n",
    "        zf1,\n",
    "        tau_type,\n",
    "    )\n",
    "    expl_path_collector = VecMdpStepCollector(\n",
    "        expl_env,\n",
    "        policy,\n",
    "    )\n",
    "    replay_buffer = TorchReplayBuffer(\n",
    "        variant['replay_buffer_size'],\n",
    "        dummy_env,\n",
    "    )\n",
    "    expert_buffer = TorchReplayBuffer(\n",
    "        variant['replay_buffer_size'] // 10,\n",
    "        dummy_env,\n",
    "    )\n",
    "    iq_args = variant['iq_kwargs']\n",
    "    expert_buffer.load(iq_args['expert_path'], iq_args['demos'], \n",
    "                       iq_args['subsample_freq'], variant['seed']\n",
    "                      )\n",
    "    trainer = IDSACTrainer(\n",
    "        args=variant,\n",
    "        env=dummy_env,\n",
    "        policy=policy,\n",
    "        target_policy=target_policy,\n",
    "        zf1=zf1,\n",
    "        zf2=zf2,\n",
    "        target_zf1=target_zf1,\n",
    "        target_zf2=target_zf2,\n",
    "        fp=fp,\n",
    "        target_fp=target_fp,\n",
    "        num_quantiles=num_quantiles,\n",
    "        **variant['trainer_kwargs'],\n",
    "    )\n",
    "    algorithm = TorchVecOnlineIQAlgorithm(\n",
    "        trainer=trainer,\n",
    "        exploration_env=expl_env,\n",
    "        evaluation_env=eval_env,\n",
    "        exploration_data_collector=expl_path_collector,\n",
    "        evaluation_data_collector=eval_path_collector,\n",
    "        replay_buffer=replay_buffer,\n",
    "        expert_buffer=expert_buffer,\n",
    "        **variant['algorithm_kwargs'],\n",
    "    )\n",
    "    algorithm.to(ptu.device)\n",
    "    algorithm.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e380543-9ff2-4947-8cda-2f5b05957627",
   "metadata": {},
   "source": [
    "# args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39fa2c3c-ae22-4830-bad1-8b99032cb01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(dsac_cfg_path,\n",
    "               expert_path,\n",
    "               iq_cfg_path='configs/dsac-normal-iqn-neutral/iq.yaml',\n",
    "               cql_cfg_path='configs/dsac-normal-iqn-neutral/cql.yaml'\n",
    "              ):\n",
    "    \n",
    "    with open(dsac_cfg_path, 'r', encoding=\"utf-8\") as f:\n",
    "        variant = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        \n",
    "    with open(iq_cfg_path, 'r', encoding=\"utf-8\") as f:\n",
    "        iq_cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    with open(cql_cfg_path, 'r', encoding=\"utf-8\") as f:\n",
    "        cql_cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        \n",
    "    iq_cfg['expert_path'] = expert_path\n",
    "    variant['iq_kwargs'] = iq_cfg\n",
    "    variant['cql_kwargs'] = cql_cfg\n",
    "    return variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a40faac3-2a8c-49fc-bece-f891df61d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant = get_config(dsac_cfg_path='configs/dsac-normal-iqn-neutral/ant.yaml',\n",
    "                     expert_path='experts/Ant-v2_25.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00203431-e8bc-4e42-977f-298ec40d51ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-10 17:05:01.205066 +0330 | Variant:\n",
      "2024-06-10 17:05:01.206185 +0330 | {\n",
      "  \"algorithm_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"min_num_steps_before_training\": 10000,\n",
      "    \"num_epochs\": 300,\n",
      "    \"num_eval_paths_per_epoch\": 10,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 1000\n",
      "  },\n",
      "  \"env\": \"Ant-v2\",\n",
      "  \"seed\": 0,\n",
      "  \"expectation_z\": false,\n",
      "  \"eval_env_num\": 10,\n",
      "  \"expl_env_num\": 10,\n",
      "  \"layer_size\": 256,\n",
      "  \"num_quantiles\": 24,\n",
      "  \"replay_buffer_size\": 1000000,\n",
      "  \"bias\": 10.0,\n",
      "  \"bias_lr\": \"0.001,\",\n",
      "  \"use_automatic_bias_tuning\": true,\n",
      "  \"trainer_kwargs\": {\n",
      "    \"alpha\": 0.01,\n",
      "    \"discount\": 0.99,\n",
      "    \"policy_lr\": 7.5e-05,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"tau_type\": \"iqn\",\n",
      "    \"use_automatic_entropy_tuning\": false,\n",
      "    \"zf_lr\": 0.0003\n",
      "  },\n",
      "  \"version\": \"normal-iqn-neutral\",\n",
      "  \"iq_kwargs\": {\n",
      "    \"expert_path\": \"experts/Ant-v2_25.pkl\",\n",
      "    \"subsample_freq\": 1,\n",
      "    \"demos\": 10,\n",
      "    \"regularize\": true,\n",
      "    \"div\": null,\n",
      "    \"loss\": \"value_policy\",\n",
      "    \"alpha\": 2.5\n",
      "  },\n",
      "  \"cql_kwargs\": {\n",
      "    \"use_cql\": false,\n",
      "    \"cql_weight\": 0.05,\n",
      "    \"num_random\": 10,\n",
      "    \"with_lagrange\": false,\n",
      "    \"lagrange_thresh\": 10.0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    ptu.set_gpu_mode(True, 0)\n",
    "    # device = torch.device('cuda:0')\n",
    "seed = variant[\"seed\"]\n",
    "set_seed(seed)\n",
    "log_prefix = \"_\".join([\"idsac\", variant[\"env\"][:-3].lower(), str(variant[\"version\"])])\n",
    "setup_logger(log_prefix, variant=variant, seed=seed)\n",
    "variant[\"device\"] = ptu.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9e0ac-93fd-4003-bfd1-29f851badba5",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa21b2b4-9bf0-4965-91a7-ff8ca706fcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddie/venvs/IQ/lib/python3.10/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(variant)\u001b[0m\n\u001b[1;32m     12\u001b[0m num_quantiles \u001b[38;5;241m=\u001b[39m variant[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_quantiles\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m tau_type \u001b[38;5;241m=\u001b[39m variant[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainer_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtau_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m dropout \u001b[38;5;241m=\u001b[39m \u001b[43mvariant\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m drop_rate \u001b[38;5;241m=\u001b[39m variant[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m zf1 \u001b[38;5;241m=\u001b[39m QuantileMlp(\n\u001b[1;32m     18\u001b[0m     input_size\u001b[38;5;241m=\u001b[39mobs_dim \u001b[38;5;241m+\u001b[39m action_dim,\n\u001b[1;32m     19\u001b[0m     output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     drop_rate\u001b[38;5;241m=\u001b[39mdrop_rate,\n\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dropout'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    experiment(variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6679a-5037-44ff-a459-897c0ee8c8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
